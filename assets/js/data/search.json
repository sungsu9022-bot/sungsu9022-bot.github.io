[ { "title": "[kubernetes-in-action] 11. 쿠버네티스 내부 이해", "url": "/posts/devlog-platform-kubernetes-in-action11/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-09-03 17:34:00 +0900", "snippet": "11. 쿠버네티스 내부 이해11.1 쿠버네티스 아키텍처 이해쿠버네티스 클러스터 구성요소 컨트롤 플레인 (워커)노드컨트롤 플레인 구성요소 클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할을 한다. etcd 분산 저장 스토리지 API 서버 스케줄러 컨트롤러 매니저워커 노드에서 실행하는 구성 요소 kubelet 쿠버네티스 서비스 프록시(kube-proxy) 컨테이너 런타임(Docker, rkt 외 기타)애드온 구성 요소 컨트롤 플레인과 노드에서 실행되는 구성 요소 외에 클러스터에서 추가 기능을 위한 구성 요소 쿠버네티스 DNS 서버 대시보드 인그레스 컨트롤러 힙스터 컨테이너 네트워크 인터페이스(CNI) 플러그인11.1.1 쿠버네티스 구성 요소의 분산 특성 모든 구송요소는 개별 프로세스로 실행된다.컨트롤 플레인 구성 요소의 상태 확인kubectl get componentstatuses구성 요소가 서로 통신하는 방법 쿠버네티스 시스템 구성 요소는 오직 API 서버하고만 통신한다. API서버는 etcd와 통신하는 유일한 구성 요소 클러스터 내에 존재하는 리소스 데이터는 etcd에 저장되고, 이를 처리하는건 API 서버라는 의미 kubectl을 이용해 로그를 가져오거나 attach 명령으로 실행중인 컨테이너에 연결할 때와 kubectl port-forward명령을 실행할때는 API 서버가 kubelet에 접속한다.(이 경우를 제외하고는 모두 API서버로만 요청한다)개별 구성 요소의 여러 인스턴스 실행 컨트롤 플레인의 구성 요소는 여러 서버에 걸쳐 실행될 수 있다. 컨트롤 플레인 구성 요소 중 etcd와 API 서버는 둘 이상 실행해 가용성을 높일수도 있다. 스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화되고 나머지는 대기 상태로 존재한다.구성 요소 실행 방법 kube-proxy와 같은 컨트롤 플레인 구성 요소는 시스템에 직접 배포하거나 파드로 실행할 수 있따. kubelet은 항상 시스템 구성 요소로 실행되는 유일한 구성 요소이며, kubelet이 다른 구성 요소를 파드로 실행한다. etcd, API 서버, 스케줄러, 컨트롤러 매니저, DNS 서버는 마스터에서 실행되고, kube-proxy, flannel 네트워킹 파드는 워커노드에서 실행된다.# 쿠버네티스 구성요소의 pod의 node 확인kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system11.1.2 쿠버네티스 etcd를 사용하는 방법 API 서버가 다시 시작되거나 실패하더라도 유지하기 위해 매니페스트가 영구적으로 저장될 필요가 있다. 그래서 쿠버네티스는 빠르고, 분산해서 저장되며, 일관된 키-값 저장소를 제공하는 etcd를 사용한다. 둘 이상의 etcd 인스턴스를 실행해 고가용성과 우수한 성능을 제공할 수 있다. 쿠버네티스가 클러스터 상태와 메타데이터를 저장하는 유일한 장소는 etcd뿐이다.낙관적 동시성 제어 데이터 조작에 잠금을 설정해 그동안 데이터를 읽거나 업데이트하지 못하도록 하기 위해서 데이터 업데이트시 버전 번호를 포함시키는 방법.(DB에서의 낙관적 락과 동일한 방식)리소스를 etcd 저장하는 방법 etcd v2는 키를 계층적 키 공간에 저장해 파일시스템의 파일과 유사한 키-값 쌍을 만든다.(디렉터리 존재) etcd v3는 디렉터리를 지원하지 않지만 키 형식은 동일하게 유지되고, 키 이름에 ‘/’을 포함시켜서 마치 디렉터리인것처럼 사용 가능하다. etcd v3에서는 ls를 사용할 수 없고 etcdctl get /registry --prefix=true을 이용하면 v2와 동일한 결과를 얻을 수 있다. etcd의 계층적 키 공간때문에, 저장된 모든 리소스를 단순하게 파일시슽메에 있는 JSON 파일로 생각할 수 있다. 쿠버네티스 1.7 이전에 Secret 리소스의 JSON을 암호화없이 etcd에 저장했으나 1.7부터 암호화하여 저장하고 있다.# etcd registry 하위 조회etcdctl ls /registry/registry/configmaps/registry/daemonsets/registry/deployments/registry/events/registry/namespaces/registry/pods# deafult 네임스페이스의 pods 조회etcdctl ls /registry/pods/default/# deafult 네임스페이스의 pods 조회etcdctl ls /registry/pods/default/# deafult 네임스페이스의 pod 중 하나의 상세 정보 조회etcdctl get /registry/pods/default/kubia-159041347-wt6ga...저장된 오브젝트의 일관성과 유효성 보장 쿠버네티스는 다른 모든 구성 요소가 API 서버를 통하도록 함으로써 데이터 불일치를 방지하고 일관성을 유지한다. API 서버 한곳에서 낙관적 락 매너키즘을 구현해서 클러스터의 상태를 업데이트하기 때문에, 오류가 발생할 가능성을 줄이고 항상 일관성을 가질 수 있다.클러스터링된 etcd의 일관성 보장 고가용성을 위해 2개 이상의 etcd 인스턴스를 실행하는것이 일반적이다. 분산 시스템은 실제 상태가 무엇인지 합의(consensus)에 도달해야 한다. etcd는 RAFT 합의 알고리즘을 사용해 어느 순가이든 각 노드의상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의한 상태 중에 하나임을 보장한다. 합의 알고리즘은 클러스터가 다음 상태로 진행하기 위해 과반수(혹은 쿼럼[quorum])가 필요하다. 2개의 노드는 여전히 과반을 가지고 있어 클라이언트에서 상태 변경 요청을 받을수 있고, 과반을 충족하지 못한 노드는 상태 변경 요청을 받을 수 없다. etcd 인스턴스 수를 홀수로 하는 이유 일반적으로 홀수로 배포한다. 2개로 할 경우 둘 중 하나라도 실패하면 과반이 존재하지 않기 때문에 상태를 변경할 수 없음.. 대규모 etcd 클러스터에서는 일반적으로 5대 혹은 7대 노드면 충분하다.11.1.3 API 서버의 기능 쿠버네티스 APi 서버는 다른 모든 구성 요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소이다. 유효성 검사와 함께 낙관적 락도 처리하기 때문에 동시에 업데이트가 발생하더라도 다른 클라이언트에 의해 오브젝트의 변경 사항이 재정의되지 않는다.kubectl을 이용해 처리하는 과정 kubectl은 파일의 내용을 API 서버에 HTTP POST 요청으로 전달하면 내부에서는 다음과 같은 일이 발생한다. Authentication, Authorization, Admission control을 거친후에 resoruce validation을 거치고 이를 통과해야 etcd에 데이터를 처리한다.인증 플러그인으로 클라이언트 인증 인증 작업은 API 서버에 구성된 하나 이상의 플러그인에 의해 수행되고 HTTP 요청을 검사해 수행한다. 클라이언트 인증서 혹은 HTTP Header를 가져와 인증하는 그런 처리들이 해당된다.인가 플러그인을 통한 클라이언트 인가 인증된 사용자가 요청한 작ㅇ버이 요청한 리소스를 대상으로 수행할 수 있는지를 판별한다. 사용자가 요청한 네임스페이스 안에 파드를 생성할 수 있는지를 결정한다.어드미션 컨트롤 플러그인으로 요청된 리소스 확인과 수정 해당 플러그인은 리소스를 여러 가지 이유로 수정할 수 있다. 누락된 필드를 기본값으로 초기화하거나 재정의하기도 하고, 요청에 없는 관계된 리소스를 수정하거나 어떤 이유로든 요청을 거부할 수도 있다. 데이터를 읽는 GET 요청의 경우에는 어드미션 컨트롤러를 거치지 않는다.어드미션 컨트롤러 플러그인 예시 AlwasysPullImages : 이미지를 항상 강제로 가져오도록 재정의 ServiceAccount : 명시적으로 지정하지 않는 경우 default 서비스 어카운트 NamespaceLifecycle : 네임스페이스가 존재하지 않을 경우 파드 생성 방지 ResourceQuota : 특정 네임스페이스 안에 있는 파드가 네임스페이스에 할당된 CPU와 메모리를 사용하도록 강제한다.리소스 유효성 확인 및 영구 저장 요청이 모든 어드미션 컨트롤 플러그인을 통과하면 API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한다.11.1.4 API 서버가 리소스 변경을 클라이언트에 통보하는 방법 API 서버는 위 과정을 제외한 다른 일은 하지 않는다. 파드를 만든다던지, 서비스의 엔드포인트를 관리한다던지.. 이런 일들 아무것도 하지 않는다.(컨트롤러 매니저의 역할) API가 직접 구성요소에게 명령을 내리는 방식이 아니다. 단지 각각 구성요소에서 관찰한 변경사항을 노티해주기만 한다. 클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지한다. 오브젝트가 갱신될 때마다, 서버는 오브젝트를 감시하고 있는 연결된 모든 클라이언트에게 오브젝트의 새로운 버전을 보낸다. 갱신된 오브젝트를 감시하고 있는 모든 관찰자에게 갱신된 오브젝트를 전달한다. kubectl 도구 또한 리소스 변경을 감시할 수 있는 API 서버의 클라이언트 중 하나이고, --watch 옵션을 통해 파드의 생성, 수정, 삭제 통보를 받을 수 있다.# watch 설정kubectl get po --watchkubectl get po -o yaml --watch11.1.5 스케줄러 스케줄러는 API 서버의 감시 매커니즘을 통해 새로 생성될 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당한다. 선택된 노드(해당 노드에서 실행중인 kubelet)에 파드를 실행하도록 지시하지 않는다. 단지 스케줄러는 API 서버로 파드 정의를 갱신한다. 이러면 API 서버는 kubelet에 파드 스케줄링된 것을 통보하고, 대상 노드의 kubelet은 파드의 컨테이너를 생성하고 실행한다. 스케줄러의 중요한 작업 중 하나는 파드에 가장 적합한 노드를 선택하는 것인데 이 작업은 단순하지는 않다. 가장 쉬운 방법 : 이미 실행중인 파드를 신경 쓰지 않고 무작위로 노드를 선택하는 것 머신 러닝 등 고급 기술을 이용해 향후 몇분 혹은 몇 시간 내에 어떤 종류의 파드를 스케줄링할지 예측해 기존 파드를 다시 스케줄링하지 않고도 하드웨어 활용을 극대화 시킬수 있는 노드를 선택하는 것. 쿠버네티스의 기본 스케줄러는 위 2가지의 사이 정도 기본 스케줄링 알고리즘 모든 노드 중에서 파드를 스케줄링할 수 있는 노드 목록을 필터링 수용 가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택 노드가 같은 차상위 점수를 가지고 있다면, 파드가 모든 노드에 고르게 배포되도록 라운드-로빈을 사용한다.수용 가능한 노드 찾기 미리 설정된 조건 함수 목록에 각 노드를 전달하여 처리된다. 노드가 하드웨어 리소스에 대한 파드 요청을 충족할 수 있는가? 노드에 리소스가 부족한가? 파드를 특정 노드로 스케줄링하도록 요청한 경우에 해당 노드인가? 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는가(정의한 경우에 한하여)? 파드가 특정 호스트 포트에 할당되도록 요청한 경우 해당 포트가 이 노드에서 이미 사용중인가? 파드 요청이 특정한 유형의 볼륨을 요청하는 경우 이 노드에서 해당 볼륨을 파드에 마운트할 수 있는가? 아니면 이 노드에 있는 다른 파드가 이미같은 볼륨을 사용하고 있는가? 파드가 노드의 테인트를 허용하는가? 파드가 노드의 파드 어피니티, 안티-어피니티 규칙을 지정했는가? 그렇다면 노드에 파드를 스케줄링하면 이 규칙을 어기게 되는가?파드에 가장 적합한 노드 선택 일반적으로는 리소스가 남아 도는 쪽의 노드를 선택하도록 되어야 한다.고급 파드 스케줄링 레플리카가 여러개인 경우, 한 노드에 스케줄링하는 것보다 가능한 많은 노드에 분산되는 것이 이상적일 것이다. 동일한 서비스 또는 레플리카셋에 속한 파드는 기본적으로 여러 노드에 분산되는데, 어피니티와 안티-어피니티 규칙을 정의해서 클러스터 전체에 퍼지거나 가깝게 유지되도록 강제할 수 있다.다중 스케줄러 사용 여러 개의 스케줄러를 실행할 수 있다.(파드 정의 안에 schedulerName 속성에 파드를 스케줄링할 때 사용할 스케줄러를 지정할 수 있음) 사용자가 직접 스케줄러를 구현해 클러스터에 배포하거나 다른 설정 옵션을 가진 쿠버네티스 스케줄러를 배포할 수도 있다.11.1.6 컨트롤러 매니저에서 실행되는 컨트롤러 소개 다양한 조정작업을 수행하는 여러 컨트롤러가 하나의 컨트롤러 매니저 프로세스에서 실행된다. 거의 대부분의 리소스에는 그에 해당하는 컨트롤러가 있고, 컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된 쿠버네티스 구성요소이다.컨트롤러 종류 레플리케이션 매니저 레플리카셋, 데몬셋, 잡 컨트롤러 디플로이먼트 컨트롤러 스테이트풀셋 컨트롤러 노드 컨트롤러 서비스 컨트롤러 엔드포인트 컨트롤러 네임스페이스 컨트롤러 퍼시스턴트볼륨 컨트롤러 그 밖의 컨트롤러컨트롤러 역할과 동작 방식 이해 모두 API 서버에서 리소스가 변경되는 것을 감시하고 각 변경 작업을 수행한다. 컨트롤러는 조정 루프를 실행해, 실제 상태를 원하는 상태로 조정하고, 새로운 상태의 리소스의 status 섹션에 기록한다. 감시 매커니즘을 이용해 변경사항을 통보받지만 모든 이벤트를 놓치지 않고 받는다는 것을 보장하진 않기 때문에, 정기적으로 목록을 가져오는 작업을 수행해 누락된 이벤트가 없는지 확인을 하기도 한다. 각 컨트롤러는 API 서버에 연결하고 감시 메커니즘을 통해 컨트롤러가 담당하는 리소스 유형에서 변경이발생하면 통보해줄 것을 요청한다.컨트롤러의 동작 방식 모든 컨트롤러는 API 서버로 API 오브젝트를 제어하고, 자신이 맡은 감시 리소스를 기준으로 적절히 정의된 동작을 수행한다.11.1.7 kubelet이 하는 일 워커 노드에서 실행하는 모든것을 담당하는 구성요소kubelet의 작업 이해 첫번째 작업은 실행중인 노드를 노드 리소스로 만들어 API 서버에 등록하는 것이다. 그리고 API 서버를 지속적으로 모니터링하다가 해당 노드에 파드가 스케줄링 되면, 파드의 컨테이너를 실행시킨다. 실행중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량을 API 서버에 보고한다. kubelet은 컨테이너 라이브니스 프로브를 실행도 담당한다.API 서버 없이 정적 파드 실행 특정 로컬 디렉터리 안에 있는 매니페스트 파일을 기반으로 파드를 실행할 수도 있다. 컨트롤 플레인 구성요소를 파드로 실행하는데 이 방식을 이용한다. 시스템 구성 요소 파드 매니페스트를 kubelet의 매니페스트 디렉터리 안에 너허서 kubelet이 실행하고 관리하도록 할 수 있다.11.1.8 쿠버네티스 서비스 프록시의 역할 모든 워커 노드는 클라이언트가 쿠버네티스 API로 정의한 서비스에 연결할 수 있도록 해주는 kube-proxy도 같이 실행시킨다. kube-proxy는 서비스의 IP와 포트로 들어온 접속을 서비스를 지원하는 하나의 파드와 연결시켜준다. 파드간에 로드 밸런싱도 수행해준다.프록시라고 부르는 이유 kube-proxy의 초기 구현은 userspace에서 동작하는 프록시였기 떄문. userspace 프록시 모드에서는 아래와 같이 iptables 규칙을 설정해 kube-proxy로 전송하여 처리하였다. 현재는 훨씬 성능이 우수한 구현체에서 iptables 규칙만 사용해 프록시 서버를 거치지 않고 패킷을 무작위로 선택한 백엔드 파드로 전달한다.(iptables 프록시모드)userspace proxy vs iptables proxy 이 두 모드의 가장 큰 차이점은 패킷이 kube-proxy를 통과해 사용자 공간에서 처리되는지, 아니면 커널에서 처리되는지 여부이다.iptables proxy 유의사항 iptables 프록시 모드는 파드를 무작위로 선택하여 라운드 로빈 방식으로 처리되지 않는 다는 것이다. 클라이언트와 파드 수가 적다면 문제가 두드러지나, 많다면 특별한 문제는 없다.11.1.9 쿠버네티스 애드온 쿠버네티스 서비스의 DNS 조회, 여러 HTTP 서비스를 단일 외부 IP 주소로 노출하는 인그레스 컨트롤러, 쿠버네티스 웹 대시보드 등이 있다.애드온 배포 방식 minikube 안에는 이런 애드온들이 레플리케이션 컨트롤러로 배포되어 있다. dns 애드온은 deployment로 배포되어있다.# rc 조회kubectl get rc -n kube-system# deployment 조회kubectl get deploy -n kube-systemDNS서버 동작 방식 클러스터의 모든 파드는 기본적으로 클러스터의 내부 DNS 서버를 사용하도록 설정되어 있다. DNS 서버 파드는 kube-dns 서비스로 노출된다. 배포된 모든 컨테이너가 가지고 있는 /etc/resolv.conf 안에 nameserver로 지정되어 있다. kube-dns 파드는 API 서버 감시 매커니즘을 이용해 모든 클라이언트가 거의 항상 최신 DNS 정보를 얻을 수 있도록 한다. 서비스 혹은 엔드포인트 리소스가 갱신되는 짧은 시간동안 유효하지 않는 타이밍이 있기는 하다. 인그레스 컨트롤러 동작 방식 리버스 프록시 서버(nginx)를 실행하고 클러스터에 정의된 인그레스, 서비스, 엔드포인트 리소스 설정을 유지한다. 인그레스 컨트롤러는 트래픽을 서비스의 IP로 보내지 않고 서비스의 파드로 직접 전달한다. 외부에서 접속한 클라이언트가 인그레스 컨트롤러로 연결할 때 IP를 보존하는데 영향을 주기 때문에 특정 사용 사례에서는 서비스를 선호하도록 만든다.다른 애드온 사용 모두 클러스터 상태를 관찰하고 변화가 생기면 그에 맞는 필요한 조치를 수행한다.11.1.10 모든 것을 함께 가져오기 쿠버네티스는 시스템 전체가 상대적으로 작고, 관심의 분리로 느슨하게 결합된 구성요소로 이뤄져있다. 또한 이런 구성요소들은 모두 함꼐 상호작용하여 실제 상태가 사용자가 지정한 원하는 상태와 일치하도록 유지시켜준다.11.2 컨트롤러가 협업하는 방법 대략적인 모습은 다음 그림과 같다.11.2.2 이벤트 체인디플로이먼트 컨트롤러가 레플리카셋 생성레플리카셋 컨트롤러가 파드 리소스 생성스케줄러가 새로 생성한 파드에 노드 할당kubelet은 파드의 컨테이너를 실행12.2.3 클러스터 이벤트 관찰 컨트롤 플레인 구성 요소와 kubelet은 이러한 작업을 수행할 때 API 서버로 이벤트를 발송한다. # event 관찰kubectl get events --watch 11.3 실행중인 파드에 관한 이해 실행중인 파드에 접근해서 컨테이너를 조회해보면 다음과 같은 내용을 확인할 수 있다.퍼즈(pause) 컨테이너 파드의 모든 컨테이너를 함꼐 담고 있는 컨테이너로, 파드의 모든 컨테이너가 동일한 네트워크와 리눅스 네임스페이스를 보유하는게 유일한 목적인 인프라스트럭쳐 컨테이너이다. 이로인해 파드의 각 컨테이너들은 인프프라스르럭처 컨테이너의 리눅스 네임스페이스를 사용할수 있다. 만약 2개의 컨테이너를 가진 파드를 실행시켰다면 결과적으로는 3개의 컨테이너가 실행되는 셈이다. 컨테이너가 종료되고 다시 시작하기 위해서는 이전과 동일한 리눅스 네임스페이스의 일부가 돼야 하기 떄문에 인프라스트럭처 컨테이너의 라이프사이클은 파드의 라이프사이클과 동일하다. 인프라스트럭처 컨테이너가 그 중간에 종료되면 kubelet이 파드의 모든 컨테이너를 다시 생성한다.11.4 파드간 네트워킹 쿠버네티스 클러스터에서는 각 파드가 고유한 Ip 주소를 가지고 다른 모든 파드와 NAT 없이 플랫 네트워크로 서로 통신할 수 있다. 이 역할은 쿠버네티스 자체가 아닌 시스템 관리자 또는 컨테이너 네트워크 인터페이스(CNI) 플러그인에 의해 ㅔㅈ공된다.11.4.1 네트워크는 어떤 모습이어야 하는가? 파드가 동일한 워커 노드에서 실행중인지 여부와 관계 없이 파드끼리 서로 통신할 수 있어야 한다. 패킷은 네트워크 주소 변환(NAT) 없이 파드 A에서 파드 B로 출발지와 목적지 주소가 변경되지 않은 상태로 도착해야 한다. 파드 내붕부에서 실행중인 애플리케이션의 네트워킹이 동일한 네트워크 스위치에 접속한 시스템에서 실행되는 것처럼 간단하고 정확하게 이뤄지도록 해주기 때문이다. 인터넷에 있는 서비스와 통신할 때는 패킷의 출발지 IP를 변경하는 것이 필요하다. 파드의 IP는 모두 사설(private)이기 때문인데, 외부로 난가는 패킷의 출발지 IP는 워커 노드의 IP로 변경된다. 이 경우에는 X-Forwarded-For HTTP header 등을 통해 client ip를 알아낼수 있다. 11.4.2 네트워킹 동작 방식 자세히 살펴보기 여기서 각 파드의 네트워크 인터페이스는 인프라스트럭처 컨테이너(퍼즈 컨테이너)로부터 설정한 것이다.동일한 노드에서 파드 간의 통신 활성화 컨테이너를 위한 가상 이더넷 인터페이스 쌍(veth쌍)이 생성된다. 이 2개의 가상 인터페이스는 파이프의 양쪽 끝과 같다. 이 이더넷 인터페이스는 브리지의 주소 범위 안에서 IP를 할당받고, 컨테이너에서 패킷을 전송할떄는 이 eth0 인터페이스로 나와서 브리지를 통해 다른쪽으로 전달된다.(브리지에 연결된 모든 네트워크 인터페이스에서 수신 가능) 노드에 있는 모든 컨테이너는 같은 브리지에 연결되어 있어서 서로 통신이 가능하다. 다른 노드에서 실행중인 컨테이너가 서로 통신하려면 노드 사이의 브리지가 서로 연결되어있으면 가능하다.서로 다른 노드에서 파드간의 통신 활성화 오버레이, 언더레이 네트워크, 일반적인 계층3 라우팅을 통해서 처리가 가능하다. 다만 파드 IP 주소는 전체 클러스터 내에서 유일해야 하기 때문에 노드 사이의 브리지는 겹치지 않는 주소 범위를 사용해서 클러스터 내 파드들이 같은 IP주소를 얻지 못하도록 한다.(이렇게 IP 충돌을 막는다.) 다른 노드로 패킷을 전송할떄는 패킷이 먼저 veth쌍을 통과하고 브리지를 통해 노드의 물리 어댑터로 전달된다. 그 다음 회선을 통해 다른 노드의 물리 어댑터로 전달되고, 노드의 브리지를 지나 목표 컨테이너의 veth쌍을 통과하게 된다. 두 노드가 라우터 없이 같은 네트워크 스위치에 연결된 경우만 동작한다. 라우터의 경우 노드간의 라우터가 늘어날수록 점점 어려워지고, 오류가 발생할 여지가 늘어나서, 소프트웨어 정의 네트워크(SDN, Software Defined Network)을 사용하는 것이 더 쉽다. SDN을 이용하면 하부 네트워크 토폴로지가 아무리 복잡해지더라도 노드들이 같은 네트워크에 연결된 것으로 볼 수 있다. 파드에서 전송한 패킷은 캡슐화돼 네트워크로 다른 파드가 실행중인 노드로 전달되고 디캡슐화 단계를 거쳐 원래 패킷 형태로 대상 파드에 전달된다.11.4.3 컨테이너 네트워크 인터페이스 소개 컨테이너를 네트워크에 쉽게 연결하기 위해 시작된 프로젝트. 쿠버네티스는 어떤 CNI 플러그인이든 설정이 가능하다. 쿠버네티스에 네트워크 플러그인 설치는 데몬셋과 다른 지원 리소스를 가지고 있는 yaml을 배포하면 된다. kubelet을 시작할 떄 –network-plugin=cni 옵셥능 주고 시작하면 노드의 CNI 인터페이스에 연결할 수 있다.CNI 플러그인 종류 Calico Flannel Romana Weave Net 그 외 기타11.5 서비스 구현 방식11.5.1 kube-proxy 소개 서비스와 관련된 모든 것은 각 노드에서 동작하는 kube-proxy 프로세스에 의해 처리된다. 초기에는 실제 프록시로서 연결을 기다리다가 들어온 연결을 위해 해당 파드로 가는 새로운 연결을 생성했었다(userspace 프록시 모드) 현재는 성능이 더 우수한 iptables 프록시 모드를 사용한다. 서비의 Ip주소는 가상이고, 서비스의 주요 핵심 사항인 서비스는 IP와 포트의 쌍으로 구성된다는것이 중요한 사항이고, 서비스 IP만으로는 아무것도 나타내지 않는다.(ping을 할 수 없는 이유)11.5.2 kube-proxy가 iptables를 사용하는 방법 서비스 리소스가 생성되면 가상 IP주소가 바로 할당되고, API 서버는 워커 노드에 실행중인 모든 kube-proxy 에이전트에 새로운 서비스가 생성됐음을 통보한다. 각 kube-proxy는 실행 중인 노드에 해당 서비스 주소로 접근할 수 있도록 만든다. 서비스의 IP/포트 쌍으로 향하는 패킷을 가로채서, 목적지 주소를 변경해 패킷이 서비스를 지원하는 여러 파드 중 하나로 리디렉션되도록 하는 몇개의 iptables 규칙을 설정하는 것. kubeproxy는 모든 엔드포인트 오브젝트를 감시한다. 처음 패킷의 목적지가 서비스의 IP와 포트로 지정되지만, 패킷이 네트워크로 전송되기 전에 노드A의 커널이 노드에 설정된 iptables 규칙에 따라 먼저 처리가 이루어지고, 임의로 선택한 파드의 IP와 포트로 교체한다.11.6 고가용성 클러스터 실행 서비스를 중단 없이 계속 실행하게 하기 위해서는 애플리케이션 뿐만 아니라 쿠버네티스 컨트롤 플레인 구성 요소도 항상 동작하고 있어야 한다. 이를 위해서는 고가용성이 필요할 수 있다.11.6.1 애플리케이션 가용성 높이기가동 중단 시간을 줄이기 위한 다중 인스턴스 실행 일반적으로 애플리케이션을 수평으로 확장할 수 있어야 하지만 수평확장할수 없는 경우라도 레플리카수를 1로 지정된 디플로이먼트를 사용하는 것이 좋다. 레플리카를 사용할수는 없지만, 문제가 생겼을때 새 레플리카로 빠르게 교체하기 위함. 수평 스케일링이 불가능한 애플리케이션을 위한 리더 선출 매커니즘 사용 중단 시간이 발생하는 것을 피하려면, 활성 복제본과 함께 비활성 복제본을 실행해두고, 빠른 임대 혹은 리더 선출 매커니즘을 이용해 단 하나만 활성화 상태로 만들어야 한다. 리더가 아닌 인스턴스들은 리더가 되는것을 기다리고 리더만 데이터를 처리한다. 그러다 리더가 문제가 생기면 대기하던 인스턴스들이 리더가 되기를 경쟁하는 방식 리더만 쓰기가 가능하고 나머지는 읽기전용으로 처리해서 사용하는 방식 경쟁 조건(race condition)으로 예측할 수 없는 시스템 동작이 발생하더라도 두 인스턴스가 같은 작업을 하지 않도록 할 수 있다. 11.6.2 쿠버네티스 컨트롤 플레인 구성 요소의 가용성 향상 컨트롤 플레인의 구성요소인 etcd, API 서버, 컨트롤러 매니저, 스케줄러의 가용성을 확장한 경우 다음과 같이 구성된다.etcd 클러스터 실행 etcd 자체가 분산 시스템으로 설게뙤어 있어서 필요한 수의 머신(3,5,7 : 홀수로 지정)에서 인스턴스를 실행하고 서로를 인식할 수 있게만 하면 된다. 모든 인스턴스에 걸쳐 데이터를 복제하기 때문에 3개의 머신으로 구성된 클러스터는 한 노드가 실패하더라도 읽기와 쓰기 작업을 모두 수행할 수 있다. 7대보다 더 크게 하는 경우 레플리케이션으로 인해 오히려 성능에 영향을 줄수도 있따.여러 APi 서버 인스턴스 실행 stateless하기 때문에 서로 인지할 필요도 없고 레플리카수를 쉽게 늘려도 된다.컨트롤러와 스케줄러의 고가용성 확보 컨트롤러 매니저나 스케줄러는 여러 인스턴스에서 동시에 실행하는 것은 어려운 일이다.(하나의 작업이 중복 수행되면 안되기 떄문에) 컨트롤러 매니저나 스케줄러 같은 구성 요소는 여러 인스턴스를 실행하기 보다는 한 번에 하나의 인스턴스만 활성화되게 해야 하고, 가용성을 위해 –leader-elect 옵션으로 제어할 수 있다. 리더만 실제로 작업을 수행하고 나머지 다른 인스턴스는 대기하면서 현재 리더가 실패할 경우를 기다린다.컨트트롤 플레인 구성 요소에서 사용되는 리더 선출 매커니즘 이해 리더를 선출하기 위해 서로 직접 대화할 필요가 없고, API 서버에 오브젝트를 생성하는 것만으로 완전히 동작시킬 수 있다. 쿠버네티스에서는 “control-plane.alpha.kubernetes.io/leader” 어노테이션의 holderIdentity 필드에 이름을 넣는데 처음 성공한 인스턴스가 리더가 되는 방식을 사용한다.(승자는 언제나 하나뿐, 낙관적 락에 의해)Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 10. 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기", "url": "/posts/devlog-platform-kubernetes-in-action10/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-09-02 17:34:00 +0900", "snippet": "10. 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기 볼륨이나 퍼시스턴트볼륨클레임에 바인딩된 퍼시스턴트볼륨을 통해서 데이터베이스를 일반 파드에 실행했었는데, 이 파드들을 스케일아웃하려면 어떻게 할수 있을지 살펴본다.10.1 스테이트풀 파드 복제하기 스테이트풀 파드를 복제할떄 레플리카셋을 이용한다면 어떨까? 레플리카셋은 하나의 파드 템플릿에서 여러 개의 파드 레플리카를 생성한다. 여러 개개의 파드 레플리카를 복제하는 데 사용하는 파드 템플릿에는 클레임에 관한 참조가 있으므로 각 레플리카가 별도의 퍼시스턴트볼륨클레임을 사용하도록 만들 수가 없음.10.1.1 개별 스토리지를 갖는 레플리카 여러 개 실행하기 이에 대한 방법은 여러가지가 있겠지만 현실성이 대부분 부족하다수동으로 파드 생성하기 레플리카셋이 파드를 감시하지 못하므로 가능한 옵션이 아니다.파드 인스턴스별로 하나의 레플리카셋 사용하기 이 경우 의도된 레플리카 수를 변경할 수 없고, 추가 레플리카셋을 생성해야 한다.(이러면 레플리카셋을 쓰는 의미가 있는가..? 없다.)동일 볼륨을 여러 개 디렉터리로 사용하기 인스턴스간 조정이 필요하고 올바르게 수행하기 쉽지 않다. 심지어 공유 스토리지 볼륨에서 병목이 발생할수도 있다.10.1.2 각 파드에 안정적인 아이덴티티 제공하기 특정 애플리케이션이 안정적인 네트워크 아이덴티티를 요구하는 이유는 무엇일까? 분산 스테이트풀 애플리케이션에서는 이러한 요구사항이 상당히 일반적이다. 이를테면 ACL을 구성하기도 하고.. 쿠버네티스에서는 매번 파드가 재스케줄링될 수 있고, 새로운 파드는 새로운 호스트 이름과 IP주소를 할당받으므로 구성 멤버가 재스케줄링될 때마다 모든 애플리케이션 클러스터가 재구성돼야 한다.각 파드 인스턴스별 전용 서비스 사용하기 개별 파드는 자신이 어떤 서비스를 통해 노출되는지 알수 없으므로 안정적인 IP를 알수 있는것도 아니고, 모든 문제를 해결할 수 조차 없다.10.2 스테이트풀셋 10.1에서 나열한 문제들을 해결할 수 있도록 쿠버네티스에서는 스테이트풀셋(StatefulSet)을 제공한다. 스테이트풀셋은 애플리케이션의 인스턴스가 각각 안정적인 이름과 상태를 가지며 개별적으로 취급돼야 하는 애플리케이션에 알맞게 만들어졌다.10.2.1 스테이트풀셋과 레플리카셋 비교애완동물 vs 가축스테이트풀셋 - 애완동물 각 인스턴스에 이름을 부여하고 개별적으로 관리한다 스테이트풀 파드가 종료되면 새로운 파드 인스턴스는 교체되는 파드와 동일한 이름, 네트워크 아이덴티티, 상태 그대로 다른 노드에서 되살아나야 한다.레플리카셋 - 가축 스테이트리스 애플리케이션의 인스턴스는 이름을 정확히 알고 있을 필요가 없고 몇마리가 있는지만 중요하다. 언제든 완전히 새로운 파드로 교체되어도 된다.10.2.2 안정적인 네트워크 아이덴티티 제공하기 스테이트풀셋으로 생성된 파드는 서수 인덱스(0부터 시작)가 할당되고 파드의 이름과 호스트 이름, 안정적인 스토리지를 붙이는 데 사용된다.거버닝 서비스 스테이트풀 파드는 때떄로 호스트 이름을 통해 다뤄져야 할 필요가 있다.(스테이트리스는 이런 니즈가 없음) 스테이트풀 파드는 각각 서로 다르므로(다른 상태를 가지거나) 요구사항에 따라 특정 스테이트풀 파드에서 동작하기를 원할수도 있다. 위와 같은 이유로 스테이트풀셋은 거버닝 헤드리스 서비스(governing headless service)를 생성해서 각 파드에게 실제 네트워크 아이덴티티를 제공해야 한다.(헤드리스 서비스를 통해 고정된 하나의 IP가 아닌 서비스에 맵핑된 모든 파드의 IP 목록을 얻는다.)스테이트풀셋 교체하기 스테이트풀셋은 레플리카셋이 하는 것과 비슷하게 새로운 인스턴스로 교체되도록 한다. 하지만 교체되더라도 이전에 사라진 파드와 동일한 호스트 이름을 갖는다. 그렇기 떄문에 파드가 다른 노드로 재스케줄링되더라도 같은 클러스터 내에서 이전과 동일한 호스트 이름으로 접근이 가능하다.스테이트풀셋 스케일링 스테이트풀셋의 스케일 다운의 좋은 점은 항상 어떤 파드가 제거될지 알수 있다는 점이다. 스테이트풀셋의 스케일 다운은 항상 가장 높은 서수 인덱스의 파드를 먼저 제거한다. 스테이트풀셋은 인스턴스 하나라도 비정상인 경우 스케일 다운 작업을 허용하지 않는다. 그 이유는 여러개 노드가 동시에 다운되는 경우 데이터를 잃을수도 있기 떄문이다.)10.2.3 각 스테이트풀 인스턴스에 안정적인 전용 스토리지 제공하기 스테이트풀 파드의 스토리지는 영구적이어야 하고 파드와는 분리되어야 한다. 스테이트풀셋의 각 파드는 별도의 퍼시스턴트볼륨을 갖는 다른 퍼시스턴트볼륨클레임을 참조해야 한다.(이를 스테이트풀셋에서 제공함.)볼륨 클레임 템플릿과 파드 템플릿을 같이 구성 스테이트풀셋이 파드를 생성하는 것과 같은 방식으로 퍼시스턴트볼륨클레임 또한 생성할 수 있다.퍼시스턴트볼륨클레임의 생성과 삭제의 이해 생성할떄는 파드와 퍼시스턴트볼륨클레임 등 2개 이상의 오브젝트가 생성이 되는데.. 스케일 다운을 할 떄는 파드만 삭제하고 클레임은 남겨둔다.(클레임이 삭제된 후 바인딩됐던 퍼시스턴트볼륨은 재활용되거나 삭제돼 콘텐츠가 손실될 수 있기 때문) 그래서 기반 퍼시스턴트볼륨을 해제하려면 퍼시스턴트볼륨클레임을 수동을 삭제해주어야 한다.동일 파드의 새 인스턴스에 퍼시스턴트볼륨클레임 다시 붙이기 스테이트풀셋은 스케일 다운되었을 때 기존에 있던 퍼시스턴트볼륨클레임을 유지했다가 스케일 업될떄 다시 해당 볼륨클레임을 연결한다.10.2.4 스테이트풀셋 보장(guarantee)안정된 아이덴티티와 스토리지의 의미 쿠버네티스상에서 보장해주지 않는다고 가정할때, 동일한 아이덴티티를 가지는 교체 파드를 생성되면 애플리케이션의 두 개 인스턴스가 동일한 아이덴티티로 시스템에서 실행하게 되면 큰 문제가 될수도 있다.스테이트풀셋 최대 하나의 의미 쿠버네티스는 두 개의 스테이트풀 파드 인스턴스가 절대 동일한 아이덴티티로 실행되지 않고, 동일한 퍼시스턴트볼륨클레임에 바인딩되지 않는것을 보장한다. 스테이트풀셋은 교체 파드를 생성하기 전에 파드가 더 이상 실행중이지 않는다는 점을 절대적으로 확신해야 처리가 이루어진다.10.3 스테이트풀셋 사용하기10.3.1 스테이트풀셋을 위해 준비사항 데이터 파일을 저장하기 위한 퍼시스턴트볼륨(동적 프로비저닝을 해도됨) 스테이트풀셋에 필요한 거버닝 헤드리스 서비스 스테이트풀셋10.3.2 스테이트풀셋 배포퍼시스턴트볼륨 생성kind: List # 여러개의 리소스를 정의할때 List를 사용할 수 있다.apiVersion: v1items:- apiVersion: v1 kind: PersistentVolume metadata: name: pv-a spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-a- apiVersion: v1 kind: PersistentVolume metadata: name: pv-b spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-b- apiVersion: v1 kind: PersistentVolume metadata: name: pv-c spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-c# 볼륨 생성kubectl create -f persistent-volumes-hostpath.yaml거버닝 서비스 생성하기 apiVersion: v1kind: Servicemetadata: name: kubiaspec: clusterIP: None # 헤드리스 서비스를 위함 selector: app: kubia ports: - name: http port: 80# 거버닝 헤드리스 서비스 생성kubectl create -f kubia-service-headless.yaml스테이트풀셋 생성하기 첫번째 파드가 생성되고 준비가 완료되어야만 두 번째 파드가 생성된다. 두 개 이상의 멤버가 동시에 생성되면 레이스 컨디션에 빠질 가능성이 있기 떄문에 스테이트풀셋은 순차적으로 하나씩만 처리된다.apiVersion: apps/v1kind: StatefulSetmetadata: name: kubiaspec: serviceName: kubia replicas: 2 selector: matchLabels: app: kubia # has to match .spec.template.metadata.labels template: metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubia-pet ports: - name: http containerPort: 8080 volumeMounts: # 볼륨 마운트 - name: data mountPath: /var/data volumeClaimTemplates: # 볼륨클레임 템플릿 - metadata: name: data spec: resources: requests: storage: 1Mi accessModes: - ReadWriteOnce# 스테이트풀셋 생성kubectl create -f kubia-statefulset.yaml# statefulset 조회kubectl get statefulset# pod 조회kubectl get po | grep kubia-# 생성된 statefulset pod 조회kubectl get po kubia-0 -o yaml10.3.3 생성된 파드 확인 파드에 피기백(exec를 통해 파드 내부로 진입해서 처리하는 방식)하거나 API 서버를 통해 데이터를 확인해본다.# api server local 프록시kubectl proxy# 파드 엔드포인트 조회curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/스테이트풀셋 파드를 삭제해 재스케줄링된 파드가 동일한 스토리지에 연결되는지 확인 새 파드는 클러스터의 어느 노드에서나 스케줄링될 수 있으며 이전 파드가 스케줄링 됐던 동일한 노드일 필요가 없다. 이전 파드의 모든 아이덴티티(이름, 호스트 이름, 스토리지)는 새 노드로 효과적으로 이동된다.# 스테이트풀셋 파드 데이터 수정curl -X POST -d &quot;Hey three! This greeting was submitted to kubia-0.&quot; localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/# 변경한 파드 삭제(삭제하면 직후 스테이트풀셋이 새로운 파드를 생성한다.)kubectl delete po kubia-0# 재확인(수정한 데이터가 그대로 남아있음.)curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/스테이트풀셋 스케일링 가장 중요한 점은 스케일 업/다운이 점진적으로 수행되며 스테이트풀셋이 초기에 생성됐을 때 개별 파드가 생성되는 방식과 유사하다는 점이다. 또한 스케일 다운시 가장 높은 서수의 파드가 먼저 삭제되고, 파드가 완전히 종료된 이후부터 다음 스케일다운이 수행된다.스테이트풀 파드를 헤드리스가 아닌 일반적인 서비스로 노출하기apiVersion: v1kind: Servicemetadata: name: kubia-publicspec: selector: app: kubia ports: - port: 80 targetPort: 8080API 서버를 통해 클러스터 내부 서비스에 연결# cluster ip service 호출curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/``## 10.4 스테이트풀셋의 피어 디스커버리 - 클러스터된 애플리케이션의 중요한 요구사항은 피어 디스커버리(클러스터의 다른 멤버를 찾는 기능)이다. - API서버와 통신해서 찾을수 있지만, 쿠버네티스의 목표 중 하나는 애플리케이션을 완전히 쿠버네티스에 독립적으로 유지하며 기능을 노출하는 것이다.### SRV 레코드 - srvlookup이라 부르는 일회용 파드(--restart=Never)를 실행하고 콘솔에 연결하며(-it) 명령어를 수행하고 종료되며, 바로 삭제된다.(--rm) - ANSWER SECTION에는 헤드리스 서비스를 뒷받침하는 두 개의 파드를 가리키는 두 개의 SRV 레코드를 확인할 수 있다. - 파드가 스테이트풀셋의 다른 모든 파드의 목록을 가져오려면 SRC DNS 룩업을 수행해서 얻을수 있다는 것이다.``` sh# srvlookup을 할수 있는 dnsutils pod을 하나 수행시켜서 명령어를 수행하고 파드 종료kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local;; ANSWER SECTION:kubia.default.svc.cluster.local. 30 IN SRV 0 50 80 kubia-0.kubia.default.svc.cluster.local.kubia.default.svc.cluster.local. 30 IN SRV 0 50 80 kubia-1.kubia.default.svc.cluster.local.;; ADDITIONAL SECTION:kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.8kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.310.4.1 DNS를 통한 피어 디스커버리 스테이트풀셋과 SRV 레코드를 사용하여 헤드리스 서비스의 모든 파드를 직접 찾는다.import dns from &#39;dns&#39;dns.resolveSrv(serviceName, (err, addresses) { // TODO 무언가 처리})10.4.2 스테이트풀셋 업데이트 편집기로 statefulset 정의 내에서 레플리카 수를 수정한다면 수정 직후 파드가 생성되는걸 확인할 수 있다. 만약 이미지를 변경한다면 디플로이먼트처럼 롱링 업데이트도 필요할텐데 쿠버네티스 1.7부터 이 기능도 지원한다.# 편집기로 statefulset 정의 열어서 수정kubectl edit statefulset kubia# pod 확인kubectl get po10.4.3 클러스터된 데이터 저장소 사용하기 SRV lookup을 통해 서비스에 포함된 모든 파드를 찾을수 있기 때문에 스테이트풀셋을 스케일 업하거나 스케일 다운하더라도 클라이언트 요청을 서비스하는 파드는 항상 그 시점에 실행중인 모든 피어를 찾을 수 있다.10.5 스테이트풀셋이 노드 실패를 처리하는 과정 스테이트풀셋은 노드가 실패한 경우 동일한 아이덴티티와 스토리지를 가진 두 개의 파드가 절대 실행되지 않는 것을 보장하므로, 스테이트풀셋은 파드가 더 이상 실행되지 않는다는 것을 확실할 때까지 대체 파드를 생성할 수 없으며, 생성해서도 안된다. 이 경우 오직 클러스터 관리자가 알려줘야만 알 수 있고, 이를 위해 관리자는 파드를 삭제하거나 전체 노드를 삭제해야 한다.(노드 삭제시 노드에 스케줄링된 모든 파드가 삭제됨) 노드가 다운된 상태에서 파드를 삭제하게 되면 쿠버네티스 클러스터(마스터) 기준으로는 파드가 삭제됐으나, 다운된 노드에서는 이를 알 방법이 없기 때문에 실제 노드에 스케줄링된 파드가 삭제되지는 않는다. 이 경우 파드를 강제로 삭제해야 할 수 있다. 노드가 더 이상 실행중이 아니거나 연결 불가함을 아는 경우가 아니라면, 스테이트풀 파드를 강제로 삭제해서는 안된다.(영구적으로 유지되는 좀비 파드가 생성될수 있다.)Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 9. 디플로이먼트 : 선언적 애플리케이션 업데이트", "url": "/posts/devlog-platform-kubernetes-in-action9/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-30 17:34:00 +0900", "snippet": "9. 디플로이먼트 : 선언적 애플리케이션 업데이트 쿠버네티스 클러스터에서 실행되는 애플리케이션을 업데이트 하는 방법과 쿠버네티스가 어떻게 무중단 업데이트 프로세스로 전환하는 데 도움을 주는지 살펴본다.9.1 파드에서 실행중인 애플리케이션 업데이트 쿠버네티스에서 실행되는 애플리케이션 기본 구성은 아래와 같다. 여기서 파드에서 실행중인 컨테이너 이미지 버전을 업데이트한다고 할때 어떻게 해야할까?모든 파드를 업데이트 하는 방법 기존 파드를 모두 삭제한 다음 새 파드를 시작한다. 새로운 파드를 시작하고, 기동하면 기존 파드를 삭제한다.9.1.1 오래된 파드를 삭제하고 새 파드로 교체 (v1 -&amp;gt; v2로 업데이트한다고 했을때) v1 파드 세트를 관리하는 레플리카셋이 있는 경우 이미지의 버전 v2를 참조하도록 파드 템플릿을 수정한 다음 이전 파드 인스턴스를 삭제해 쉽게 교체할 수 있을것이다.9.1.2 새 파드 기동과 이전 파드 삭제 한 번에 여러 버전의 애플리케이션이 실행하는 것을 지원하는 경우(다른 버전의 애플리케이션이 같이 서빙되어도 문제가 없는 경우) 새 파드를 모두 기동한 후 이전 파드를 삭제할 수 있다. 잠시동안 동시에 두 배의 파드가 실행되므로 더 많은 하드웨어 리소스가 필요하다.한 번에 이전 버전에서 새 버전으로 전환 새 버전을 실행하는 파드를 불러오는 동안 서비스는 파드의 이전 버전에 연결된다. 새 파드가 모두 실행되면 서비스의 레이블 셀렉터를 변경하고 서비스를 새 파드로 전환할 수 있다.(블루-그린 디플로이먼트) kubectl set selector 명령어를 사용해 서비스의 파드 셀렉터 변경이 가능롤링 업데이트 수행 이전 파드를 한번에 삭제하는 방법 대신 파드를 단계적으로 교체하는 롤링 업데이트를 수행할 수도 있다. 2개의 레플리카셋을 이용해서 상태를 보아가면서 수행할수도 있겠지만, 쿠버네티스에서는 하나의 명령으로 롱링 업데이트를 수행할 수 있다.9.2 레플리케이션컨트롤러로 자동 롤링 업데이트 수행하나의 YAML에 여러개의 쿠버네티스 리소스를 정의하는 방법 ---(대시 3개)를 구분자로 여러 리소스 정의를 포함할 수 있다.``` yamlapiVersion: v1kind: ReplicationControllermetadata: name: kubia-v1spec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: image: luksa/kubia:v1name: nodejs— # YAML 구분자apiVersion: v1kind: Servicemetadata: name: kubiaspec: type: LoadBalancer selector: app: kubia ports: port: 80targetPort: 8080```9.2.2 kubectl을 이용한 롤링 업데이트# v1 rc and service 생성kubectl create -f kubia-rc-and-service-v1.yaml# v2 롤링 업데이트 수행kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 rolling-update 명령어를 수행하면 일단 kubia/v2에 대한 rc가 만들어지면서 롤링 업데이트가 수행된다.롤링 업데이트 과정동일한 이미지 태그로 업데이트 푸시하기 워커 노드에서 일단 이미지를 한번 가져오면 이미지는 노드에 저장되고, 동일한 이미지를 사용해 새 파드를 실행할 때 이미지를 리모트에서 다시 가져오지 않는다.(도커 이미지 기본 정책) 즉 이미지의 변경한 내용을 같은 이미지 태그로 푸시하더라도 이미지가 변경되지 않는다. 이런 일을 해결할 수 있는 방법으로 컨테이너의 imagePullPolicy속성을 Always로 설정하면 가져올 수 있다. 이미지의 latest 태그를 참조하는 경우에는 imagePullPolicy의 기반값은 always로 항상 리모트에서 가져오지만, 다른 태그인 경우에는 기본 정책인 ifNotPresnet이다. 가장 좋은 방법은 이미지를 변경할 때마다 새로운 태그로 지정하는 방식이다.롤링 업데이트가 시작되기 전 kubeclt이 수행한 단계 이해하기 롤링 업데이트 프로세스는 첫 번째 rc의 셀렉터도 수정한다. 레이블에 deployment라는 키가 추가되고 value로 hashValue가 추가된다. v2에는 다른 value를 가진 deployment 가 추가된다.# rc kubia-v1 상태 확인kubectl describe rc kubia-v1# pod label 확인kubectl get po --show-labels레플리케이션컨트롤러 두 개를 스케일업해 새 파드로 교체 service selector는 app=kubia로만 참조되므로, rc에서 하나씩 scale up / scale down이 일어나면서 파드가 교체되고, 롤링 업데이트가 이루어진다. 롤링 업데이트를 계속하면 v2 파드에 대한 요청 비율이 점점 더 높아지기 시작한다. 마지막 v1 파드가 삭제되고, 서비스가 이제 v2 파드에 의해서만 지원하게 되는데, 이때 kubectl은 v1 rc를 삭제하고 업데이트 프로세스가 완료된다.Scaling kubia-v2 up to 1Scaling kubia-v1 down to 29.2.3 kubectl rolling-update를 더 이상 사용하지 않는 이유1) 스스로 만든 오브젝트를 쿠버네티스가 수정하기 떄문에 클러스터 개발자가 등록한 매니페스트를 무시하고 쿠버네티스가 변경한다.(deployment label 같은 경우)2) 롤링 업데이트를 수행하는 레벨이 클라이언트에서 이루어지기 때문 --v 옵션을 사용해 자세한 로딩을 켜면 이를 확인할 수 있다. kubectl 클라이언트가 쿠버네티스 마스터 대신 스케일링을 수행하는 중임을 볼수 있다. 서버가 아닌 클라이언트가 업데이트 프로세스를 수행하면 왜 문제일까? 업데이트를 수행하는 동안 네트워크 연결이 끊어지는 경우, 중간 상태로 프로세스가 종료될수 있기 떄문에 리스크가 있다. 3) rolling-update 자체가 명령(imperative)을 나타내기 떄문 쿠버네티스에 파드를 추가하거나 초과된 파드를 제거하라고 지시하지 마라(대신 레플리카 수를 변경하여 쿠버네티스 서버가 알아서 하도록 맡겨야 한다.)9.3 애플리케이션을 선언적으로 업데이트 하기 위한 디플로이먼트 사용하기 낮은 수준의 개념으로 간주되는 RC, RS을 통해 수행하는 대신 애플리케이션을 배포하고 선언적으로 업데이트 하기 위한 높은 수준의 리소스 디플로이먼트를 생성하면 레플리카셋 리소스가 그 아래에 생성된다. 애플리케이션 업데이트할 때는 추가 레플리케이션컨트롤러를 도입하고, 두 컨트롤러가 잘 조화하도록 조정해야 하는데, 이를 전체적으로 통제하는 것이 디플로이먼트이다.9.3.1 디플로이먼트 생성 디플로이먼트는 레이블 셀렉터, 원하는 레플리카수, 파드 템플릿으로 구성된다. 리소스가 수정될 때 업데이트 수행 방법을 정의하는 디폴로이먼트 전략을 지정할수 도 있음.디플로이먼트 매니페스트 생성apiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs selector: matchLabels: app: kubia# rc 제거kubectl delete rc --all# deployment 생성# --recode 명령줄을 포함시켜 개정 이력(revision history)에 기록하여야 한다.kubectl create -f kubia-deployment-v1.yaml --record# deployment 조회kubectl get deploy# deployment에 의해 생성된 rc 조회kubectl get rs# deployment, rs에 의해 생성된 pod 조회ubectl get po | grep kubia-59d857디플로이먼트 롤아웃 상태 출력# 롤아웃 상태 출력kubectl rollout status deploy kubia# pod 확인kubectl get po디플로이먼트가 레플리카셋을 생성하는 방법과 레플리카셋이 파드를 생성하는 방식 이해 컨트롤러 이름과 임의로 생성된 문자열로 구성된다. 디플로이먼트에서 생성한 파드 3개에는 이름 중간에 숫자 값이 추가로 포함된다. 이 숫자 값은 파드 템플릿의 해시값을 나타낸다.# 레플리카셋 조회kubectl get rs 디플로이먼트에서 생성된 레플리카셋의 이름을 보면 여기에도 파드 템플릿의 해시값이 포함되어 있다. 디플로이먼트는 파드 템플릿의 각 버전마다 하나씩 여러 개의 레플리카셋을 만든다. 이 파드 템플릿의 해시 값을 사용하면 디플로이먼트에서 지정된 버전의 파드 템플릿에 관해 항상 동일한 레플리카셋을 사용할 수 있다.9.3.2 디플로이먼트 업데이트 디플로이먼트 리소스에 정의된 파드 템플릿을 수정하기만 하면 쿠버네티스가 실제 시스템 상태를 리소스에 정의된 상태로 만드는 데 필요한 모든 단계를 수행한다.사용 가능한 디플로이먼트 전략 기본 전략은 RollingUpdate 전략이다. RollingUpdate 전략은 이전 버전과 새 버전을 동시에 실행할 수 있는 경우에만 사용해야 한다. 대안으로 존재하는 Recreate 전략은 한 번에 기존 모든 파드를 삭제한 뒤 새로운 파드를 만드는 전략이다. 이는 앱이 여러 버전을 병렬로 실행하는 것을 지원하지 않고 새 버전을 시작하기 전에 이전 버전을 완전히 중지해야 하는 경우 사용할 수 있는데, 짧게 서비스 다운타임이 발생하는 문제가 있다.롤링 업데이트 속도 느리게 하기 디플로이먼트의 minReadySeconds 속성을 설정하여 롤링 업데이트 속도를 느리게 만들 수 있다.# deployment spec 수정kubectl patch deployment kubia -p &#39;{&quot;spec&quot;:{&quot;minReadySeconds&quot;:10}}&#39;# deployment 상태 확인kubectl get deploy -o yaml롤링 업데이트 시작 kubectl set image 명령어를 사용해 컨테이너가 포함된 모든 리소스(rc, rs, deployment 등등)을 수정할 수 있다.# image 변경kubectl set image deployment kubia nodejs=luksa/kubia:v2 --record# deployment 상태 확인kubectl get deploy -o yaml depolyment의 파드 템플릿이 업데이트돼 nodejs 컨테이너에 사용된 이미지가 kubia:v2로 변경된다.디플로이먼트와 그 외의 리소스를 수정하는 방법 명령 설명 example kubectl edit 기본 편집기로 수정 kubectl edit deploy kubia kubectl patch 오브젝트의 개별 속성 수정 kubectl patch deployment kubia -p ‘{“spec”:{“minReadySeconds”:10}}’ kubectl apply 전체 yaml/json 파일의 속성 값을 적용해 오브젝트를 수정 kubectl apply -f kubia-deployment-v2.yaml kubectl replace yaml / json 파일로 오브젝트를 새것으로 교체 kubectl replace -f kubia-deployment-v2.yaml kubectl set image 정의된 컨테이너 이미지 변경 kubectl set image deployment kubia nodejs=luksa/kubia:v2 디플로이먼트의 놀라움 파드 템플릿을 변경하는 것만으로 애플리케이션을 최신 버전으로 업데이트할 수 있음. 디플로이먼트의 파드 템플릿이 컨피그맵(또는 시크릿)을 참조하는 경우 컨피그맵은 수정하더라도 업데이트를 시작하지 않는다.(단, 새 컨피그맵을 만들고 파드 템플릿이 새 컨피그맵을 참조하도록 수정하면 업데이트가 수행된다.) 여기서 중요한 사실 중 하나는 기존 rs도 여전히 남아있는다는 것이다. 이는 롤백이나 이런 부분에서 재사용될 수 있음. 단일 디플로이먼트 오브젝트를 관리하는 것이 여러 레플리케이션 컽느롤러를 처리하고 추적하는것보다 훨씬 쉬움# rs 조회 ( 2개가 그대로 남아있는것을 알 수 있다.)kubectl get rs9.3.3 디플로이먼트 롤백# 이미지 변경kubectl set image deploy kubia nodejs=luksa/kubia:v3 --record# rollout 상태 확인kubectl rollout status deploy kubia롤아웃 되돌리기 업데이트 된 v3가 에러를 발생하기 시작할떄 롤백을 수행할 수 있다. 롤아웃 프로세스가 진행중인 동안에도 롤아웃을 중단하려면 실행 취소 명령을 사용해서 중단시킬 수 있다. (롤아웃 중에 생성된 파드는 제거되고 이전 파드로 다시 교체된다.)# 이전 버전으로 롤백kubectl rollout undo deploy kubia디플로이먼트 롤아웃 이력 표시 롤아웃 이력에 포함시키려면 –record 명령줄을 포함시켜야만 한다.# 롤아웃 이력 표시kubectl rollout history deploy kubia특정 디플로이먼트 개정으로 롤백 개정(revision) 번호를 지정해 특정 개정으로 롤백할 수 있다. 디플로이먼트를 처음 수정했을 때 비활성화된 레플리카셋이 남아있던 이유는 이 롤백을 위함이다. 모든 개정 내역의 수는 디폴로이먼트 리소스의 editionHistoryLimit 속성에 의해 제한된다.(쿠버네티스 버전이 올라가면서 revisionHistoryLimit로 변경된듯 하다) 현재 쿠버네티스 버전 기준으로 revisionHistoryLimit의 기본값은 10이다. https://kubernetes.io/ko/docs/concepts/workloads/controllers/deployment/# 1번 revision으로 롤백kubectl rollout undo deployment kubia --to-revision=19.3.4 롤아웃 속도 제어 롤링 업데이트 전략의 두 가지 추가 속성을 통해 새 파드를 만들고 기존 파드를 삭제하는 과정에서 속도를 제어할 수 있다.롤링 업데이트 전략의 maxSurge와 maxUnavailable 속성 소개 이 2개의 속성에 의해서 한 번에 몇개의 파드를 교체할지를 결정된다. maxSurge : 레플리카 수보다 얼마나 많은 파드 인스턴스 수를 허용할지를 나타낸다.(기본값 : 25%), 만약 레플리카수가 4로 설정한 경우 4의 25%인 1개만큼 더 허용될수 있다. 즉 5개의 파드까지 생성될 수 있음을 의미한다. 백분위로 설정하기 떄문에 이 값은 반올림 처리 된다. maxUnavailable : 업데이트 중에 의도하는 레플리카 수를 기준으로 사용할 수 없는 파드 인스턴스 수(기본값 25%), 레플리카수가 4인 경우 25%인 1개만큼 unavailable되는것을 허용한다. 업데이트 과정에서 사용할 수 없는 파드는 최대 1개여야 한다. 즉 최소 3개의 파드는 항상 가용한 상태를 유지하면서 롤링 업데이트가 수행되어야 한다는 의미이다. 이 백분위 값을 처리할떄는 내림으로 처리된다.apiVersion: apps/v1beta1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate 위 에제에서 레플리카수가 3이고, maxSurge가 1이고, maxUnavailable을 0으로 설정한 경우에는 다음과 같이 동작한다. 모든 파드수는 4개가 되는 것까지 허용했으며, maxUnavailable은 0이기 떄문에 available한 파드는 레플리카수와 동일한 3개여야 한다.(항상 3개의 파드를 사용할 수 있어야 한다.)maxUnavailable 속성 이해 위 예제에서 maxUnavailable만 1로 변해보면 어떻게 동작할까? 레플리카수는 3개인데 maxUnavailable이 1이기 떄문에, 롤링 업데이트 과정에서 최소 2개의 파드가 사용가능한 상황을 허용한다. 최대 동시에 실행될 수 있는 파드는 기존과 동일하게 4개이고, 2개가 가용하지 않아도 된다. 위 예제와 비교했을떄 maxUnavailable=1로 인해서 2개까지만 가용한 상태면 되기 때문에 결과적으로는 롤링 업데이트가 더 빠르게 수행될수 있다.9.3.5 롤아웃 프로세스 일시 중지 롤아웃 프로세스를 일시정지해서 일종의 카나리 릴리스를 실행할 수가 있다.(카나리 릴리스를 하는데 아주 좋은 방법은 아닌듯 하다.) 정확히 내가 원하는 레플리카수를 보장하기 힘들다. 타이밍 이슈가 발생하기 떄문에 좋은 방법이 아니다.(책의 저자가 아닌 글 작성자의 개인적인 생각) n개의 파드가 있을 때 v4의 파드는 하나만 구동시키고 나머지는 기존 버전으로 구동시킨다.(이러한 방식으로 사용자들에게 영향을 최소화하면서 변경된 로직이 문제없는지 체크하는 방식)# v4로 버전 변경kubectl set image deploy kubia nodejs=luksa/kubia:v4 --record# 롤아웃 일시 정지kubectl rollout pause deploy kubia# pod 확인kubectl get po 위 파드 목록을 보면 v4로 구동된 파드 ( kubia-586b45dbdc-5cgc6) 1개가 추가로 구동중인것을 볼수 있다.롤아웃 재개 새 버전이 제대로 작동한다고 확신하면 디플로이먼트를 다시 시작해 이전 파드를 모두 새 파드로 교체할 수 있다. 책이 쓰여진 시점 기준, 카나리 릴리스를 수행하는 적절한 방법은 두 가지 다른 디플로이먼트를 사용해 적절하게 확장하는 것이다.# 롤아웃 재개kubectl rollout resume deploy kubia롤아웃을 방지하기 위한 일시 중지 기능 사용 일시 중지 기능을 사용하면 롤아웃 프로세스가 시작돼 디플로이먼트를 업데이트하는 것을 막을 수 있고, 여러 번 변경하면서 필요한 모든 변경을 완료한 후에 롤아웃을 시작하도록 할 수 있다.9.3.6 잘못된 버전의 롤아웃 방지 minReadySeconds 속성으로 롤아웃 속도를 늦춰 롤링 업데이트 과정을 직접 볼수 있는데, 이 기능은 오작동 버전의 배포를 방지하는 목적으로도 사용할 수 있다.minReadySeconds의 적용 가능성 이해 minReadySeconds는 파드를 사용 가능한 것으로 취급하기 전에 새로 만든 파드를 준비할 시간을 지정하는 속성이다. 이것과 레디니스 프로브를 함께 이용하여 오작동 버전의 롤아웃을 효과적으로 차단할 수 있다. 모든 파드의 레디니스 프로브가 성공하면 파드가 준비상태가 되는데, minReadySeconds가 지나기전에 레디니스 프로브가 실패하기 시작하면 새 버전의 롤아웃은 차단이 된다. 적절하게 구성된 레디니스 프로브와 적절한 minReadySeconds 설정으로 쿠버네티스는 버그가 있는 버전을 배포하지 못하게 할 수 있다.적절한 minReadySeconds와 레디니스 프로브 정의apiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 minReadySeconds: 10 # minReadySeconds를 10초로 설정 selector: matchLabels: app: kubia strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 # 디플로이먼트가 파드를 하나씩만 교체하도록 0으로 설정 type: RollingUpdate template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v3 name: nodejs readinessProbe: # 레디니스 프로브 정의 periodSeconds: 1 # 매 초마다 레디니스 프로브 수행 httpGet: path: / port: 8080kubectl apply 를 통한 deploy 업데이트 apply를 통해 업데이트할 때 원하는 레플리카 수를 변경하지 않으려면 replicas 필드를 포함시키면 안된다.# kubectl apply 를 통한 deploy 업데이트kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml# 롤아웃 상태 확인kubectl rollout status deploy kubia레디니스 프로브가 잘못된 버전으로 롤아웃되는 것을 방지하는 법 잘못된 버전은 레디니스 프로브 단계에서 차단되어 파드가 생성되지 않는다. rollout status 명령어느 하나의 새 레플리카만 시작됐음을 보여준다. 사용 가능한 것으로 간주되려면 10초 이상 준비돼 있어야 하기 때문에 해당 파드가 사용 가능할 때까지 롤아웃 프로세스는 새 파드를 만들지 않는다. 여기서 maxUnavailable 속성이 0으로 설정되었기 때문에 원래 파드도 제거되지 않는다. 만약 위 상황에서 minReadySeconds를 짧게 설정했더라면 레디니스 프로브의 첫 번쨰 호출이 성공한 후 즉시 새 파드가 사용 가능한것으로 간주해버릴 수도 있다. 그러면 잘못된 버전으로 롤아웃이 일어나기 때문에 이 값을 적절하게 잘 설정해야 한다.롤아웃 데드라인 설정 기본적으로 쿠버네티스에서는 롤아웃이 10분동안 진행되지 않으면 실패한 것으로 간주된다. 이 값은 progressDeadlineSeconds 속성을 통해 설정할 수 있다. progressDeadlineSeconds에 지정된 시간이 초과되면 롤아웃이 자동으로 중단된다.# deploy 정보 확인kubectl describe deploy kubia잘못된 롤아웃 중지# 롤아웃 중지kubectl rollout undo deployment kubiaReference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 8. 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 엑세스하기", "url": "/posts/devlog-platform-kubernetes-in-action8/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-23 17:34:00 +0900", "snippet": "8. 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 엑세스하기 Downward API사용방법과 쿠버네티스 REST API 사용방법, 인증과 서버 검증을 kubectl proxy에 맡기는 방법, 컨테이너 내에서 API 서버에 접근하는 방법, 앰배서더 컨테이너 패턴의 이해, 쿠버네티스 클라이언트 라이브러리 사용방법 등을 살펴본다. 특정 파드와 컨테이너 메타데이터를 컨테이너로 전달하는 방법과 컨테이너 내에서 실행중인 애플리케이션이 쿠버네티스 API 서버와 통신해 클러스터에 배포된 리소스의 정보를 얻는 것이 얼마나 쉬운지, 이런 리소스를 생하거나 수정하는 방법을 살펴보자. 8.1 Downward API로 메타데이터 전달 파드의 IP, 호스트 노드 이름, 파드 자체의 이름과 같이 실행 시점까지 알려지지 않은 데이터는 어떻게 얻어와야할까? 이러한 정보들을 여러 곳에서 반복해서 설정하는건 말이 안된다. 위 2가지 문제는 쿠버네티스의 Downward API를 사용하면 해결할 수 있다. Downward API는 애플리케이션이 호출해서 데이터를 가져오는 REST 엔드포인트와는 다르다. 파드 매니페스트에 정의한 메타 데이터를 기준으로 volume을 정의하고 이를 환경변수에 할당하여 사용할 수 있다.8.1.1 사용 가능한 메타데이터 이해 Downward API를 사용하면 파드 자체의 메타데이터를 해당 파드 내에서 실행중인 프로세스에 노출시킬 수 있다. 이러한 데이터는 OS로 직접 얻을수도 있겠지만, Downward API는 더 간단한 대안을 제공한다.메타데이터 종류 파드의 이름 파드의 IP 주소 파드가 속한 네임스페이스 파드가 실행중인 노드의 이름 파드가 실행중인 서비스 어카운트 이름 ( 일단 파드가 API 서버와 통신할 때 인증하는 계정 정도로 이해하면 된다.) 각 컨테이너의 CPU와 메모리 요청 각 컨테이너의 CPU와 메모리 제한 파드의 레이블 파드의 어노테이션8.1.2 환경변수로 메타데이터 노출하기 환경변수로 파드와 컨테이너의 메타데이터를 컨테이너에 전달하는 방법apiVersion: v1kind: Podmetadata: name: downwardspec: containers: - name: main image: busybox command: [&quot;sleep&quot;, &quot;9999999&quot;] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 20Mi # 메모리 사이즈 4Mi로는 파드가 안뜨고 20으로 올려줘야 정상동작( https://github.com/kubernetes/minikube/issues/6160 ) env: - name: POD_NAME # 특정 값을 설정하는 대신 파드 매니페스트의 metadata.name을 참조 valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: CONTAINER_CPU_REQUEST_MILLICORES valueFrom: resourceFieldRef: # 컨테이너의 CPU/메모리 요청과 제한은 fieldRef 대신 resourceFieldRef를 사용해 참조 resource: requests.cpu divisor: 1m # 리소스 필드의 경우 필요한 단위의 값을 얻으려면 제수(divisor)을 정의한다. - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Ki 제수는 어떤 수를 나누는 수라는 뜻으로 위에서 CPU 메모리 요청의 용량 단위# downward API를 사용하는 pod 생성kubectl create -f downward-api-env.yaml# pod의 env 조회kubectl exec downward envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=downwardPOD_NAMESPACE=defaultPOD_IP=172.17.0.4NODE_NAME=minikubeSERVICE_ACCOUNT=defaultCONTAINER_CPU_REQUEST_MILLICORES=15CONTAINER_MEMORY_LIMIT_KIBIBYTES=20480POD_NAME=downwardKUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_PORT=443KUBIA_PORT=tcp://10.102.194.76:80KUBIA_PORT_80_TCP_ADDR=10.102.194.76KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1KUBIA_SERVICE_HOST=10.102.194.76KUBIA_PORT_80_TCP=tcp://10.102.194.76:80KUBERNETES_PORT=tcp://10.96.0.1:443KUBIA_SERVICE_PORT=80KUBIA_PORT_80_TCP_PORT=80KUBERNETES_SERVICE_HOST=10.96.0.1KUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_PORT_HTTPS=443KUBIA_PORT_80_TCP_PROTO=tcpHOME=/root8.1.3 downwardAPI 볼륨에 파일로 메타데이터 전달 환경변수 대신 파일로 메타데이터를 노출하려는 경우 downwwardAPI 볼륨을 정의해서 컨테이너에 마운트할 수 있다. downward라는 볼륨을 정의하고 컨테이너의 /etc/downward 아래에 마운트하는 예제apiVersion: v1kind: Podmetadata: name: downward labels: # 이 레이블과 어노테이션은 downwardAPI 볼륨으로 노출된다. foo: bar annotations: key1: value1 key2: | multi line valuespec: containers: - name: main image: busybox command: [&quot;sleep&quot;, &quot;9999999&quot;] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 20Mi volumeMounts: # downward 볼륨 /etc/downward에 마운트 - name: downward mountPath: /etc/downward volumes: - name: downward # downwardAPI 볼륨 정의 downwardAPI: items: - path: &quot;podName&quot; # metadata.name에 정의한 이름은 podName 파일에 기록된다. fieldRef: fieldPath: metadata.name - path: &quot;podNamespace&quot; fieldRef: fieldPath: metadata.namespace - path: &quot;labels&quot; fieldRef: fieldPath: metadata.labels - path: &quot;annotations&quot; fieldRef: fieldPath: metadata.annotations - path: &quot;containerCpuRequestMilliCores&quot; resourceFieldRef: containerName: main resource: requests.cpu divisor: 1m - path: &quot;containerMemoryLimitBytes&quot; resourceFieldRef: containerName: main resource: limits.memory divisor: 1# pod 생성kubectl create -f downward-api-volume.yaml# pod 데이터 확인kubectl exec downward -- ls -al /etc/downward/kubectl exec downward -- cat /etc/downward/labelskubectl exec downward -- cat /etc/downward/annotations레이블과 어노테이션 업데이트 파드가 실행되는 동안 레이블, 어노테이션 값을 업데이트될 수 있다. downwardAPI 볼륨을 이용하는 경우에는 업데이트시에도 최신 데이터를 볼수 있다. 환경변수를 사용하는 경우에는 나중에 업데이트할 수 없다.볼륨 스펙에서 컨테이너 수준의 메타데이터 참조 리소스 제한 또는 요청(resourceFieldRef)과 같은 컨테이너 수준의 메타데이터를 노출하는 경우 리소스 필드를 참조하는 컨테이너의 이름을 필수로 지정해야 한다.(컨테이너가 하나인 파드에서도 필수 지정) 볼륨이 컨테이너가 아니라 파드 수준에서 정의되었지만, 리소스 제한은 컨테이너 기준이기 때문 환경변수를 사용하는 것보다 약간 더 복잡하지만 필요할 경우 한 컨테이너의 리소스 필드를 다른 컨테이너에 전달할 수 있는 장점이 있다. 환경변수로는 컨테이너 자신의 리소스 제한과 요청만 전달할 수 있다.spec: volumes: - name: downward downwardAPI: items: - path: &quot;containerCpuRequestMilliCores&quot; resourceFieldRef: containerName: main # 컨테이너 이름이 필수로 지정되어야 한다. resource: requests.cpu divisor: 1mDownward API 사용 시기 이해 Downward API를 사용하면 애플리케이션은 쿠버네티스에 독립적으로 유지할 수 있게 한다.(기존에 환경변수의 특정 데이터를 활용하고 있는 경우 유용할 수도 있다.) Downward API로 가져올 수 없는 다른 데이터들이 필요한 경우 쿠버네티스 API를 통해 가져와야 한다.8.2 쿠버네티스 API 서버와 통신하기 Downward API는 단지 파드 자체의 메타데이터와 모든 파드의 데이터 중 일부만 노출한다. 애플리케이션에서 클러스터에 정의된 다른 파드나 리소스에 대한 정보가 필요한 경우도 있는데 이 경우에는 쿠버네티스 API를 이용해야 한다.8.2.1 쿠버네티스 REST API 살펴보기# 쿠버네티스 클러스터 정보 조회kubectl cluster-infoKubernetes master is running at https://192.168.64.2:8443KubeDNS is running at https://192.168.64.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy# 호출 ( Forbidden 403 에러)curl https://192.168.64.2:8443 -kkubectl proxy로 API 서버 엑세스하기# proxy 실행kubectl proxy# local proxy 호출curl http://localhost:8001{ &quot;paths&quot;: [ &quot;/api&quot;, &quot;/api/v1&quot;, # 대부분의 리소스 타입을 여기서 확인 &quot;/apis&quot;, &quot;/apis/&quot;, &quot;/apis/admissionregistration.k8s.io&quot;, &quot;/apis/admissionregistration.k8s.io/v1&quot;, &quot;/apis/admissionregistration.k8s.io/v1beta1&quot;, &quot;/apis/apiextensions.k8s.io&quot;, &quot;/apis/apiextensions.k8s.io/v1&quot;, &quot;/apis/apiextensions.k8s.io/v1beta1&quot;, &quot;/apis/apiregistration.k8s.io&quot;, &quot;/apis/apiregistration.k8s.io/v1&quot;, &quot;/apis/apiregistration.k8s.io/v1beta1&quot;, &quot;/apis/apps&quot;, &quot;/apis/apps/v1&quot;, &quot;/apis/authentication.k8s.io&quot;, &quot;/apis/authentication.k8s.io/v1&quot;, &quot;/apis/authentication.k8s.io/v1beta1&quot;, &quot;/apis/authorization.k8s.io&quot;, &quot;/apis/authorization.k8s.io/v1&quot;, &quot;/apis/authorization.k8s.io/v1beta1&quot;, &quot;/apis/autoscaling&quot;, &quot;/apis/autoscaling/v1&quot;, &quot;/apis/autoscaling/v2beta1&quot;, &quot;/apis/autoscaling/v2beta2&quot;, &quot;/apis/batch&quot;, &quot;/apis/batch/v1&quot;, &quot;/apis/batch/v1beta1&quot;, &quot;/apis/certificates.k8s.io&quot;, &quot;/apis/certificates.k8s.io/v1beta1&quot;, &quot;/apis/coordination.k8s.io&quot;, &quot;/apis/coordination.k8s.io/v1&quot;, &quot;/apis/coordination.k8s.io/v1beta1&quot;, &quot;/apis/discovery.k8s.io&quot;, &quot;/apis/discovery.k8s.io/v1beta1&quot;, &quot;/apis/events.k8s.io&quot;, &quot;/apis/events.k8s.io/v1beta1&quot;, &quot;/apis/extensions&quot;, &quot;/apis/extensions/v1beta1&quot;, &quot;/apis/networking.k8s.io&quot;, &quot;/apis/networking.k8s.io/v1&quot;, &quot;/apis/networking.k8s.io/v1beta1&quot;, &quot;/apis/node.k8s.io&quot;, &quot;/apis/node.k8s.io/v1beta1&quot;, &quot;/apis/policy&quot;, &quot;/apis/policy/v1beta1&quot;, &quot;/apis/rbac.authorization.k8s.io&quot;, &quot;/apis/rbac.authorization.k8s.io/v1&quot;, &quot;/apis/rbac.authorization.k8s.io/v1beta1&quot;, &quot;/apis/scheduling.k8s.io&quot;, &quot;/apis/scheduling.k8s.io/v1&quot;, &quot;/apis/scheduling.k8s.io/v1beta1&quot;, &quot;/apis/storage.k8s.io&quot;, &quot;/apis/storage.k8s.io/v1&quot;, &quot;/apis/storage.k8s.io/v1beta1&quot;, &quot;/healthz&quot;, &quot;/healthz/autoregister-completion&quot;, &quot;/healthz/etcd&quot;, &quot;/healthz/log&quot;, &quot;/healthz/ping&quot;, &quot;/healthz/poststarthook/apiservice-openapi-controller&quot;, &quot;/healthz/poststarthook/apiservice-registration-controller&quot;, &quot;/healthz/poststarthook/apiservice-status-available-controller&quot;, &quot;/healthz/poststarthook/bootstrap-controller&quot;, &quot;/healthz/poststarthook/crd-informer-synced&quot;, &quot;/healthz/poststarthook/generic-apiserver-start-informers&quot;, &quot;/healthz/poststarthook/kube-apiserver-autoregistration&quot;, &quot;/healthz/poststarthook/rbac/bootstrap-roles&quot;, &quot;/healthz/poststarthook/scheduling/bootstrap-system-priority-classes&quot;, &quot;/healthz/poststarthook/start-apiextensions-controllers&quot;, &quot;/healthz/poststarthook/start-apiextensions-informers&quot;, &quot;/healthz/poststarthook/start-cluster-authentication-info-controller&quot;, &quot;/healthz/poststarthook/start-kube-aggregator-informers&quot;, &quot;/healthz/poststarthook/start-kube-apiserver-admission-initializer&quot;, &quot;/livez&quot;, &quot;/livez/autoregister-completion&quot;, &quot;/livez/etcd&quot;, &quot;/livez/log&quot;, &quot;/livez/ping&quot;, &quot;/livez/poststarthook/apiservice-openapi-controller&quot;, &quot;/livez/poststarthook/apiservice-registration-controller&quot;, &quot;/livez/poststarthook/apiservice-status-available-controller&quot;, &quot;/livez/poststarthook/bootstrap-controller&quot;, &quot;/livez/poststarthook/crd-informer-synced&quot;, &quot;/livez/poststarthook/generic-apiserver-start-informers&quot;, &quot;/livez/poststarthook/kube-apiserver-autoregistration&quot;, &quot;/livez/poststarthook/rbac/bootstrap-roles&quot;, &quot;/livez/poststarthook/scheduling/bootstrap-system-priority-classes&quot;, &quot;/livez/poststarthook/start-apiextensions-controllers&quot;, &quot;/livez/poststarthook/start-apiextensions-informers&quot;, &quot;/livez/poststarthook/start-cluster-authentication-info-controller&quot;, &quot;/livez/poststarthook/start-kube-aggregator-informers&quot;, &quot;/livez/poststarthook/start-kube-apiserver-admission-initializer&quot;, &quot;/logs&quot;, &quot;/metrics&quot;, &quot;/openapi/v2&quot;, &quot;/readyz&quot;, &quot;/readyz/autoregister-completion&quot;, &quot;/readyz/etcd&quot;, &quot;/readyz/log&quot;, &quot;/readyz/ping&quot;, &quot;/readyz/poststarthook/apiservice-openapi-controller&quot;, &quot;/readyz/poststarthook/apiservice-registration-controller&quot;, &quot;/readyz/poststarthook/apiservice-status-available-controller&quot;, &quot;/readyz/poststarthook/bootstrap-controller&quot;, &quot;/readyz/poststarthook/crd-informer-synced&quot;, &quot;/readyz/poststarthook/generic-apiserver-start-informers&quot;, &quot;/readyz/poststarthook/kube-apiserver-autoregistration&quot;, &quot;/readyz/poststarthook/rbac/bootstrap-roles&quot;, &quot;/readyz/poststarthook/scheduling/bootstrap-system-priority-classes&quot;, &quot;/readyz/poststarthook/start-apiextensions-controllers&quot;, &quot;/readyz/poststarthook/start-apiextensions-informers&quot;, &quot;/readyz/poststarthook/start-cluster-authentication-info-controller&quot;, &quot;/readyz/poststarthook/start-kube-aggregator-informers&quot;, &quot;/readyz/poststarthook/start-kube-apiserver-admission-initializer&quot;, &quot;/readyz/shutdown&quot;, &quot;/version&quot; ]}배치 api 그룹의 REST 엔드포인트 살펴보기# apis/batch 엔드포인트 조회curl http://localhost:8001/apis/batch{ &quot;kind&quot;: &quot;APIGroup&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;name&quot;: &quot;batch&quot;, &quot;versions&quot;: [ # 제공되는 groupVersion 종류(batch API그룹은 2가지 버전을 갖는다는것을 의미함) { &quot;groupVersion&quot;: &quot;batch/v1&quot;, &quot;version&quot;: &quot;v1&quot; }, { &quot;groupVersion&quot;: &quot;batch/v1beta1&quot;, &quot;version&quot;: &quot;v1beta1&quot; } ], &quot;preferredVersion&quot;: { # 클라이언트는 preferredVersion을 사용하는것을 권장한다는 의미 &quot;groupVersion&quot;: &quot;batch/v1&quot;, &quot;version&quot;: &quot;v1&quot; }}# batch/v1 리소스 유형curl http://localhost:8001/apis/batch/v1{ &quot;kind&quot;: &quot;APIResourceList&quot;, # batch/v1 API 그룹 내의 API 리소스 목록 &quot;apiVersion&quot;: &quot;v1&quot;, &quot;groupVersion&quot;: &quot;batch/v1&quot;, &quot;resources&quot;: [ # 이 그룹의 모든 리소스 유형을 담는 배열 { &quot;name&quot;: &quot;jobs&quot;, &quot;singularName&quot;: &quot;&quot;, &quot;namespaced&quot;: true, # 네임스페이스에 속하는 리소스라는 의미 ( persistentvolumes같은 것들은 false) &quot;kind&quot;: &quot;Job&quot;, &quot;verbs&quot;: [ # 이 리소스와 함꼐 사용할 수 있는 제공되는 API(단일, 여러개를 한꺼번에 추가 삭제할수 있고, 검색, 감시 업데이트 할수 있음) &quot;create&quot;, &quot;delete&quot;, &quot;deletecollection&quot;, &quot;get&quot;, &quot;list&quot;, &quot;patch&quot;, &quot;update&quot;, &quot;watch&quot; ], &quot;categories&quot;: [ &quot;all&quot; ], &quot;storageVersionHash&quot;: &quot;mudhfqk/qZY=&quot; }, { &quot;name&quot;: &quot;jobs/status&quot;, # 리소스의 상태를 수정하기 위한 특수한 REST 엔드포인트 &quot;singularName&quot;: &quot;&quot;, &quot;namespaced&quot;: true, &quot;kind&quot;: &quot;Job&quot;, &quot;verbs&quot;: [ &quot;get&quot;, &quot;patch&quot;, &quot;update&quot; ] } ]}클러스터 안에 있는 모든 잡 인스턴스 나열하기 items 하위에 나열된다.# job 생성kubectl create -f my-job.yaml# 모든 잡 인스턴스 조회curl http://localhost:8001/apis/batch/v1/jobs이름별로 특정 잡 인스턴스 검색# api를 통한 job 조회curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job# job 정보 조회kubectl get job my-job -o json# 위 2가지 결과는 같다.8.2.2 파드 내에서 API 서버와 통신 파드 내에서 통신하려면 API 서버의 위치를 찾아야하고, 서버로 인증을 해야 한다.API 서버와의 통신을 시도하기 위해 파드 실행kubectl create -f curl.yaml# 파드의 shell 접근kubectl exec -it curl bashAPI 서버 주소 찾기 실제 애플리케이션에서는 서버 인증서 확인을 절대로 건너뛰면 안된다. 중간자 공격(man-in-the-middle attack)으로 인증 토큰을 공격자에게 노출할수 있기 때문 중간자 공격(man-in-the-middle attack)은 통신을 연결하는 두 사람 사이에 중간자가 침입해 두 사람은 상대방에 연결했다고 생각하지만 실제로는 두 사람은 중간자에게 연결돼 있으며, 중간자가 한쪽에서 전달된 정보를 도청 및 조작한 후 다른쪽으로 전달하는 방식 # kubernetes 서비스kubectl get svc# KUBERNETES_SERVICE_HOST, KUBERNETES_SERVICE_PORT 변수를 통해 얻을수 있다.env | grep KUBERNETES_SERVICEKUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_HOST=10.96.0.1KUBERNETES_SERVICE_PORT_HTTPS=443# FQDN을 이용한 방법(403)curl https://kubernetes -k서버의 아이덴티티 검증 각 컨테이너의 /var/run/secrets/kubernetes.io/serviceaccount/에 마운트되는 자동 생성된 default-token-xyz 라는 이름의 시크릿을 기준으로 처리할수 있다.# 컨테이너 내부에서 조회ls /var/run/secrets/kubernetes.io/serviceaccount/ca.crt namespace token# --cacert 옵션을 통해 인증서 지정 ( 여전히 403)curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes# 환경변수 지정export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt# 재호출curl https://kubernetesAPI 서버로 인증 Authorization HTTP 헤더 내부에 토큰을 전달하여 토큰을 인증된 것으로 인식하여 적절한 응답을 받을 수 있다. 이런 방식을 통해 네임스페이스 내에 있는 모든 파드를 조회할 수 있다. 그러나 먼저 curl 파드가 어떤 네임스페이서에서 실행중인지 알아야 한다.TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)# 호출 ( 왜안되느지 모르겠지만 안됨 ㅠ)curl -H &quot;Authorization: Bearer $TOKEN&quot; https://kubernetes역할 기반 엑세스 제어(RBAC) 비활성화 RBAC가 활성화된 쿠버네티스 클러스터를 사용하는 경우 서비스 어카운트가 API 서버에 엑세스할 권한이 없을 수 있다.# 모든 서비스 어카운트에 클러스터 관리자 권한이 부여됨kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts# 위험하고 프로덕션 클러스터에서는 해서는 안됨.파드가 실행중인 네임스페이스 얻기 시크릿 볼륨 디렉터리에 있는 3개의 파일을 사용해 파드와 동일한 네임스페이스에서 실행중인 모든 파드를 나열할 수 있다. GET 대신 PUT이나 PATCH를 전송해 업데이트도 가능하다.# namespace 지정 ( 실제로 default인데 아무값도 없다..)NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)# 호출curl -H &quot;Authorization: Bearer $TOKEN&quot; https://kubernetes/api/v/namespaces/$NS/pods파드가 쿠버네티스와 통신하는 방법 정리 애플리케이션 API 서버의 인증서가 인증기관으로부터 서명됐는지를 검증해야 하고, 인증 기관의 인증서는 ca.cart 파일에 있다. 애플리케이션은 token 파일의 내용을 Authorization HTTP 헤더에 Bearer 토큰으로 넣어 전송해서 자신을 인증해야 한다. namespace 파일은 파드의 네임스페이스 안에 있는 API 오브젝트의 CRUD 작업을 수행할 때 네임스페이스를 API 서버로 전달하는데 사용해야 한다.8.2.3 앰배서더 컨테이너를 이용한 API 서버 통신 간소화 보안을 유지하면서 통신을 훨씬 간단하게 만들수 있다.(kubectl proxy 활용)앰배서더 컨테이너 패턴 소개 메인 컨테이너 옆의 앰배서더 컨테이너에서 kubectl proxy를 실행하고 이를 통해 APi 서버와 통신할 수 있다. 메인 컨테이너의 애플리케이션은 HTTPS 대신 HTTP로 앰배서더에 연결하고 앰배서더 프록시가 APi 서버에 대한 HTtPS 연결을 처리하도록해 보안을 투명하게 관리할 수 있다. 파드의 모든 컨테이너는 동일한 루프백 네트워크 인터페이스를 공유하므로 애플리케이션은 localhost의 포트로 프록세이 엑세스할 수 있다.추가적인 앰배서더 컨테이너를 사용한 curl 파드 실행 kubectl proxy는 8001에 바인딩되며 curl localhost:8001에 접속할 수 있다. 외부 서비스에 연결하는 복잡성을 숨기고 메인 컨테이너에서 실행되는 애플리케이션을 단순화하기 위해 앰배서더 컨테이너를 사용하는 좋은 예시이다. 단점은 추가 프로세스를 실행해야 해서 리소스가 추가로 소비된다는 것이다.apiVersion: v1kind: Podmetadata: name: curl-with-ambassadorspec: containers: - name: main image: tutum/curl command: [&quot;sleep&quot;, &quot;9999999&quot;] - name: ambassador # kubectl-proxy 이미지를 실행하는 앰배서더 컨테이너 image: luksa/kubectl-proxy:1.6.2# pod 생성kubectl create -f curl-with-ambassador.yaml# shell 접속kubectl exec -it curl-with-ambassador -c main bash# 호출curl localhost:80018.2.4 클라이언트 라이브러리를 사용해 API 서버와 통신 단순한 API 요청 이상을 수행하려면 쿠버네티스 API 클라이언트 라이브러리 중 하나를 사용하는 것이 좋다. https://kubernetes.io/ko/docs/reference/using-api/client-libraries/ ( 다양한 언어를 지원함.) 현재 SIG(Special Interest Group)에서 지원하는 API는 Go, Python, Java ,.net, JavaScript, Haskell이 있다. 이 라이브러리를 사용하는 경우 기본적으로 HTTPS를 지원하고, 인증을 관리하므로 앰배서더 컨테이너를 사용할 필요가 없다.Java 예제 ( https://github.com/kubernetes-client/java/ ) 책에 있는 Fabric8 java 클라이언트가 아니라 SIG에서 지원하는 Java 클라이언트 라이브러리를 첨부// list all podspublic class Example { public static void main(String[] args) throws IOException, ApiException{ ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); // 라이브러리 메소드 설계는 좀 이상하게 해놓은듯, 모두 null이면 arguments 정의를 안했어야지..) V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); for (V1Pod item : list.getItems()) { System.out.println(item.getMetadata().getName()); } }}// watch on namespace object:public class WatchExample { public static void main(String[] args) throws IOException, ApiException{ ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); Watch&amp;lt;V1Namespace&amp;gt; watch = Watch.createWatch( client, api.listNamespaceCall(null, null, null, null, null, 5, null, null, Boolean.TRUE, null, null), new TypeToken&amp;lt;Watch.Response&amp;lt;V1Namespace&amp;gt;&amp;gt;(){}.getType()); for (Watch.Response&amp;lt;V1Namespace&amp;gt; item : watch) { System.out.printf(&quot;%s : %s%n&quot;, item.type, item.object.getMetadata().getName()); } }}스웨거와 Open API를 사용해 자신의 라이브러리 구축 쿠버네티스 API 서버는 /swaggerapi 에서 스웨거 API 정의를 공개하고 /swagger.json에서 OepnAPI 스펙을 공개하고 있다.스웨거 UI로 API 살펴보기 스웨거 UI로 REST API를 더 나은 방식으로 탐색할 수 있다. API 서버를 --enable-swagger-ui=true옵션으로 실행하면 활성화된다.# minikube swaggerUI = true 적용minikube start --extra-config=apiserver.Features.EnableSwaggerUI=true# kubectl proxykubectl proxy --port=8080 &amp;amp;# 호출 ( swagger-ui 안됨..)http://localhost:8080/swagger-uihttp://localhost:8080/swagger.jsonhttp://192.168.64.2:8443/swagger-ui8.3 요약 파드의 이름, 네임스페이스 및 기타 메타데이터가 환경변수 또는 downward API 볼륨의 파일로 컨테이너 내부의 프로세스에 노출시키는 방법 CPU와 메모리의 요청 및 제한이 필요한 단위로 애플리케이션에 전달되는 방법 파드에서 downward API 볼륨을 사용해 파드가 살아 있는 동안 변경될 수 있는 최신 메타데이터를 얻는 방법(레이블과 어노테이션 등) kubectl proxy로 쿠버네티스 REST API를 탐색하는 방법 쿠버네티스에 정의된 다른 서비스와 같은 방식으로 파드가 환경변수 또는 DNS로 API 서버의 위치를 찾는 방법 파드에서 실행되는 애플리케이션이 API 서버와 통신하는지 검증하고, 자신을 인증하는 방법 앰배서더 컨테이너를 사용해 애플리케이션 내에서 API 서버와 훨씬 간단하게 통신하는 방법 클라이언트 라이브러리ㅏ로 쉽게 쿠버네티스와 상호작용할 수 있는 방법Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 7. 컨피그맵과 시크릿 : 애플리케이션 설정", "url": "/posts/devlog-platform-kubernetes-in-action7/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-22 17:34:00 +0900", "snippet": "7. 컨피그맵과 시크릿 : 애플리케이션 설정 빌드된 애플리케이션 자체에 포함되지 말아야 하는 설정(배포된 인스턴스별로 다른 세팅, 외부 시스템 엑세스를 위한 자격증명 등)이 필요하다.쿠버네티스는 이런 앱을 실행할때 설정 옵션을 전달할수 있는 방법을 제공한다.7.1 컨테이너화된 애플리케이션 설정 필요한 모든 설정을 앱에 포함하는 경우를 제외하면 일반적으로 명령줄 인수를 통해 앱에 필요한 설정을 넘겨주면서 앱을 실행시킨다. 옵션 목록이 커지면 이 옵션들을 파일에 저장하고 사용하기도 한다. 아니면 서버의 환경변수를 통해 전달하기도 한다. 도커 컨테이너를 기반으로 한다고 했을때 내부에 있는 설정 파일을 사용하는것은 약간 까다롭다.(컨테이너 이미지 안에 넣는것은 소스코드에 하드코딩하는것과 다를바가 없음) 인증 정보나 암호화 키와 같이 비밀로 유지해야 하는 내용을 포함하게 되면 해당 이미지에 접근할 수 있는 모든 사람은 누구나 정보를 볼수 있게 되버린다. 쿠버네티스에서는 설정 데이터를 최상위 레벨의 쿠버네티스 오브젝트에 저장하고 이를 기타 다른 리소스 정의와 마찬가지로 깃 저장소 혹은 다른 파일 기반 스토리지에 저장할 수 있다. 이러한 목적으로 쿠버네티스는 컨피그맵이라는 리소스를 제공한다. 대부분의 설정 옵션에서는 민감한 정보가 포함돼있지 않지만, 자격증명, 개인 암호화 키, 보안을 유지해야 하는 유사한 데이터들도 있다. 이를 위한 시크릿이라는 또다른 유형의 오브젝트도 제공한다.애플리케이션에 설정을 전달하는 방법 컨테이너에 명령줄 인수 전달 각 컨테이너를 위한 사용자 정의 환경변수 지정 특수한 유형의 볼륨을 통해 설정7.2 컨테이너에 명령줄 인자 전달7.2.1 도커에서 명령어와 인자 정의ENTRYPOINT와 CMD 이해 ENTRYPOINT는 컨테이너가 시작될때 호출될 명령어 정의 CMD는 ENTRYPOINT에 전달되는 인자를 정의shell과 exec 형식간의 차이점 shell 형식 : ENTRYPOINT node app.js exec 형식 : ENTRYPOINT [“node”, “app.js”]fortune 이미지에서 간격을 설정할 수 있도록 만들기 INTERVAL 변수를 추가하고 첫 번쨰 명령줄 인자의 값으로 초기화# fortuneloop.sh#!/bin/bashtrap &quot;exit&quot; SIGINTINTERVAL=$1 # 인자echo Configured to generate new fortune every $INTERVAL secondsmkdir -p /var/htdocswhile :do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune &amp;gt; /var/htdocs/index.html sleep $INTERVALdone# DockerfileFROM ubuntu:latestRUN apt-get update ; apt-get -y install fortuneADD fortuneloop.sh /bin/fortuneloop.shENTRYPOINT [&quot;/bin/fortuneloop.sh&quot;] # exec 형태의 ENTRYPOINT 명령CMD [&quot;10&quot;] # 실행할때 사용할 기본 인자7.2.2 쿠버네티스에서 명령과 인자 재정의 command와 args 필드로 맵핑된다. 이는 파드 생성 이후에는 업데이트 할수 없다.apiVersion: v1kind: Podmetadata: name: fortune2sspec: containers: - image: some/image command: [&quot;/bin&quot;/commend&quot;] args: [&quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;]사용자 정의 주기로 fortune 파드 실행apiVersion: v1kind: Podmetadata: name: fortune2sspec: containers: - image: luksa/fortune:args args: [&quot;2&quot;] # 스크립트가 2초마다 새로운 fortune 메시지를 생성하도록 인자 지정 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}여러개의 인자 처리 args: - foo - bar - &quot;15&quot;숫자 인자에 대한 처리 문자열인 경우는 ““로 묶을 필요 없지만 숫자는 묶어야 한다.7.3 컨테이너의 환경변수 설정 파드의 각 컨테이너를 위한 환경변수 리스트를 지정할 수 있다. 컨테이너 명령이나 인자와 마찬가지로 환경변수 목록도 파드 생성 후에는 업데이트 불가 컨테이너별로 다른 환경변수 설정도 가능하다.환경변수로 fortune 이미지 안에 간격을 설정할 수 있도록 만들기 변수 초기화하는 부분을 제거하면 끝.#!/bin/bashtrap &quot;exit&quot; SIGINTecho Configured to generate new fortune every $INTERVAL secondsmkdir -p /var/htdocswhile :do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune &amp;gt; /var/htdocs/index.html sleep $INTERVALdone7.3.1 컨테이너 정의에 환경변수 지정 각 컨테이너를 설정할 때, 쿠버네티스는 자동으로 동일한 네임스페이스 안에 있는 각 서비스에 환경변수를 노출하는것은 알고 있어야 한다.(파드에 정의한 환경변수명이 같아 덮어써지는 부분이 있을수 있어서인듯)apiVersion: v1kind: Podmetadata: name: fortune-envspec: containers: - image: luksa/fortune:env env: # 환경변수 목록에 단일 변수 추가 - name: INTERVAL value: &quot;30&quot; name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}7.3.2 변숫값에서 다른 환경변수 참조env: - name: FIRST_VAR value: &quot;foo&quot; - name: SECOND_VAR value: &quot;${FIRST_VAR}bar&quot; # foobar가 됨7.3.3 하드코딩된 환경변수의 단점 하드코딩된 값을 가져오는게 효율적일 수 있지만, 프로덕션과 개발환경의 파드를 별도로 정의해야할수도 있다는 의미 여러 환경에서 동일한 파드 정의를 재사용하려면 파드 정의에서 설정을 분리하는것이 좋다.7.4 컨피그맵으로 설정 분리 환경에 따라 다르거나 자주 변경되는 설정 옵션을 애플리케이션 소스 코드와 별도로 유지하는 것.7.4.1 컨피그맵 소개 쿠버네티스에서는 설정 옵션을 컨피그맵이라 부르는 별도 오브젝트로 분리할 수 있다. 짧은 문자열에서 전체 설정 파일에 이르는 값을 가지는 키/값 쌍으로 구성된 맵이다. 쿠버네티스 REST API 엔드포인트를 통해 컨피그맵의 내용을 직접 읽는것이 가능하지만, 반드시 필요한 경우가 아니라면 애플리케이션 내부는 쿠버네티스와 무관하도록 유지해야 한다. 파드는 컨피그맵을 이름으로 참조하여 모든 환경에서 동일한 파드 정의를 사용해 각 환경에서 서로 다른 설정을 사용할 수 있다.7.4.2 컨피그맵 생성apiVersion: v1kind: ConfigMapmetadata: name: fortune-configdata: sleep-interval: &quot;25&quot; kubectl create configmap로 생성 가능 컨피그맵 키는 유효한 DNS 서브도메인이어야 한다.(영숫자, 대시, 밑줄, 점만 포함 가능)# 컨피그맵 생성 (file)kubectl create -f fortune-config.yaml# 컨피그맵 생성 (literal)kubectl create configmap fortune-config --from-literal=sleep-interval=25# 컨피그맵 생성 (여러개의 literal)kubectl create configmap myconfigmap --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two# 컨피그맵 목록 조회kubectl get cm# 컨피그맵 정의 확인kubectl get configmap fortune-config -o yaml파일 내용으로 컨피그맵 생성 전체 설정 파일 같은 데이터를 통째로 컨피그맵에 저장할 수 있다.# 파일 통째로 컨피그맵 생성# - 파일 자체가 값으로 지정됨kubectl create configmap my-config --from-file=config-file.conf# 파일 통째로 저장하되 customkey 지정kubectl create configmap my-config --from-file=customkey=config-file.conf디렉터리에 있는 파일 컨피그맵 생성kubectl create configmap my-config --from-file=/path/to/dir#### 다양한 옵션 결합``` shkubectl create configmap my-config --from-file=foo.json # 단일 파일 --from-file=bar=foobar.conf # 사용자 정의 키 밑에 파일 저장 --from-file=config-opts/ # 전체 디렉터리 --from-literal=some=thing # 문자열 값7.4.3 컨피그맵 항목을 환경변수로 컨테이너에 전달 생성한 맵의 값을 어떻게 파드 안의 컨테이너를 전달할수 있는 방법을 살펴보자apiVersion: v1kind: Podmetadata: name: fortune-env-from-configmapspec: containers: - image: luksa/fortune:env env: - name: INTERVAL # INTERVAL 환경변수 설정 valueFrom: configMapKeyRef: # 컨피그맵 키에서 값을 가져와 초기화 name: fortune-config # 참조하는 컨피그맵 이름 key: sleep-interval # 컨피그맵에서 해당 키 아래에 저장된 값으로 변수 셋팅 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}파드에 존재하지 않는 컨피그맵 참조 존재하지 않는 컨피그맵을 참조하려고 하면 컨테이너는 시작하는데 실패한다. 하지만 참조하지 않는 다른 컨테이너는 정상적으로 시자된다. 누락된 컨피그맵을 생성하면 실패했던 컨테이너는 파드를 다시 만들지 않아도 자동으로 시작된다. 컨피그맵 참조를 옵션으로 표시할수도 있다. ( configMapKeyRef.optional : true로 지정), 이런 경우는 컨피그맵이 존재하지 않아도 컨테이너가 시작된다.7.4.4 컨피그맵의 모든 항목을 한번에 환경변수로 전달 컨피그맵의 모든 항목을 환경변수로 노출할 수 있는 방법을 제공한다. 접두사는 선택사항이고, 이를 생략하면 환경변수의 이름은 키와 동일한 이름을 갖게 된다. CONFIG_FOO-BAR는 대시를 가지고 있어 올바른 환경변수 이름이 아니기 때문에 이런 경우 환경변수로 변환되지 않는다.(올바른 형식이 아닌 경우 쿠버네티스에서 생략함)spec: containers: - image: some-image envForm: # env 대신 envForm 사용 - prefix: CONFIG_ # 모든 환경변수는 CONFIG_ prefix로 설정됨. configMapRef: # my-config-map 이름의 컨피그맵 참조 name: my-config-map...7.4.5 컨피그맵 항목을 명령줄 인자로 전달 pod.spec.containers.args 필드에서 직접 컨피그맵 항목을 참조할 수는 없지만 컨피그맵 항목을 환경변수로 먼저 초기화하고 이 벼수를 인자로 참조할 수 있다.apiVersion: v1kind: Podmetadata: name: fortune-args-from-configmapspec: containers: - image: luksa/fortune:args # 환경변수가 아닌 첫번째 인자에서 간격을 가져오는 이미지 env: - name: INTERVAL # 컨피그맵에서 환경변수 정의 valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval args: [&quot;$(INTERVAL)&quot;] # 인자에 앞에서 정의한 환경변수를 지정 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}7.4.6 컨피그맵 볼륨을 사용해 컨피그맵 항목을 파일로 노출 컨피그맵은 모든 설정 파일을 포함한다. 컨피그맵 볼륨을 사용해서도 적용할 수가 있다. 컨피그맵 볼륨은 파일로 컨피그맵의 각 항목을 노출한다.컨피그맵 생성 nginx 서버가 클라이언트로 응답을 gzip 압축해서 보내는 니즈가 있다고 해보자. nginx gzip 압축 옵션을 활성화하고 이에 대한 컨피그맵을 생성해야 한다.# nginx gzip config 정의server { listen 80; server_name www.kubia-example.com; gzip on; # gzip 압축 활성화 gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; }}# configMap 삭제kubectl delete cm fortune-config# configMap dir로 생성kubectl create configmap fortune-config --from-file=configmap-files# configMap 확인kubectl get cm fortune-config -o yaml# 컨피그맵 내용apiVersion: v1data: my-nginx-config.conf: | # 파이프라인(|) 문자는 여러 줄의 문자열이 이어진다는 것을 의미한다. server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25kind: ConfigMapmetadata: creationTimestamp: &quot;2020-08-23T07:55:47Z&quot; managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:my-nginx-config.conf: {} f:sleep-interval: {} manager: kubectl operation: Update time: &quot;2020-08-23T07:55:47Z&quot; name: fortune-config namespace: default resourceVersion: &quot;3405&quot; selfLink: /api/v1/namespaces/default/configmaps/fortune-config uid: bc35aea1-618a-4961-a8ef-06136d71825c볼륨 안에 있는 컨피그맵 항목 사용 컨피그맵 항목에서 생성된 파일로 볼륨을 초기화하는 방법apiVersion: v1kind: Podmetadata: name: fortune-configmap-volumespec: containers: - image: luksa/fortune:env env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: # 컨피그맵 볼륨 마운트 - name: html mountPath: /usr/share/nginx/html # 컨피그맵 볼륨을 마운트하는 컨테이너 위치 readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: config mountPath: /tmp/whole-fortune-config-volume readOnly: true ports: - containerPort: 80 name: http protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: # 이 볼륨은 fortune-config 컨피그맵을 참조하는 볼륨 name: fortune-confignginx 서버가 마운트한 설정 파일을 사용하는지 확인# pod 생성kubectl create -f fortune-pod-configmap-volume.yaml# port forward kubectl port-forward fortune-configmap-volume 8080:80 &amp;amp;# reqeuestcurl -H &quot;Accept-Encoding: gzip&quot; -I localhost:8080# 마운트된 컨피그맵 볼륨 내용 살펴보기kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d볼륨에 특정 컨피그맵 항목 노출 아래와 같이 설정하면 컨테이너 마운트 위치 ‘/etc/nginx/conf.d/’ 디렉터리에는 gzip.conf 파일만 포함된다.apiVersion: v1kind: Podmetadata: name: fortune-configmap-volume-with-itemsspec: containers: - image: luksa/fortune:env name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d/ # 컨테이너 마운트 path readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config items: # 볼륨에 포함할 항목을 조회해 선택 - key: my-nginx-config.conf # 해당 키 아래에 항목을 포함 path: gzip.conf # 항목 값이 지정된 파일에 저장디렉터리를 마운트할 때 디렉터리의 기존 파일을 숨기는 것에 대한 이해 리눅스 파일시스템을 비어 있지 않은 디렉터리에 마운트할떄 발생할 수 있는 문제로, 디렉터리는 마운트한 파일시스템에 있는 파일만 포함하고, 원래 있던 파일은 해당 파일시스템이 마운트돼 있는 동안 접근할 수 없게 된다. 중요한 파일을 포함하는 /etc 디렉터리에 볼륨을 마운트한다고 하면 /etc 디렉터리에 있어야 하는 모든 원본 파일이 더 이상 존재하지 않게 되어 전체 컨테이너가 손상될 수 있다.디렉터리 안에 다른 파일을 숨기지 않고 개별 컨피그맵 항목을 파일로 마운트 전체 볼륨을 마운트하는 대신 volumeMount에 subPath 속성으로 파일이나 디렉터리 하나를 볼륨에 마운트할 수 있다.spec: containers: - image: some/image volumeMounts: - name: myvolume mountPath: /etc/someconfig.conf # 디렉터리가 아닌 파일을 마운트 subPath: myconfig.conf # 전체 볼륨을 마운트하는 대신 myconfig.conf 항목만 마운트 subPath 속성은 모든 종류의 볼륨을 마운트할때 사용할 수 있다. 하지만 개별 파일을 마운트하는 이 방법은 파일 업데이트와 관ㄹ녀해 상대적으로 큰 결함을 가지고 있다.컨피그맵 볼륨 안에 있는 파일 권한 수정 기본적으로 컨피그맵 볼륨의 모든 파일 권한은 644(-rw-r-r–)로 설정된다. defaultMode 속성을 설정하여 변경이 가능하다. volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config defaultMode: 0660 # 모든 파일 권한을 660(-rw-rw---) 로 설정7.4.7 애플리케이션을 재시작하지 않고 설정 업데이트 컨피그맵을 사용해 볼륨으로 노출하면 파드를 다시 만들거나 컨테이너를 다시 시작할 필요 없이 설정을 업데이트할 수 있다. 컨피그맵을 업데이트한 후에 파일이 업데이트되기까지는 생각보다 오랜 시간이 걸릴수 있다.(최대 1분)컨피그맵 편집# configmap 수정kubectl edit configmap fortune-config# web-server 컨테이너 내 설정파일 확인kubectl exec fortune-configmap-volume -c web-server cat /etc/nginx/conf.d/my-nginx-config.conf설정을 다시 로드하기 위해 nginx에 신호 전달kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload파일이 한꺼번에 업데이트되는 방법 이해 쿠버네티스 컨피그맵은 모든 파일이 한번에 업데이트된다. (심볼릭 링크 방식으로 동작하기 때문)# 컨테이너 디렉토리 확인kubectl exec -i -t fortune-configmap-volume -c web-server -- ls -al /etc/nginx/conf.d# 결과drwxrwxrwx 3 root root 4096 Aug 23 08:14 .drwxr-xr-x 3 root root 4096 Aug 14 00:37 ..drwxr-xr-x 2 root root 4096 Aug 23 08:14 ..2020_08_23_08_14_11.042426742lrwxrwxrwx 1 root root 31 Aug 23 08:14 ..data -&amp;gt; ..2020_08_23_08_14_11.042426742lrwxrwxrwx 1 root root 27 Aug 23 08:14 my-nginx-config.conf -&amp;gt; ..data/my-nginx-config.conflrwxrwxrwx 1 root root 21 Aug 23 08:14 sleep-interval -&amp;gt; ..data/sleep-interval이미 존재하는 디렉터리에 파일만 마운트했을 때 업데이트가 되지 않는 것 이해하기 단일 파일만 컨테이너에 마운트한 경우 파일이 업데이트 되지 않는다.(단순 컨피그맵 + 볼륨 기능만 이용하는 경우에 한하여)컨피그맵 업데이트의 결과 이해하기 컨테이너의 가장 주용한 기능은 불변성(immutability)이다. 앱이 설정을 다시 읽는 기능을 지원하지 않는 경우에 심각한 문제가 발생한다. 컨피그맵을 변경한 이후 생성된 파드는 새로운 설정을 사용하지만 예전 파드는 계속 예전 설정을 사용하기 때문. 애플리케이션이 설정을 자동으로 다시 읽는 기능을 가지고 있지 않다면 이미 존재하는 컨피그맵을 수정하는것은 좋은 방법이 아니다.7.5 시크릿으로 민감한 데이터 컨테이너에 전달7.5.1 시크릿 소개 쿠버네티스는 민감한 정보를 보관하고 배포하기 위하여 시크릿이라는 오브젝트를 제공한다. 시크릿은 키-값 쌍을 가진 맵으로 컨피그맵과 매우 비슷하다. 컨피그맵과 마찬가지로 환경변수로 시크릿 항목을 컨테이너에 전달하거나 볼륨 파일로 노출시킬 수 있다. 시크릿에 접근해야 하는 파드가 실행되고 있는 노드에만 개별 시크릿을 배포해 시크릿을 안전하게 유지한다. 노드 자체적으로 시크릿을 항상 메모리에만 저장하게 되고 물리 저장소에는 기록되지 않도록 처리한다. 마스터 노드의 etcd에는 시크릿을 암호화되지 않는 형식으로 저장하므로 시크릿에 저장한 민감한 데이터를 보호하려면 마스터 노드를 보호해야 한다.(쿠버네티스 1.7 이하에서만, 그 이후로는 암호화된 형태로 저장함.)시크릿 vs 컨피그맵 어떤것을 사용해야 할지에 대한 기준 민감하지 않고, 일반 설정 데이터는 컨피그맵을 사용하라. 본질적으로 민감한 데이터는 시크릿을 사용해 키 아래에 보관하는 것이 필요하다. 민감한 데이터와 그렇지 않는 데이터를 모두 가지고 있는 경우 해당 파일은 시크릿 안에 저장해야 한다.7.5.2 기반 토큰 시크릿 모든 파드에는 sercret 볼륨이 자동으로 연결되어 있다.Containers: # default-token (Secret) 마운트 Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-xkr6d (ro)Volumes: default-token-xkr6d: Type: Secret (a volume populated by a Secret) SecretName: default-token-xkr6d Optional: false# pod 정보 조회kubectl describe po kubia-8pw7z# 시크릿 리소스 조회kubectl get secrets# 시크릿 정보 조회kubectl describe secrets 시크릿이 갖고 있는 3가지 항목(ca.crt, namespace, token)은 파드 안에서 쿠버네티스 API 서버와 통신할 때 필요한 것이다. 기본적으로 default-token 시크릿은 모든 컨테이너에 마운트된다. 파드 스펙 안에 auto mountService-AccountToken 필드 값을 false로 지정하거나 파드가 사용하는 서비스 어카운트를 false로 지정해 비활성화할 수 있다.7.5.3 시크릿 생성# 인증서와 개인키 생성openssl genrsa -out https.key 2048openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.kubia-example.com# secret 생성 ( fortune-https 이름을 가진 generic 시크릿을 생성 )kubectl create secret generic fortune-https --from-file=https.key --from-file=https.cert --from-file=foo시크릿의 유형 docker-registry ( 도커 레지스트리 사용을 위한) tls ( TLS 통신을 위한) generic (일반적인 상황)7.5.4 컨피그맵과 시크릿 비교 시크릿 항목의 내용은 base64 인코딩 문자열로 표시되고, 컨피그맵의 내용은 일반 텍스트로 표시된다.# 시크릿 조회kubectl get secret fortune-https -o yaml# 컨피그맵 조회kubectl get configmap fortune-config -o yaml바이너리 데이터 시크릿 사용 base64 인코딩을 사용하는 이유는 일반 텍스트 뿐만 아니라 바이너리 값도 담을 수 있기 때문이다. 민감하지 않은 데이터도 시크릿을 사용할수 있지만 시크릿의 최대 크기는 1MB로 제한된다.stringData 필드 소개 쿠버네티스는 시크릿의 값을 stringData 필드로 설정할 수 있게 해준다. stringData 필드는 쓰기 전용이다.(값을 설정할 때만 사용 가능)apiVersion: v1kind: SecretstringData # 바이너리 데이터가 아닌 시크릿 데이터에 사용할 수 있다. foo: plain text # &quot;plain text&quot;는 base64 인코딩되지 않는다.data: https.cert: ... https.key: ...파드에서 시크릿 항목 읽기 secret 볼륨을 통해 시크릿을 컨테이너에 노출하면, 시크릿 항목의 값이 일반 텍스트인지 바이너리 데이터인지에 관계 없이 실제 형식으로 디코딩돼 파일에 기록된다.7.5.5 파드에서 시크릿 사용 configmap의 nignx 설정에 https 인증서와 개인키를 추가해주고 시크릿을 파드에 마운트 참고 : 시크릿도 defaultMode 속성을 통해 볼륨에 노출된 파일 권한을 지정할 수 있음fortune-https 시크릿을 파드에 마운트apiVersion: v1kind: Podmetadata: name: fortune-httpsspec: containers: - image: luksa/fortune:env name: html-generator env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs # nginx 서버가 인증서와 키를 /etc/nginx/certs에서 읽을수 있도록 해당 위치에 secret 마운트 mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config items: - key: my-nginx-config.conf path: https.conf - name: certs # 시크릿 볼륨 정의 secret: secretName: fortune-httpsnginx가 시크릿의 인증서와 키를 사용하는지 테스트# https 파드 생성kubectl create -f fortune-pod-https.yaml# port forwardkubectl port-forward fortune-https 8443:443 &amp;amp;# curl callcurl https://localhost:8443 -k -v시크릿 볼륨을 메모리에 저장하는 이유 시크릿 볼륨은 시크릿 파일을 저장하는 데 인메모리 파일시스템(tmpfs)을 사용한다. tmpfs를 사용하는 이유는 민감한 데이터를 노출시킬 수도 있는 디스크에 저장하지 않기 위해서이다.# 컨테이너 확인kubectl exec fortune-https -c web-server -- mount | grep certstmpfs on /etc/nginx/certs type tmpfs (ro,relatime)환경변수로 시크릿 항목 노출 configMapKeyRef 대신 secretKeyRef를 사용해 컨피그맵과 유사한 방식으로 참조가 가능하다. 시크릿을 환경변수로 노출할 수 있게 해주기는 하지만, 이 기능 사용은 권장하지는 않는다. 앱에서 일반적으로 오류 보고서에 환경변수를 기록하거나 로그에 환경변수를 남겨 의도치 않게 시크릿이 노출될 가능성이 있다. 또한 자식 프로세스는 부모 프로세스의 모든 환경변수를 상속받는데, 앱이 타사(third-party) 바이너리를 실행할 경우 시크릿 데이터를 어떻게 사용하는지 알 수 있는 방법이 없다. env: # 변수는 시크릿 항목에서 설정 - name: FOO_SECRET valueFrom: &quot;30&quot; secretKeyRef: name: fortune-https # 시크릿 이름 지정 key: foo # 시크릿의 키 이름7.5.6 이미지를 가져올 때 사용하는 시크릿 이해 쿠버네티스에서 자격증명을 전달하는것이 필요할때가 있다.(프라이빗 컨테이너 이미지 레지스트리)도커 허브에서 프라이빗 이미지 사용 도커 레지스트리 자격증명을 가진 시크릿 생성 파드 매니페스트 안에 imagePullSecrets 필드에 해당 시크릿 참조도커 레지스트리 인증을 위한 시크릿 생성# 도커 레지스트리용 시크릿 생성kubectl create secret docker-registry mydockerhubsecret --docker-username=myusername --docker-password=mypassword --docker-email=my.email@providercom파드 정의에서 도커 레지스트리 시크릿 사용apiVersion: v1kind: Podmetadata: name: private-podspec: imagePullSecrets: # 프라이빗 이미지 레지스트리에서 이미지를 가져올 수 있도록 설정 - name: mydockerhubsecret containers: - image: username/private:tag name: main모든 파드에서 이미지를 가져올 때 사용할 시크릿을 모두 지정할 필요는 없다. 이미지를 가져올 때 사용할 시크릿을 서비스어카운트에 추가해 모든 파드에 자동으로 추가되도록 할수도 있다.(12장)7.6 요약 컨테이너 이미지에 정의된 기본 명령어를 파드 정의 안에 재정의 주 컨테이너 프로세스에 명령줄 인자 전달 컨테이너에서 사용할 환경변수 설정 파드 사양에서 설정을 분리해 컨피그맵 안에 넣기 민감한 데이터를 시크릿 안에 넣고 컨테이너에 안전하게 전달 docker-registry 시크릿을 만들고 프라이빗 이미지 레지스트리에서 이미지를 가져올 때 사용Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 6. 볼륨 : 컨테이너에 디스크 스토리지 연결", "url": "/posts/devlog-platform-kubernetes-in-action6/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-17 17:34:00 +0900", "snippet": "6. 볼륨 : 컨테이너에 디스크 스토리지 연결 파드는 내부에 프로세스가 실행되고 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유한다. 하지만 디스크는 공유되지 않는다. 파드 내부의 각 컨테이너는 고유하게 분리된 파일 시스템을 가지기 때문이다.(컨테이너 이미지로부터 제공되는) 새로 시작한 컨테이너는 이전에 실행했던 컨테이너에 쓰여진 파일 시스템의 어떤 것도 볼수 없다. 전체 파일 시스템이 유지될 필요는 없지만 실제 데이터를 가진 디렉터리를 보존하고 싶을 수 있음. 이를 위해 쿠버네티스는 스토리지 볼륨으로 기능을 제공한다. 볼륨은 파드와 같은 최상위 리소스는 아니지만 파드의 일부분으로 정의되며 파드와 일반적으로는 동일한 라이프 사이클을 가진다.6.1 볼륨 소개 쿠버네티스 볼륨은 파드의 구성 요소로 컨테이너와 동일하게파드 스펙에서 정의된다. 볼륨은 독립적인 쿠버네티스 오브젝트가 아니므로 자체적으로 생성, 삭제될 수 없다. 접근하려는 컨테이너에서 각각 마운트 되어야 한다.6.1.1 볼륨 예제 볼륨 2개를 파드에 추가하고, 3개의 컨테이너 내부의 적절한 경로에 마운트 리눅스에서 파일시스템을 파일 트리의 임의 경로에 마운트할 수 있는 방식을 이용 같은 볼륨을 2개의 컨테이너에 마운트하면 컨테이너는 동일한 파일로 동작 가능하다. 마운트되지 않은 볼륨이 같은 파드안에 있더라도 접근할수 없고, 접근하려면 volumeMount를 컨테이너 스펙에 정의해야 한다.6.1.2 사용 가능한 볼륨 유형 소개 emptyDir : 일시적인 데이터를 저장하는 데 사용되는 간단한 빈 디렉터리 hostPath : 워커 노드의 파일시스템을 파드의 디렉터리로 마운트 gitRepo : 깃 리포지터리의 콘텐츠를 체크아웃해 초기화한 볼륨 nft : NFS 공유를 파드에 마운트 gcePersistentDisk, awsElasticBlockStore, azureDisk 등 : 클라우드 제공자의 전용 스토리지 마운트 cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rdb, flexVolume, vsphereVolume, photonPersistentDisk, scaleIO : 다른 유형의 네트워크 스토리지 마운트 configMap, secret, downwardAPI : 쿠버네티스 리소스나 클러스터 정보를 파드에 노출하는 데 사용되는 특별한 유형의 볼륨 persistentVolumeClaim : 사전 혹은 동적으로 프로비저닝된 퍼시스턴트 스토리지를 사용하는 방법6.2 볼륨을 사용한 컨테이너간 데이터 공유6.2.1 emptyDir 볼륨 사용 빈 디렉터리로 시작되며, 볼륨의 라이프사이클이 파드에 묶여 있으므로 파드가 삭제되면 볼륨의 콘텐츠도 같이 사라진다. 컨테이너에서 가용한 메모리에 넣기에 큰 데이터 세트의 정렬 작업을 수행하는 것과 같은 임시 데이터를 디스크에 쓰는 목적인 경우 사용할 수 있다.파드에 emptyDir 볼륨 사용(동일한 볼륨을 공유하는 컨테이너 2개가 있는 파드)apiVersion: v1kind: Podmetadata: name: fortunespec: containers: - image: luksa/fortune name: html-generator # 첫번째 컨테이너 html-generator volumeMounts: # html이라는 이름의 볼륨을 컨테이너 /var/htdocs에 마운트 - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server # 두번째 컨테이너 web-server volumeMounts: # html이라는 이름의 볼륨을 컨테이너 /usr/share/nginx/html에 마운트 - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: # html이라는 단일 emptyDir 볼륨을 위의 컨테이너 2개에 마운트하기 위한 정의 - name: html emptyDir: {}실행중인 파드 보기# pod 조회kubectl get po# fortune pod 포트 포워딩kubectl port-forward fortune 8080:80# requestcurl http://localhost:8080# html-generator 컨테이너 내부 확인kubectl exec -i -t fortune -c html-generator -- /bin/bash# web-server 컨테이너 index.html 파일 확인kubectl exec -i -t fortune -c web-server -- cat /usr/share/nginx/html/index.htmlemptyDir을 사용하기 위한 매체 지정하기 워커 노드의 실제 디스크에 생성하는 경우 노드 디스크가 어떤 유형인지에 따라 성능이 결정될수 있음. 쿠버네티스에 emptyDir을 디스크가 아닌 메모리를 사용하는 tmpfs 파일시스템으로 생성하도록 요청할수도 있음. volumes: - name: html emptyDir: medium: Memory # 이 emptyDir의 파일들은 메모리에 저장된다.6.2.2 깃 리포지터리를 볼륨으로 사용하기 gitRepo 볼륨은 emptyDir base이고, 파드가 시작되면 깃 리포를 복제하여 데이터를 채운다. 볼륨이 생성된 후에 참조하는 리포지터리와 동기화되지는 않는다. 파드가 삭제되고 새 파드가 생성되면 그 파드는 최신 커밋을 포함하게 된다. 최신 변경사항을 동기화하고 싶은 경우 github web hook 같은 것을 이용하면 될듯복제된 깃 리포지터리 파일을 서비스하는 웹 서버 실행하기apiVersion: v1kind: Podmetadata: name: gitrepo-volume-podspec: containers: - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html gitRepo: # gitRepo 정의하여 볼륨 정의 가능 repository: https://github.com/luksa/kubia-website-example.git revision: master directory: .gitRepo 볼륨에 대한 정리 gitRepo 볼륨은 emptyDir 볼륨과 유사하게 기본적으로 볼륨을 포함하는 파드를 위해 특별히 생성되고 독점적으로 사용되는 전용 디렉터리이다. 파드가 삭제되면 볼륨과 콘텐츠는 모두 삭제된다.6.3 워커 노드 파일시스템의 파일 접근 대부분의 파드는 호스트 노드를 인식하지 못하므로 노드의 파일 시스템에 있는 어떤 파일에도 접근하면 안 된다. 하지만 특정 시스템 레벨의 파드(데몬셋과 같은)는 이런 파일 시스템 접근이 필요할 수 있다. 쿠버네티스는 hostPath 볼륨으로 이 기능을 지원한다.6.3.1 hostPath 볼륨 소개 hostPath 볼륨은 노드 파일 시스템의 특정 파일이나 디렉터리를 가리킨다.(퍼시스턴트 스토리지) gitRepo나 emptyDir 볼륨의 콘텐츠는 파드가 종료되면 삭제되지만, hostPath 볼륨의 콘텐츠는 삭제되지 않는다. 이전 파드와 동일한 노드에서 새롭게 스케줄링 되는 새로운 파드는 이전 파드가 남긴 모든 항목을 볼 수 있다. 다만 데이터베이스의 데이터 디렉터리를 지정할 위치로 사용하기에는 적절하지 않다.(db pod은 다른 노드로 스케줄링 될 가능성이 있으므로) hostPath 볼륨은 파드가 어떤 녿에 스케줄되느냐에 따라 민감하기 때문에 일반적인 파드에서는 사용하지 않는것이 좋다.6.3.2 hostPath 볼륨을 사용하는 시스템 파드 검사하기 노드의 로그 파일이나 kubeconfig(쿠버네티스 구성 파일), CA 인증서를 접근하기 위한 데이터들을 hostPath로 구성되어있음 노드의 시스템 파일에 읽기/쓰기를 하는 경우에만 hostPath 볼륨을 사용해야 한다.(여러 파드에 걸쳐 데이터를 유지하기 위해서는 사용 금지)# kube-system 네임스페이스의 시스템 파드 조회kubectl get pods --namespace kube-system# 시스템 파드 Path 확인kubectl describe po kube-controller-manager-minikube --namespace kube-system# path 내용들ca-certs: Type: HostPath (bare host directory volume) Path: /etc/ssl/certs HostPathType: DirectoryOrCreate flexvolume-dir: Type: HostPath (bare host directory volume) Path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec HostPathType: DirectoryOrCreate k8s-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs HostPathType: DirectoryOrCreate kubeconfig: Type: HostPath (bare host directory volume) Path: /etc/kubernetes/controller-manager.conf HostPathType: FileOrCreate6.4 퍼시스턴트 스토리지 사용 파드에서 실행중인 애플리케이션이 디스크에 데이터를 유지해야 하고 파드가 다른 노드로 재스케줄링된 경우에도 동일한 데이터를 사용해야 하는 경우를 NAS 같은 유형에 데이터를 저장해야 한다. 이를 위한 방법을 쿠버네티스가 제공한다. minikube로 연습하는 경우에는 hostPath 볼륨으로 사용하면 된다.6.4.1 GCE 퍼시스턴트 디스크를 파드 볼륨에 사용하기apiVersion: v1kind: Podmetadata: name: mongodbspec: volumes: - name: mongodb-data gcePersistentDisk: # 볼륨의 유형은 GCE 퍼시스턴트 디스크 pdName: mongodb fsType: ext4 # 리눅스 파일시스템 유형 containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db # 컨테이너 내 마운트 되는 path ports: - containerPort: 27017 protocol: TCP6.4.2 기반 퍼시스턴트 스토리지로 다른 유형의 볼륨 사용하기 awsElasticBlockStore, azureFile이나 azureDisk 볼륨을 사용할 수 있음.AWS Elastic Block Store 볼륨 사용apiVersion: v1kind: Podmetadata: name: mongodb-awsspec: volumes: - name: mongodb-data awsElasticBlockStore: # awsElasticBlockStore volumeID: my-volume fsType: ext4...NFS 볼륨 사용apiVersion: v1kind: Podmetadata: name: mongodb-nfsspec: volumes: - name: mongodb-data nfs: server: 1.2.3.4 path: /some/path...다른 스토리지 기술 사용 쿠버네티스는 왠만한 모든 기술의 다양한 스토리지를 지원한다.6.5 기반 스토리지 기술과 파드 분리 쿠버네티스에 앱을 배포하는 개발자는 기저에 어떤 종류의 스토리지 기술이 사용되는 알 필요 없어야 하고, 동일한 방식으로 파드를 실행하기 위해 어떤 유형의 물리 서버가 사용되는지 알 필요 없어야 한다.(이상적) 파드의 볼륨이 실제 기반 인프라스르럭처를 참조한다는 것은 쿠버네티스가 추구하는 바가 아님 인프라 스트럭처 관련 정보를 파드 정의에 포함한다는 것은 파드 정의가 특정 쿠버네티스 클러슽에 밀접하게 연결됨을 의미한다. 동일한 파드 정의를 다른 클러스터에서는 사용할 수 없다.6.5.1 퍼시스턴트볼륨(PV, PersistentVolume)과 퍼시스턴트볼륨클레임(PVC, PersistentVolumeClaim) 인프라스트럭처의 세부 사항을 처리하지 않고 앱이 스토리지를 요청할 수 있도록 하기 위한 리소스 유형 관리자는 네트워크 스토리지 유형을 서정하고, PV 디스크립터를 게시하여 퍼시스턴트볼륨을 생성한다. 사용자는 퍼시스턴트볼륨클레임(PVC)을 생성하면, 쿠버네티스가 적당한 크기와 접근모드의 PV를 찾아서 PVC를 PV에 바인딩시킨다. 사용자는 이제 PVC를 참조하는 볼륨을 가진 파드를 생성한다.6.5.2 퍼시스턴트볼륨 생성 퍼시스턴트볼륨을 생성할 때 동시에 단일 또는 다수 노드에 읽기나 쓰기가 가능한지 여부 등을 지정해야 하고, 퍼시스턴트볼륨 해제시 어떤 동작을 해야 할지 정의해야 한다. 퍼시스턴트 볼륨을 지원하는 실제 스토리지의 유형, 위치, 그 밖의 속성 정보를 지정 퍼시스턴트 볼륨은 특정 네임스페이스에 속하지 않고, 노드와 같은 수준의 클러스터 리소스이다.정의apiVersion: v1kind: PersistentVolumemetadata: name: mongodb-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce # 단일 클라이언트의 읽기/쓰기용으로 마운트 - ReadOnlyMany # 여러 클라이언트의 읽기 전용으로 마운트 persistentVolumeReclaimPolicy: Retain # 클레임이 해제된 후 퍼시스턴트볼륨을 유지한다. hostPath: # ohstPath 볼륨 (minikube) path: /tmp/mongodbpersistentVolumeReclaimPolicy 현재 NFS 및 HostPath만 재활용을 지원한다. AWS EBS, GCE PD, Azure Disk 및 Cinder 볼륨은 삭제를 지원한다. Retain(보존) – 수동 반환 Recycle(재활용) – 기본 스크럽 (rm -rf /thevolume/*) Delete(삭제) – AWS EBS, GCE PD, Azure Disk 또는 OpenStack Cinder 볼륨과 같은 관련 스토리지 자산이 삭제됨생성 및 조회# hostPath PV 생성kubectl create -f mongodb-pv-hostpath.yaml# pv 조회kubectl get pv6.5.3 퍼시스턴트볼륨클레임 생성을 통한 퍼시스턴트볼륨 요청 파드가 재스케줄링되더라도 동일한 퍼시스턴트볼륨클레임이 사용 가능한 상태로 유지되기를 원하므로 퍼시스턴트 볼륨에 대한 클레임은 파드를 생성하는 것과 별개의 프로세스이다.퍼시스턴트볼륨클레임 생성하기 퍼시스턴트볼륨클레임이 생성되자마자 쿠버네티스는 적절한 퍼시스턴트볼륨을 찾고 클레임에 바인딩한다. 용량은 퍼시스턴트볼륨클레임의 요청을 수용할만큼 충분히 커야하고, 볼륨 접근 모드는 클레임에서 요청한 접근모드를 포함하는 상태여야 바인딩이 이루어진다.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: resources: requests: # 1GiB의 스토리지를 요청 storage: 1Gi accessModes: - ReadWriteOnce # 단일 클라이언트를 지원하는 읽기/쓰기 스토리지 storageClassName: &quot;&quot;퍼시스턴트볼륨클레임 조회하기# 퍼시스턴트볼륨클레임 생성kubectl create -f mongodb-pvc.yaml# 퍼시스턴트볼륨클레임 조회kubectl get pvc퍼시스턴트볼륨 접근모드 RWO(ReadWriteOnce) : 단일 노드만이 읽기/쓰기용으로 볼륨을 마운트 ROX(ReadOnlyMany) : 다수 노드가 읽읽기용으로 볼륨을 마운트 RWX(ReadWriteMany) : 다수 노드가 읽기/쓰기용으로 볼륨을 마운트6.5.4 파드에서 퍼시스턴트볼륨클레임 사용하기apiVersion: v1kind: Podmetadata: name: mongodbspec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data # 파드 볼륨에서 이름으로 퍼시스턴트볼륨클레임을 참조 persistentVolumeClaim: claimName: mongodb-pvcmongodb test# 퍼시스턴트클레임을 이용하는 mongodb pod 생성kubectl create -f mongodb-pod-pvc.yaml# mongodb 셸 접속kubectl exec -it mongodb mongo# mongodb db 변경use mystore# 데이터 insertdb.foo.insert({name:&#39;foo&#39;})# 데이터 조회db.foo.find()6.5.5 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임 사용의 장점 이해하기 GCE 퍼시스터턴트 디스크를 직접 사용하는 경우와 PVC + PV를 사용하는 경우 비교 개발자가 직접 인프라스트럭처에서 스토리지를 가져오는 방식보다는 PV + PVC을 통해간접적으로 가져오는 방식이 더 간단하다(인프라스트럭처를 몰라도됨) 또한 동일한 파드와 클레임 매니페스트는 인프라스트럭처와는 관련된 어떤것도 참조하지 않으므로 다른 쿠버네티스 클러스터에서도 그대로 사용할 수 있다. “클레임은 x만큼의 스토리지가 필요하고 한 번에 하나의 클라이언트에서 읽기와 쓰기를 할 수 있어야 한다”만 명시한다.6.5.6 퍼시스턴트볼륨 재사용# mongodb pod 삭제kubectl delete pod mongodb# pvc 삭제kubectl delete pvc mongodb-pvc#pvc, pod 재생성 ( 이 경우 pvc는 바로 volume을 할당받지 못하고 Pending 상태가 된다.)kubectl create -f mongodb-pvc.yamlkubectl create -f mongodb-pod-pvc.yaml# pvc 조회kubectl get pvc# pv 조회 ( 퍼시스턴트볼륨의 상태가 Released로 표시되고 Available이 아니다. 그 이유는 이미 볼륨을 사용했기 떄문에 데이터를 가지고 있어서 새로운 클레임을 바인딩할 수 없는 상태)kubectl get pv퍼시스턴트볼륨을 수동으로 다시 클레임하기 persistentVolumeClaimPolicy를 Retain으로 설정하면 퍼시스턴트볼륨클레임이 해제되더라도 데이터가 남아있으면 상태가 Available로 풀리지 않는다.퍼시스턴트볼륨을 자동으로 다시 클레임하기 다른 리클레임 정책인 Recycle과 Delete가 있는데 Recycle은 볼륨의 콘텐츠를 삭제하고 다시 클레임될수 있도록 만드는 옵션이다. Delete 정책은 쿠버네티스에서 퍼시스턴트볼륨 오브젝트와 외부 인프라(예: AWS EBS, GCE PD, Azure Disk 또는 Cinder 볼륨)의 관련 스토리지 자산을 모두 삭제한다. Recycle과 Delete의 차이는 pvc가 삭제될때 pv까지 삭제하느냐 안하느냐에 대한 차이가 있음(Delete는 pvc를 삭제하면 pv까지 삭제함)6.6 퍼시스턴트볼륨의 동적 프로비저닝6.6.2 퍼시스턴트볼륨클레임에서 스토리지 클래스 요청하기특정 스토리지클래스를 요청하는 pvc 정의 클레임을 생성하면 fast 스토리지클래스 리소스에 참조된 프로비저너가 퍼시스턴트볼륨을 생성한다. PVC에서 존재하지 않는 스토리지클래스를 참조하면 PV 프로비저닝은 실패한다. kubectl describe 로 확인해보면 ProvisioningFailed 이벤트 표시됨.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: storageClassName: fast # PVC는 사용자 정의 스토리지 클래스를 요청 resources: requests: storage: 100Mi accessModes: - ReadWriteOnce스토리지클래스 정의apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: fastprovisioner: k8s.io/minikube-hostpathparameters: type: pd-ssd동적 프로비저닝된 PV와 생성된 PVC 조회 이렇게 생성된 PV는 리클레임 정책 Delete을 가지며, PVC가 삭제되면 PV도 삭제된다.# 스토리지클래스 생성kubectl create -f storageclass-fast-hostpath.yaml# PVC 생성kubectl create -f mongodb-pvc-dp.yaml# pvc 조회kubectl get pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pvc Bound pvc-b767134f-218a-48cc-b1a4-4787a661fd09 100Mi RWO fast 16m# pv 조회kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-b767134f-218a-48cc-b1a4-4787a661fd09 100Mi RWO Delete Bound default/mongodb-pvc fast 16m스토리지 클래스 사용하는 법 이해하기 스토리지클래스의 좋은 점은 클레임 이름으로 이를 참조한다는 사실. 그래서 다른 클러스터간 스토리지클래스 이름을 동일하게 사용한다면 PVC 정의를 다른 클러스터로 이식도 가능하다.6.6.3 스토리지 클래스를 지정하지 않는 동적 프로비저닝# 스토리지 클래스 조회kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEfast k8s.io/minikube-hostpath Delete Immediate false 18mstandard (default) k8s.io/minikube-hostpath Delete Immediate false 26h# 기본 스토리지 클래스 확인kubectl get sc standard -o yaml스토리지 클래스를 지정하지 않고 퍼시스턴트볼륨클레임 생성하기apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvc2spec: resources: requests: storage: 100Mi accessModes: - ReadWriteOnce storageClassName 속성을 지정하지 않고 PVC를 생성하면 구글 쿠버네티스 엔진에서는 pd-standard 유형의 퍼시스턴트 디스크가 프로비저닝된다.# 스토리지 클래스를 지정하지 않고 pvc 생성kubectl create -f mongodb-pvc-dp-nostorageclass.yaml# pvc 확인kubectl get pvc# pv 확인kubectl get pv퍼시스턴트볼륨클레임을 미리 프로비저닝된 퍼시스턴트볼륨으로 바인딩 강제화하기 storageClassName 속성을 빈 문자열로 지정하지 않으면 미리 프로비저닝된 퍼시스턴트볼륨이 있다고 할지라도 동적 볼륨 프로비저너는 새로운 퍼시스턴트볼륨을 프로비저닝한다. 미리 프로비저닝된 PV에 바인딩하기 위해서는(미리 만들어둔 PV에 바인딩하려면) 명시적으로 storageClassName을 ““로 지정해야 한다.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: &quot;&quot; # 빈 문자열을 스토리지클래스 이름으로 지정하면 PVC가 새로운 PV를 동적 프로비저닝하지 않고 미리 프로비저닝된 PV에 바인딩된다.퍼시스턴트볼륨 동적 프로비저닝의 플로우 클러스터 관리자는 퍼시스턴트볼륨 프로비저너를 설정하고, 하나 이상의 스토리지 클래스를 생성하고 기본값 정의 사용자는 스토리지클래스 중 하나를 참조해 PVC를 생성 PVC는 스토리지클래스와 거기서 참조된 프로비저너를 보고 PVC로 요청된 접근모드, 스토리지 크기, 파라미터를 기반으로 새 PV를 프로비저닝하도록 요청 프로비저너는 스토리지를 프로비저닝하고 PV를 생성한 후 PVC에 바인딩한다. 사용자는 PVC를 이름으로 참조하는 볼륨과 파드를 생성6.7 요약 다중 컨테이너 파드 생성과 파드의 컨테이너들이 볼륨을 파드에 추가하고 각 컨테이너에 마운트해 동일한 파일로 동작하게 할 수 있다. emptyDir 볼륨을 사용해 임시, 비영구 데이터를 저장할 수 있다. gitRepo 볼륨을 사용해 파드의 시작 시점에 깃 리포지터리의 콘텐츠로 디렉터리를 쉽게 채울수 있다. hostpath 볼륨을 사용해 호스트 노드의 파일에 접근한다. 외부 스토리지를 볼륨에 마운트해 파드가 재시작돼도 파드의 데이터를 유지한다. 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임을 사용해 파드와 스토리지 인프라스트럭처를 분리할수 있다. 스토리지클래스를 이용하면 PVC가 원하는 만큼의 PV를 프로비저닝할 수 있다. PVC을 미리 프로비저닝된 PV에 바인딩하고자 할 때 동적 프로비저너가 간섭하는 것을 막을 수도 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 5. 서비스 : 클라이언트가 파드를 검색하고 통신을 가능하게 함.", "url": "/posts/devlog-platform-kubernetes-in-action5/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-16 17:34:00 +0900", "snippet": "5. 서비스 : 클라이언트가 파드를 검색하고 통신을 가능하게 함. 파드가 다른 파드에게 제공하는 서비스를 사용하려면 다른 파드를 찾는 방법이 필요하다.쿠버네티스에서는 서비스를 제공하는 서버의 정확한 IP 주소나 호스트 이름을 지정해 각 클라이언트 앱을 구성하는것과는 다른 방식이 필요하다.기존의 방식과 다른 방식이 필요한 이유 쿠버네티스 파드는 일시적이고, 언제든 다른 노드로 이동될 수 있기 때문(클러스터 IP는 언제든 변경될수 있다.) 쿠버네티스는 노드에 파드를 스케줄링한 후 파드가 시작되기 바로 직전에 파드의 IP주소를 할당함. 따라서 클라이언트에서 특정 파드의 IP주소를 미리 알 수 없다. 오토 스케일링 등을 위해서는 여러 파드가 동일한 서비스로 제공할 수 있어야 하는데, 각 파드마다 고유한 IP주소가 있고, 클라이언트는 서비스를 지원하는 파드의 수와 IP에 상관없이 단일 IP주소로 모든 파드에 엑세스할수 있어야 한다.5.1 서비스 소개 쿠버네티스의 서비스는 동일한 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들어주는 리소스이다.서비스 설명 서비스를 만들고 클러스터 외부에서 엑세스할 수 있도록 구성하면 외부 클라이언트가 파드에 연결할 수 있는 하나의 고정 IP가 노출 서비스가 관리하는 파드들의 IP가 변경되더라도 서비스의 IP주소는 변경되지 않는다. 내부 클라이언트와 외부 클라이언트 모두 서비스로 파드에 접속5.1.1 서비스 생성 서비스를 지원하는 파드가 한개 혹은 그 이상일 수 있다. 서비스 연결은 뒷단의 모든 파드로 로드밸런싱된다. 서비스에서도 레플리카셋과 동일하게 레이블 셀렉터 매커니즘을 그대로 적용된다.kubectl expose 서비스를 생성하기 가장 쉬운 방법은 kubectl expose 명령어를 사용하는 것이다. expose 명령어를 사용하는 것 대신 kubernetes 명령어를 이용해 서비스 리소스를 생성할 수도 있다.YAML 디스크립터를 통한 서비스 생성apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - port: 80 # 서비스가 사용할 포트 targetPort: 8080 # 서비스가 포워드할 컨테이너 포트 selector: app: kubia # app=kubia 레이블이 있는 모든 파드가 이 서비스에 포함된다는것을 의미새 서비스 검사하기# 서비스 생성kubectl create -f kubia-svc.yaml# 서비스 정보 조회 ( 내부 클러스터 IP가 할당되었는지 확인 )kubectl get svc 이렇게 클러스터 IP가 할당되면 클러스터 내부에서는 바로 엑세스할 수 있다.실행인 컨테이너에 원격으로 명령어 실행 kubectl exec 명령어를 사용하면 기존 파드의 컨테이너 내에서 원격으로 임의의 명령어를 실행할 수 있다.# 특정 Pod에 접속하여 서비스의 클러스터 IP로 http 요청kubectl exec kubia-4dkws -- curl -s http://10.102.206.16# pod shell 접근kubectl exec --stdin --tty kubia-4dkws -- /bin/bash더블 대시를 사용하는 이유 명령어의 더블 대시(–)는 kubectl 명령줄 옵션의 끝을 의미함. 더블 대시 뒤의 모든 것은 파드 내에서 실행돼야 하는 명령이다. kubectl exec kubia-4dkws -- curl -s http://10.102.206.16 의 예제에서 더블 대시가 없다면 -s 옵션은 kubectl exec의 옵션으로 해석하여 처리 되지 않는다.서비스의 세션 어피니티 구성 동일한 클라이언트에서 요청하더라도 서비스 프록시가 각 연결을 임의의 파드를 선택해서 연결을 다시 전달(forward)하기 때문에 요청할 때마다 다른 파드가 선택된다. 특정 클라이언트의 모든 요청을 매번 같은 파드로 리디렉션하려면 서비스의 세션 어피니티(sessionAffinity)속성을 기본값 None 대신 ClientIP로 설정하면 된다. 쿠버네티스에서는 None, ClientIP두 가지 유형의 서비스 세션 어피니티만 지원. 서비스 레벨에서는 HTTP 수준에서는 작동하지 않고 TPC / UDP 패킷을 처리하고 그들이 가지고 있는 payload는 신경쓰지 않는다.(쿠키 기반으로 할 수 없음)apiVersion: v1kind: Servicemetadata: name: kubiaspec: sessionAffinity: ClientIP # 동일한 클라이언트 IP의 모든 요청을 동일한 파드로 전달동일한 서비스에서 여러 개의 포트 노출 파드가 2개의 포트(http : 8080, https : 8443)을 수신한다면 하나의 서비스를 사용해 포트 80과 433을 파드의 포트 8080과 8443으로 전달할 수 있음. 하나의 서비스를 사용해 멀티 포트 서비스를 사용하면 단일 클러스터 IP로 모든 서비스 포트가 노출된다.이름이 지정된 포트 사용 포트 번호가 잘 알려진 경우가 아니더라도 서비스 스펙을 좀 더 명확히 할 수 있는 방법. 나중에 서비스 스펙을 변경하지 않고도 pod의 포트 번호를 변경할 수 있다는 큰 장점이 있음.# 파드 정의에 포트 이름 사용kind: Podspec: ports: - name: http containerPort: 8080 - name: https containerPort: 8443# 서비스에 이름이 지정된 포트 참조apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - name: http port: 80 targetPort: http # 포트 80은 컨테이너 포트의 이름이 http인 것에 매핑 - name: https port: 443 targetPort: https # 포트 443은 컨테이너 포트의 이름이 https인 것에 매핑 selector: app: kubia5.1.2 서비스 검색 서비스의 파드는 생성되기도 하고 사라지기도 하고, 파드 IP가 변경되거나 파드 수는 늘어나거나 줄어들 수도 있다. 이러한 상황 속에서 항상 서비스의 IP주소로 엑세스할 수 있어야 한다. 쿠버네티스는 클라이언트 파드가 서비스의 IP와 포트를 검색할 수 있는 방법을 제공한다.환경변수를 통한 서비스 검색 파드가 시작되면 쿠버네티스는 해당 시점에 존재하는 각 서비스를 가리키는 환경변수 세트를 초기화한다. 환경변수 네이밍 규칙은 서비스 이름의 대시(-)는 밑줄(_)로 변환되고 서비스 이름이 환경변수 이름의 접두어로 쓰이면서 모든 문자는 대문자로 표시한다.kubectl exec kubia-4dkws env# KUBIA_SERVICE_HOST=10.102.206.16 # 서비스 클러스터 IP# KUBIA_PORT_80_TCP_PORT=80 # 서비스가 제공되는 포트DNS를 통한 서비스 검색 kube-system 네임스페이스에 파드 중 kube-dns라는 게 있었다. kube-dns는 DNS 서버를 실행하며 클러스터에서 실행중인 다른 모든 파드는 자동으로 이를 사용하도록 구성되는 파드이다. 파드에서 실행중인 프로세스에서 수행된 모든 DNS 쿼리는 시스템에서 실행중인 모든 서비스를 알고 있는 쿠버네티스의 자체 DNS 서버로 처리된다. 각 서비스는 내부 DNS 서버에서 항목을 가져오고 서비스 이름을 알고 있는 클라이언트 파드는 환경변수 대신 FQDN(정규화된 도메인 이름)으로 엑세스 할수 있다.FQDN을 통한 서비스 연결 backend-database.default.svc.cluster.local“backend-database” -&amp;gt; 서비스 이름“default” -&amp;gt; 네임스페이스“svc.cluster.local” -&amp;gt; 모든 클러스트의 로컬 서비스 이름에 사용되는 도메인 접미사 클라이언트는 여전히 서비스의 포트번호를 알아야 한다. 표준 포트가 아닌 경우 문제가 될수 있다.(환경변수에서 포트 번호를 얻을수 있어야 함) 접미사와 네임스페이스는 생략이 가능하다. 위 예제에서는 “backend-database” FQDN만으로 서비스에 엑세스할 수 있다.파드의 컨테이너 내에서 셸 실행 파드 컨테이너 내부의 DNS resolver가 구성되어 있기 때문에 네임스페이스와 접미사를 생략할 수 있다.# 파드 컨테이너 냉서 셸 실행kubectl exec -it kubia-4dkws bash# DNS resolver 확인cat /etc/resolv.conf# FQDN을 통한 서비스 호출(모두 같은 결과)curl http://kubia.default.svc.cluster.localcurl http://kubia.defaultcurl http://kubia서비스 IP에 핑을 할 수 없는 이유 서비스로 crul은 동작하지만 핑은 응답이 오지 않는다. 이는 서비스의 클르서트 IP가 가상 IP이므로 서비스 포트와 결합된 경우에만 의미가 있기 때문5.2 클러스터 외부에 있는 서비스 연결 클러스터에서 실행중인 파드는 내부 서비스에 연결하는 것처럼 외부 서비스에 연결할 수 있다.5.2.1 서비스 엔드포인트 소개 서비스는 파드에 직접 연결(link)되지 않는다. 대신 엔드포인트 리소스가 그 사이에 있다. 파드 셀렉터는 서비스 스펙에 정의돼 있지만 들어오는 연결을 전달할 때 직접 사용하지 않고, IP와 포트 목록을 작성하는데 사용되며, 엔드포인트 리소스에 저장된다.# service 정보 조회kubectl describe svc kubia# kubia endpoints 조회kubectl get endpoints kubia5.2.2 서비스 엔드포인트 수동 구성 서비스의 엔드포인트를 서비스와 분리하면 엔드포인트를 수동으로 구성하고 업데이트할수 있다. 수동으로 관리되는 엔드포인트를 사용해 서비스를 만들려면 서비스와 엔드포인트 리소스를 모두 만들어야 한다.셀렉터 없이 서비스 생성apiVersion: v1kind: Servicemetadata: name: external-service # 엔드포인트 오브젝트 이름과 일치해야 함.spec: # spec에 selector를 정의하지 않음. ports: - port: 80셀렉터가 없는 서비스에 관한 엔드포인트 리소스 생성 엔드포인트 오브젝트는 서비스 이름과 같아야 하고, 서비스를 제공하는 대상 IP주소와 포트 목록을 가져야 함.apiVersion: v1kind: Endpointsmetadata: name: external-service # 서비스 이름과 일치시킴.subsets: - addresses: # 서비스가 연결을 전달할 엔드포인트 IP 생성 - ip: 11.11.11.11 - ip: 22.22.22.22 ports: - port: 80 # 엔드포인트의 대상 포트외부 엔드포인트를 가지는 서비스를 만들어야 하는 목적 나중에 외부 서비스를 쿠버네티스 내에서 실행되는 파드로 마이그레이션하기로 한 경우 서비스에 셀렉터를 추가해 엔드포인트를 자동으로 관리 할 수 있다. 이를 통해 서비스의 실제 구현이 변경되는 동안에도 서비스 IP 주소가 일정하게 유지될 수 있다.5.2.3 외부 서비스를 위한 별칭 생성ExternalName 서비스 생성 외부 서비스의 별칭으로 하려는 경우 유형(type) 필드를 ExternalName으로 설정하면 된다.apiVersion: v1kind: Servicemetadata: name: external-servicespec: type: ExternalName # ExternalName 유형 externalName: api.somecompany.com # FQDN(Fully Qualified Domain Name) 이름 지정 ports: - port: 80 여기서 FQDN을 사용하는 대신 external-service.default.svc.clster.local 도메인 이름으로 외부 서비스에 연결할수도 있다. ExternalName 서비스는 DNS 레벨에서만 구현된다. 서비스에 연결하는 클라이언트는 서비스 프록시를 완전히 무시하고 외부 서비스에 직접 연결된다. 이러한 이유로 Cluster IP를 얻을 수 없음.5.3 외부 클라이언트에 서비스 노출 쿠버네티스느 외부에서 서비스를 엑세스할 수 있는 방법을 몇가지 제공해준다.5.3.1 노트포트 서비스 서비스를 생성하고 유형을 노드포트로 설정하는 방법노드포트 서비스 생성 nodePort를 생략할 경우 쿠버네티스가 임의의 포트를 선택한다.apiVersion: v1kind: Servicemetadata: name: kubia-nodeportspec: type: NodePort # 노드포트 서비스 유형 ports: - port: 80 # 서비스 클러스터 IP 포트 targetPort: 8080 # 서비스 대상 파드의 포트 nodePort: 30123 # 각 클러스터 노드의 포트 30123을 통해 서비스에 엑세스 할수 있음. selector: app: kubia노드포트 서비스 확인# 노드포트 서비스 생성kubectl create -f kubia-svc-nodeport.yaml# 노드포트 서비스 확인# kubectl get svc kubia-nodeport# 노드 Ip 조회(minikube에서는 안됨)kubectl get nodes -o jsonpath=&#39;{.items[*].status.addresses[?(@.type==&quot;ExternalIP&quot;)].address}&#39;# minikube nodeport service(노드포트 서비스를 로컬에서 접근 가능하도록 처리)minikube service kubia-nodeportcrul http://127.0.0.1:53446/|-----------|----------------|-------------|-------------------------|| NAMESPACE | NAME | TARGET PORT | URL ||-----------|----------------|-------------|-------------------------|| default | kubia-nodeport | 80 | http://172.17.0.2:30123 ||-----------|----------------|-------------|-------------------------|🏃 Starting tunnel for service kubia-nodeport.|-----------|----------------|-------------|------------------------|| NAMESPACE | NAME | TARGET PORT | URL ||-----------|----------------|-------------|------------------------|| default | kubia-nodeport | | http://127.0.0.1:53446 ||-----------|----------------|-------------|------------------------|노드포트 서비스의 단점 클라이언트가 하나의 노드에만 요청하는 경우 노드에 장애가 발생할 경우 더 이상 서비스에 엑세스할 수 없으므로, 노드 앞에 로드밸런서를 배치하는 것이 좋다.5.3.2 외부 로드밸런서로 서비스 노출 쿠버네티스 클러스터는 로드밸런서를 자동으로 프로비저닝하는 기능을 제공한다. 쿠버네티스가 로드밸런서 서비스를 지원하지 않는 환경에서 실행중인 경우 로드밸런서는 프로비저닝되지는 않지만 서비스는 여전히 노드포트 서비스처럼 작동한다.로드밸런서 서비스 생성apiVersion: v1kind: Servicemetadata: name: kubia-loadbalancerspec: type: LoadBalancer # 쿠버네티스 클러스터를 호스팅하는 인프라에서 로드밸런서를 얻을수 있다. ports: - port: 80 targetPort: 8080 selector: app: kubia로드밸런서를 통한 서비스 연결# 로드밸런서 서비스 생성kubectl create -f kubia-svc-loadbalancer.yaml# 확인kubectl get svc kubia-loadbalancer세션 어피니티와 웹 브라우저 웹 브라우저에서 세션 어피티니가 None이더라도 같은 파드로 계속 요청하는 현상을 볼수 있음. 그 이유는 브라우저의 http keep-alive header을 사용하기 때문이다.5.3.3 외부 연결의 특성 이해불필요한 네트워크 홉의 이해와 예방 외부 클라이언트가 노드포트로 서비스에 요청할 경우 임의로 선택된 파드가 연결을 수신한 동일한 노드에서 실행중일 수도 있고, 그렇지 않을 수도 있다. 파드에 도달하려면 추가적인 네트워크 홉이 필요할 수 있으며 이것이 항상 바람직한 것은 아니다. 요청을 수신한 노드에서 실행중인 파드로만 외부 트래픽을 전달하도록 서비스를 구성해 추가 홉을 방지할 수 있는 옵션을 제공한다.spec: externalTrafficPolicy: LocalexternalTrafficPolicy을 이용할때 주의사항 서비스 프록시는 로컬에 실행중인 파드를 선택하는데 로컬 파드가 업으면 요청을 중단시킨다.(로드밸런서는 파드가 하나 이상 있는 노드에만 연결을 전달하도록 해야 함) 모든 파드에 균등하게 분산되지 않을 수 있다.클라이언트 IP가 보존되지 않음 인식 노드포트로 연결을 수신하면 패킷에서 소스 네트워크 주소 변환(SNAT)이 수행되므로 패킷의 소스 IP가 변경된다. 웹 서버의 경우 엑세스 로그에 브라우저의 IP를 표시할 수 없다는 것은 의미함.. 로컬 외부 트래픽 정책(Local External Traffic Policy은 연결을 수신하는 노드와 대상 파드를 호스팅하는 노드 사이에 추가 홉이 없기 때문에 클라이언트 IP 보존에 영향을 미친다.5.4 인그레스 리소스로 서비스 외부 노출인그레스가 필요한 이유 인그레스는 한 IP 주소로 수십 개의 서비스에 접근이 가능하도록 지원한다. 네트워크 스택의 애플리케이션 계층(HTTP)에서 작동하며, 서비스가 할 수 없는 쿠키 기반 세션 어피니티 등과 같은 기능 제공이 가능하다.Minikube에서 인그레스 애드온 활성화# minikube addons 확인minikube addons list# ingress addons 활성화( minikube start --vm=true 로 시작해야 가능)minikube addons enable ingress# 기존에 설치된 docker 기반 minikube deleteminikube delete# minikube vm 모드로 시작minikube start --vm=true --driver=hyperkit# 모든 네임스페이스 pods 조회kubectl get po --all-namespaces5.4.1 인그레스 리소스 생성apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com # 인그레스는 kubia.example.com 도메인 이름으로 서비스에 매핑된다. http: paths: - path: / backend: serviceName: kubia-nodeport # 모든 요청은 kubia-nodeport 서비스의 포트 80으로 전달된다. servicePort: 805.4.2 인그레스 서비스 엑세스# ingress 리소스 생성kubectl create -f kubia-ingress.yaml# ingresses 조회kubectl get ingresses# host 설정sudo vi etc/hosts192.168.64.2 kubia.example.com# 호출curl http://kubia.example.com인그레스 동작 방식5.4.3 하나의 인그레스로 여러 서비스 노출 규칙과 경로가 모두 배열이라 여러 항목을 가질 수 있다. 여러 호스트(host)와 경로(path)를 여러 서비스(backend.serviceName)에 매핑할 수 있다.동일한 호스트의 다른 경로로 여러 서비스 매핑 - host: kubia.example.com http: paths: - path: /kubia backend: # kubia.example.com/kubia으로의 요청을 kubia 서비스로 라우팅된다. serviceName: kubia servicePort: 80 - path: /bar backend: # kubia.example.com/bar으로의 요청을 bar 서비스로 라우팅된다. serviceName: bar servicePort: 80서로 다른 호스트로 서로 다른 서비스 매핑하기spec: rules: - host: foo.example.com http: paths: - path: / backend: # foo.example.com으로의 요청을 foo 서비스로 라우팅된다. serviceName: foo servicePort: 80 - host: bar.example.com http: paths: - path: / backend: # bar.example.com으로의 요청을 bar서비스로 라우팅된다. serviceName: bar servicePort: 805.4.4 TLS 트래픽을 처리하도록 인그레스 구성인그레스를 위한 TLS 인증서 생성# 개인키 생성openssl genrsa -out tls.key 2048# 인증서 생성openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com# 시크릿 리소스 생성kubectl create secret tls tls-secret --cert=tls.cert --key=tls.keyCertificateSigningRequest 리소스로 인증서 서명 인증서를 직접 서명하는 대신 CSR 리소스를 만들어 인증서에 서명할 수 있음.kubectl certificate approve &amp;lt;name of th CSR&amp;gt;tls 트래픽을 처리하는 인그레스 생성apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubia-tlsspec: tls: # TLS 구성 - hosts: - kubia.example.com secretName: tls-secret # 개인키와 인증서는 위에서 생성한 tls-secret 참조 rules: - host: kubia.example.com http: paths: - path: / backend: serviceName: kubia-nodeport servicePort: 80# https request 호출curl -k -v https://kubia.example.com/kubia5.5 레디니스 프로브 ( 파드가 연결을 수락할 준비가 됐을 때 신호 보내기 ) 파드는 구성에 시간이 걸릴 수 있다. 데이터를 로드하는 데 시간이 걸리거나, 첫번 째 사용자 요청이 너무 오래걸리거나 사용자 경험에 영향을 미치는 것을 방지하고자 준비 절차를 수행해야 할 수도 있다.5.5.1 레디니스 프로브 소개 라이브니스 프로브와 비슷하게 파드에 레디니스 프로브를 정의할 수 있다. 주기적으로 호출되며 특정 파드가 클라이언트 요청을 수신할 수 있는지를 결정한다. 애플리케이션 특성에 따라 상세한 레디니스 프로브를 작성하는 것은 개발자의 몫레디니스 프로브 유형 Exec 프로브 : 프로세스를 실행 HTTP GET 프로브 : HTTP GET 요청 수행 TCP 소켓 프로브 : 컨테이너에 TCP 연결 확인레디니스 프로브의 동작 컨테이너가 시작될 때 쿠버네티스는 첫 번째 레디니스 점검을 수행하기 전에 구성 가능한 시간이 경과하기를 기다릴 수 있도록 구성 가능 주기적으로 프로브를 호출하고 레디니스 프로브의 결과에 따라 작동 파드가 준비되지 않았다고 하면 서비스에서 제거하고, 파드가 준비되면 서비스에 다시 추가한다. 컨테이너가 준비 상태 점검에 실패하더라도 컨에티너가 종료되거나 다시 시작시키지 않는다.라이브니스 프로브 vs 레디니스 프로브 라이브니스 프로브는 상태가 좋지 않은 컨테이너를 제거하고 새롭고 건강한 컨테이너로 교체해 파드의 상태를 정상으로 유지시킨다. 레디니스 프로브는 요청을 처리할 준비가 된 파드의 컨테이너만 요청을 수신하도록 한다.(L7 health check와 유사)레디니스 프로브가 중요한 이유 파드 그룹이 다른 파드에서 제공하는 서비스에 의존한다고 했을때(웹 앱 -&amp;gt; 백엔드 데이터베이스) 웹 앱 파드중 하나만 DB에 연결할 수 없는 경우, 요청을 처리할 준비가 되지 않았다고 신호를 주는게 현명할 수 있다. 레디니스 프로브를 사용하면 클라이언트가 정상 상태인 파드하고만 통신할 수 있다. 그래서 시스템에 문제가 있다는 것을 절대 알아차리지 못한다.5.5.2 파드에 레디니스 프로브 추가# replicaSet 수정kubectl edit rs kubia# yamlapiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: sungsu9022/kubia ports: - name: http containerPort: 8080 readinessProbe: # 파드의 각 컨테이너에 레디니스 프로브를 정의 exec: # ls /var/ready 명령어를 주기적으로 수행하여 존재하면 0(성공), 그렇지 않으면 다른 값(실패) command: - ls - /var/ready 이렇게 하면 /var/ready 파일이 없으므로 READY에 준비된 컨테이너가 없다고 표시된다.# /var/ready 파일 생성kubectl exec kubia-4wjqj -- touch /var/ready# 확인(kubia-4wjqj 가 READY 1/1로 변경됨)kubectl get pods# 레디니스 프로브 조회kubectl describe pod kubia-4wjqj# 하나의 READY 파드로 서비스를 호출curl http://kubia.example.com5.5.3 실제 환경에서 레디니스 프로브가 수행해야 하는 기능 서비스에서 파드를 수동으로 추가하거나 제거하려면 파드와 서비스의 레이블 셀렉터에 enabled=true 레이블을 추가한다. 서비스에서 파드를 제거하려면 레이블을 제거하라.레디니스 프로브를 항상 정의하라 파드의 레디니스 프로브를 추가하지 않으면 파드가 시작하는 즉시 서비스 엔드포인트가 된다. 여전히 시작 단계로 수신 연결을 수락할 준비가 되지 않은 상태에서 파드로 전달된다. 따라서 클라이언트가 Connection Refused 유형의 에러를 보게 된다. 기본 URL에 HTTP 요청을 보내더라도 항상 레디니스 프로브를 정의해야 한다.레디니스 프로브에 파드의 종료 코드를 포함하지 마라 파드가 종료할 때, 실행되는 앱은 종료 신호를 받자마자 연결 수단을 중단한다. 쿠버네티스는 파드를 삭제하자마자 모든 서비스에서 파드를 제거하기 때문에 굳이 별도로 이런 처리를 할 필요가 없다.5.6. 헤드리스 서비스로 개별 파드 찾기 클라이언트가 모든 파드에 연결해야 하는 경우 어떻게 할수 있을까? 파드가 다른 파드에 각각 연결해야 하는 경우 어떻게 해야 할까? 클라이언트가 모든 파드에 연결하려면 각 파드의 IP를 알아야 한다. 쿠버네티스는 클라이언트가 DNS 조회로 파드 IP를 찾을 수 있도록 한다. 쿠버네티스 서비스에 클러스터 IP가 필요하지 않다면 ClusterIP 필드를 None으로 설정하여 DNS 서버는 하나의 서비스 IP 대신 파드 IP 목록들을 반환한다. 이때 각 레코는 현재시점 기준으로 서비스를 지원하는 개별 파드의 IP를 가리킨다. 따라서 클라이언트는 간단한 DNS A 레코드 조회를 수행하고 서비스에 포함된 모든 파드의 IP를 얻을 수 있다.5.6.1 헤드리스 서비스 생성 서비스 스펙의 clusterIP필드를 None으로 설정하면 클라이언트가 서비스의 파드에 연결할 수 있는 클러스터 IP를 할당하지 않기 떄문에 서비스가 헤드리스 상태가 된다. 클러스터 Ip가 없고 엔드포인트에 파드 셀렉터와 일치하는 파드가 포함돼 있음을 확인할 수 있다.apiVersion: v1kind: Servicemetadata: name: kubia-headlessspec: clusterIP: None # 헤드리스 서비스로 만드는 spec 옵션 ports: - port: 80 targetPort: 8080 selector: app: kubia5.6.2 DNS로 파드 찾기 실제로 클라이언트 관점에서는 헤드리스 서비스를 사용하나 일반 서비시를 사용하나 관계 없이 서비스의 DNS 이름에 연결해 파드에 연결할 수 있다. 차이점이 있다면 헤드리스 서비스의 경우 클라이언트는 서비스 프록시 대신 파드에 직접 연결한다. 헤드리스 서비스는 여전히 파드간에 로드밸런싱을 제공하지만 서비스 프록시 대신 DNS 라운드 로빈 매커니즘으로 처리된다.# dnsutils pod 생성kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity# 헤드리스 서비스를 위해 반환된 DNS A 레코드( 레코드 목록이 표시된다. )kubectl exec dnsutils nslookup kubia-headless# 일반 서비스 nslookup ( 클러스터 IP가 표시된다. )kubectl exec dnsutils nslookup kubia5.6.3 모든 파드 검색 - 준비되지 않은 파드도 포함 쿠버네티스가 파드의 레디니스 상태에 관계 없이 모든 파드를 서비스에 추가되게 하려면 서비스에 다음 어노테이션을 추가해야 한다. tolerate-unready-endpoints는 deprecated되었고, publishNotReadyAddresses를 사용해서 동일한 기능을 처리할 수 있다.( https://github.com/kubedb/project/issues/242 )kind: Service#metadata:# annotations:# service.alpha.kubernetes.io/tolerate-unready-endpoints: &quot;true&quot;spec publishNotReadyAddresses: true5.7 서비스 문제 해결 FAQ 서비스로 파드에 엑세스할 수 없는 경우 다음 내용을 확인해보면 도움이 된다. 외부가 아닌 클러스터 내에서 서비스의 클러스터 IP에 연결되는지 확인 서비스에 엑세스할 수 있는지 확인하려고 서비스 IP로 핑을 하는 케이스(핑 동작 안함) 레디니스 프로브를 정의했다면 성공했는지 확인하라, pod의 READY 여부 확인 파드가 서비스의 일부인지 확인하려면 kubectl get endpoints를 사용해 해당 엔드포인트 오브젝트를 확인하라. FQDN이나 그 일부로 서비스에 엑세스하려고 하는데 작동하지 않는 경우 FQDN 대신 클러스터 IP를 사용해 엑세스할수 있는지 확인하라 대상 포트가 아닌 서비스로 노출된 포트에 연결하고 있는지 확인하라 파드 IP에 직접 연결해 파드가 올바른 포틍 연결돼있는지 확인하라 앱이 로컬호스트에만 바인딩하고 있는지 확인하라. 5.8 요약 서비스는 안정된 단일 IP 주소와 포트로 특정 레이블 셀렉터와 일치하는 여러 개의 파드를 노출 기본적으로 클러스터 내부에서 서비스에 엑세스할 수 있지만 유형을 노드포트 또는 로드밸런서로 설정해 클러스터 외부에서 서비스에 엑세스할 수 있다. 파드는 환경변수를 검색해 IP 주소와 포트로 서비스를 검색 할수 있다. 엔드포인트 리소스를 만드는 대신 셀렉터 설정 없이 서비스 리소스를 생성해 클러스터 외부에 있는 서비스를 검색하고 통신할 수 있다. ExternalName 서비스 유형으로 외부 서비스에 대한 DNS CNAME(별칭)을 제공할 수 있다. 단일 인그레스로 여러 HTTP 서비스를 노출할 수 있다. 파드 컨테이너의 레디니스 프로브는 파드를 서비스 엔드포인트에 포함해야 하는지 여부를 결정한다. 헤드리스 서비스를 생성하면 DNS로 파드 IP를 검색할 수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 4. 레플리케이션과 그 밖의 컨트롤러 : 관리되는 파드 배포", "url": "/posts/devlog-platform-kubernetes-in-action4/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-07 17:34:00 +0900", "snippet": "4. 레플리케이션과 그 밖의 컨트롤러 : 관리되는 파드 배포4.1 파드를 안정적으로 유지하기 쿠버네티스를 사용하면 얻을 수 있는 주요 이점은 쿠버네티스에 컨테이너 목록을 제공하면 해당 컨테이너를 클러스터 어딘가에서 계속 실행되도록 할수 있는 것이다. 파드가 노드에 스케줄링되면 노드의 Kubelet은 이 파드가 존재하는 한 컨테이너가 계속 실행되도록 하고, 컨테이너의 주 프로세스에 크래시가 발생하면 Kubelet이 컨테이너를 다시 시작시킨다. 하지만 JVM 기준으로 OOM이 발생하거나 애플리케이션 무한 루프나 교착상태에 빠져서 응답을 주지 못하는 경우 앱을 재실행하려면 앱 내부의 기능에 의존해서는 안되고, 외부에서 앱의 상태를 체크해야 한다.4.1.1. 라이브니스 프로브 (Liveness probe) 쿠버네티스는 라이브니스 프로브를 통해 컨테이너가 살아 있는지 확인할 수 있다. 파드의 스펙에 각 컨테이너의 라이브니스 프로브를 지정하면 된다. 쿠버네티스는 주기적으로 프로브를 실행하고 프로브가 실패할 경우 컨테이너를 다시 시작한다.쿠버네티스의 프로브 매커니즘1) HTTP GET 프로브 지정한 IP, 포트, 경로에 HTTP GET 요청을 수행 프로브가 응답을 수신하고 응답코드가 오류가 아닌 경우 성공(2xx, 3xx) 오류 응답코드이면 실패로 간주(4xx, 5xx)2) TCP 소켓 프로브 컨테이너의 지정된 포트에 TCP 연결을 수행 연결이 성공하면 성공, 실패하면 실패3) Exec 프로브 컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드 확인 상태 코드가 0이면 성공, 다른코드는 모두 실패로 간주4.1.2 HTTP 기반 라이브니스 프로브 생성apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: luksa/kubia-unhealthy name: kubia livenessProbe: # 라이브니스 프로브 추가 httpGet: path: / # HTTP 요청 경로 port: 8080 # 프로브가 연결해야 하는 포트4.1.3 동작중인 라이브니스 프로브 확인# 라이브니스 프로브 생성kubectl create -f kubia-liveness-probe.yaml# 라이브니스 프로브 확인kubectl get po kubia-liveness# 크래시된 컨테이너의 애플리케이션 로그 조회kubectl logs {podName} --previous# pod이 다시 시작되는 이유 확인kubectl describe po kubia-livenessPod Last State Exit Code 정상적으로 종료된 경우 0 외부에 의해서 종료된 경우 128 + x의 값을 가진다. x는 프로세스에 전송된 시그널 번호임. SIGKILL 번호인 9로 강제종료된다면 128+9 = 137이 된다. 4.1.4 라이브니스 프로브의 추가 속성 설정kubectl describe을 통해 라이브니스 추가 정보를 확인할 수 있다.Liveness : http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 delay(지연) - 컨테이너가 시작된 후 바로 프로브 시작된다. timeout(제한시간) - 컨테이너가 제한시간 안에 응답이 와야 한다. period(기간) - 기간마다 프로브를 수행한다 #failure(실패수) - n번 연속 실패시 컨테이너가 다시 시작한다.설정 정의 initialDelaySeconds 옵션을 정의하여 설정할 수 있다.apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: luksa/kubia-unhealthy name: kubia livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 15라이브니스 프로브 정의시 주의사항 애플리케이션 시작시간을 고려해서 초기 지연을 설정해야 한다.4.1.5 효과적인 라이브니스 프로브 생성 운영환경이라면 반드시 라이브니스 프로브를 정의하는 것이 좋다.(L7 health check 같은 느낌으로) 정의하지 않으면 쿠버네티스가 앱이 살아 있는지 알 수 있는 방법이 없음.(프로세스가 떠있더라도 실제 문제 상황이 있을 수 있기 때문)라이브니스 프로브가 확인해야 할 사항 URL에 요청하도록 프로브를 구성해 앱 내 실행중인 모든 주요 구성요소가 살아있는지 또는 응답이 없는지 확인하도록 구성할 수도 있다. 특정 가벼운 API를 호출해본다던지 별도의 헬스체크 URL을 백엔드 엔드포인트로 구성한다던지 라이브니스 프로브는 앱 내부만 체크하고 외부 요인의 영향을 받지 않도록 구성해야 한다. 예를 들면 DB 서버에 문제가 있는 경우 이 프로브가 실패하도록 구성하는것은 지양해야 한다. 그 이유는 근본적인 원인이 DB라면 앱 컨테이너를 재시작한다 하더라도 문제가 해결되지 않기 때문이다. 프로브를 가볍게 유지하기 기본적으로 프로브는 비교적 자주 실행되며 1초 내에 완료되어야 한다. 너무 많은 일을 하는 프로브는 컨테이너 속도를 저하 시킬수 있다. 컨테이너에서 JVM 과 같은 기동 절차에 상당한 연산 리소스가 필요한 경우 Exec 프로브보다는 HTTP GET 라이브니스 프로브가 적합하다.프로브에 재시도 루프를 구현하지 마라. 프로브의 실패 임계값을 설정할수 있기도 하고, 실패 임계값이 1이더라도 쿠버네티스는 실패를 한번 했다고 간주하기 전에 여러번 재시도를 한다. 따라서 자체적인 재시도 루프를 구현하지 말아야 한다.라이브니스 프로브 요약 라이브니스 프로브에 대한 로직은 해당 워커 노드의 Kubelet에서 수행이 된다. 만약 워커 노드 자체에 크래시가 발생한 경우 해당 노드의 중단된 모든 파드의 대체 파드를 생성해야 하는것은 컨트롤 플레인의 역할 레플리케이션컨트롤러같은 리소스 외에 직접 생선한 Pod들은 워커 노드 자체가 고장나면 아무것도 할 수 없음.4.2 레플리케이션 컨트롤러 소개 레플리케이션 컨트롤러는 파드가 항상 실행되도록 보장하는 쿠버네티스 리소스이다. 파드의 여러 복제본(레플리카)을 작성하고 관리하기 위한 리소스이다.동작 원리 노드1의 파드 A는 종료된 이후 레플리케이션 컨트롤러가 관리하지 않기 때문에 다시 생성되지 않는다. 레플리케이션컨트롤러는 파드B가 사라진것을 인지하고 새로운 파드 인스턴스를 생성한다.4.2.1 레플리케이션컨트롤러의 동작 실행중인 파드 목록을 지속적으로 모니터링하고, 특정 유형의 실제 파드 수가 의도하는 수와 일치하는지 항상 확인한다.의도하는 수의 복제본수가 변경되는 케이스 파드 유형이란 특정 레비을 셀렉터와 일치하는 파드 세트(실제로 유형이라는 개념은 특별히 없다.) 누군가 같은 유형의 파드를 수동으로 만드는 경우 누군가 기존 파드의 유형을 변경하는 경우 누군가 의도하는 파드 수를 줄이는 경우 컨트롤러 조정 루프 레플리케이션컨트롤러의 역할은 정확한 수의 파드가 항상 레이블 셀렉터와 일치하는지 확인하는 것이다.레플리케이션컨트롤러의 3가지 요소 레이블 셀렉터는 범위에 있는 파드를 결정 레플리카수는 실행할 파드의 의도하는 수를 지정 파드 템플릿은 새로운 파드 레플리카 만들때 사용(파드 정의)컨트롤러의 레이블 셀렉터 또는 파드 템플릿 변경의 영향 이해 레이블 셀렉터와 파드 템플릿을 변경하더라도 기존 파드에는 영향을 미치지 않음. 레플리케이션컨트롤러는 파드를 생성한 후에는 실제 컨텐츠(컨테이너 이미지, 환경변수 등)에 신경 쓰지 않음. 그래서 변경 이후에 새 파드가 생성되는 시점에서만 영향을 미친다.레플리케이션컨트롤러 사용시 이점 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 보장할 수 있다. 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행중인 모든 파드에 관한 교체 복제본이 생성된다. 수동 또는 자동으로 파드를 쉽게 수평 확장할수 있다.4.2.2 레플리케이션컨트롤러 생성apiVersion: v1kind: ReplicationControllermetadata: name: kubia # 레플리케이션컨트롤러 이름spec: replicas: 3 # 의도한 파드 인스턴스 수 selector: # 관리하는 파드 셀렉터 app: kubia template: # 파드 템플릿 metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubia ports: - containerPort: 8080 레플리케이션 spec.selector를 지정하지 않을 수도 있다.(Optional) 셀렉터를 지정하지 않으면 템플릿의 레이블로 자동 설정된다. 레플리케이션컨트롤러를 정의할 때 셀렉터를 지정하지 않는것이 좋다.(쿠버네티스가 자동으로 추출하도록 하는게 간결하고 더 단순하다)레플리케이션 컨트롤러 생성kubectl create -f kubia-rc.yaml4.2.3 레플리케이션 컨트롤러 동작 확인kubectl get pods삭제된 파드에 관한 레플리케이션 컨트롤러의 반응# 삭제kubectl delete pod kubia-2lqjr# 삭제된 팟 확인kubectl get pods# 레플리케이션 정보 확인kubectl get rc# 레플리케이션컨트롤러 추가 정보 확인kubectl describe rc kubia컨트롤러가 새로운 파드를 생성한 원인 정확히 이해하기 레플레키에션컨트롤러가 삭제 액션에 반응한 것이 아니다. 결과적인 상태(부족한 파드수)에 대응한 것을 알아야 한다. 파드 삭제가 일어난 이후 컨트롤러의 실제 파드 수 확인하였고, 이에 대한 적절한 조치로 새로운 파드가 생성된 것노드 장애 대응 쿠버네티스를 사용하지 않는 환경에서 노드에 장애가 발생하면 앱을 수동으로 다른 시스템에 마이그레이션해야 함.(매우 오랜 시간이 걸리고 큰 문제) 레플리케이션컨트롤러는 노드의 다운을 감지하자마자 파드를 대체하기 위해 새 파드를 기동시킨다.# 노드의 eth0 을 다운시킨 후 노드 확인# 이 경우 해당 노드는 NotReady 상태로 변경된다.kubectl get node# 파드 확인# NotReady 상태의 노드에 있던 Pod는 Unknown 상태로 변경되고 삭제된다.(새로운 파드가 다른 노드에서 생성됨)kubectl get pods4.2.4 레플리케이션컨트롤러 범위 안팎으로 파드 이동하기 레플리케이션컨트롤러는 레이블 셀렉터와 일치하는 파드만을 관리한다. 파드의 레이블을 변경하면 범위에서 제거되거나 추가시킬 수 있다. 파드는 metadata.ownerReferences 필드에서 레플리케이션컨트롤러 참조 정보를 확인할 수 있다. 파드의 레이블을 변경하여 범위에서 제거하면 수동으로 만든 다른 파드처럼 변경된다.레플리케이션 컨트롤러가 관리하는 파드에 레이블 추가 관리하지 않는 레이블이라면 아무런 영향이 없다.관리되는 파드의 레이블 변경 레플리케이션컨트롤러에서 관리하지 않는 레이블로 변경하게 되면 범위에서 제거된것으로 간주하고 레플리케이션컨트롤러는 새로운 파드를 생성한다.컨트롤러에서 파드를 제거하는 실제 사례 특정 파드에서만 어떤 작업을 하려는 경우 레이블을 변경하여 범위에서 제거하면 작업이 수월해질 수 있다. 오동작하는 파드가 하나 있을때 이를 범위 밖으로 빼내고(이때 레플리케이션컨트롤러에 의해 레플리카수는 유지될것이다.) 원하는 방식으로 파드를 디버그하거나 문제를 재연해볼 수 있다.레플리케이션컨트롤러의 레이블 셀렉터 변경 컨트롤러의 레이블을 변경하면 모든 파드들이 범위를 벗어나게 되므로 새로운 N개의 파드를 생성하게 된다.4.2.5 파드 템플릿 변경 템플릿을 변경하는것은 쿠키 커터를 다른것으로 교체하는 것과 같다. 나중에 잘라낼 쿠키에만 영향을 줄뿐 이미 잘라낸 쿠키에는 아무런 영향을 미치지 않는다.템플릿을 수정하는 방법kubectl edit rc kubia 기본 텍스트 편집기에서 레플리케이션컨트롤러의 YMAL 정의가 열려서 이를 수정할 수 있다. KUBE_EDITOR 환경변수를 설정해서 텍스트 편집기를 커스텀할 수 있다.4.2.6 수평 파드 스케일링레플리케이션컨트롤러 스케일 업(확장) / 스케일 다운(축소)# 확대kubectl scale rc kubia --replicas=10# 조회kubectl get rc# 축소kubectl scale rc kubia --replicas=3 위와 같이 수정을 하게 되면 레플리케이션컨트롤러가 업데이트되고 즉시 파드 수가 10개로 확장되었다가 다시 3개로 축소된다.스케일링에 대한 선언적 접근 방법 이해 쿠버네티스에게 무엇을 어떻게 하라고 말하는게 아니라 의도하는 상태로 변경하는 것뿐. 쿠버네티스는 상태를 보고 상태에 맞게 조정한다.4.2.7 레플리케이션컨트롤러 삭제 kubectl delete를 통해 컨트롤러를 삭제하면 파드도 같이 삭제된다.# 파드와 컨트롤러 모두 삭제kubectl delete rc kubia# 파드를 삭제하지 않고 레플리케이션 컨트롤러를 삭제하는 방법kubectl delete rc kubia --cascade=false4.3 레플리카셋 레플리카셋은 레플리케이션컨트롤러와 유사한 리소스이고 레플리케이션컨트롤러를 대체하기 위한 나온 리소스이다. 일반적으로는 레플리카셋을 직접 생성하지는 않고 상위 수준의 디플로이먼트 리소스를 생성하면 자동으로 생성된다.4.3.1 레플리카셋과 레플리케이션컨트롤러 비교 레플리카셋이 좀 더 풍부한 파드 셀렉터 표현식을 사용할 수 있다. 특정 레이블이 없는 파드나 레이블의 값과 상관없이 특정 레이블의 키를 갖는 파드를 매칭시킬 수도 있다. 또한 하나의 레플리카셋으로 두 파드 세트를 모두 매칭시켜 하나의 그룹으로 취급하는것도 가능하다. 위 부분을 제외하고는 다르지 않음.4.3.2 레플리카셋 정의하기apiVersion: v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector:# matchLabels:# app: kubia matchExpressions: - key: app operator: In values: - kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubiaAPI 버전 속성 API 그룹, API 버전으로 구분되며 “apps/v1bet2”인 경우 그룹은 apps, 버전은 v1beta2이다. core API 그룹에 속할 경우에는 그룹을 명시하지 않아도 된다. (v1)4.3.3. 레플리카셋 생성 및 검사 레플리카셋 생성은 api v1으로만 해야한다.(버전업되면서 변경됨)# 레플리카셋 생성kubectl create -f kubia-replicaset.yaml# 레플리카셋 조회kubectl get rs4.3.4 레플리카셋의 더욱 표현적인 레이블 셀렉터 In은 레이블의 값이 지정된 값 중 하나와 일치해야 함. NotIn은 레이블의 값이 지정된 값과 일치하지 않아야 함. Exists는 파드의 지정된 키를 가진 레이블이 포함되어야 한다.(값은 관계없기에 value 필드를 지정하지 않는다.) DoesNotExists는 파드에 지정된 키를 가진 레이블이 포함돼 있지 않아야 한다.(value 지정 X)4.3.5 레플리카셋 삭제kubectl delete rs kubia4.4. 데몬셋 데몬셋을 사용하면 각 노드에서 정확히 한 개의 파드만 실행시킬 수 있다. 레플리카셋은 노드는 관계없이 지정된 수만큼 파드를 실행하는데 데몬셋은 이런 수를 지정하는것이 없고 클러스터의 모든 노드에 노드당 하나의 파드만 실행시키는 리소스이다. 시스템 수준의 작업을 수행하는 인프라 관련 파드가 있다고 하면 데몬셋이 가장 적합할것이다. 다른 예는 kube-proxy 프로세스가 데몬셋의 예이다. 서비스를 작동시키기 위해 모든 노드에서 실행되어야 하기 때문이다.4.4.1 데몬셋으로 모든 노드에 파드 실행하기 모든 클러스 노드마다 파드를 하나만 실행하고자 할때 사용하면 되는 리소스이다. 만약 하나의 노드가 다운되더라도 다른곳에서 파드를 생성하지 않고, 새로운 노드가 클러스터에 추가되면 즉시 새 파드 인스턴스를 해당 노드에 배포한다.4.4.2 데몬셋을 사용해 특정 노드에서만 파드 실행하기 파드가 노드의 일부에서만 실행되도록 지정하지 않으면 데몬셋은 클러스터 모든 노드에 파드를 배포한다. 하지만 파드 템플릿에서 node-Selector 속성을 지정하면 특정 노드에만 배포할 수 있다.데몬셋과 파드 스케줄링 쿠버네티스를 이용하면 노드에 스케줄링 되지 않게 해서 파드가 노드에 배포되지 않도록 할수도 있는데 이는 스케줄링 기반으로 동작하는 처리방식이다. 데몬셋이 관리하는 파드의 경우는 스케줄러와는 무관하기 때문에 스케줄링이 되지 않는 노드에서도 파드가 실행된다.데몬셋 적용 예제 노드에 레레이블을 지정하여 노드2를 제외한곳에만 데몬셋 파드가 실행되도록 처리한 예제이다.데몬셋 YAML 정의 apiVerison이 쿠버네티스 업데이트 따라 변경되어서 “apps/v1” 을 이용해야 함.apiVersion: apps/v1kind: DaemonSetmetadata: name: ssd-monitorspec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: luksa/ssd-monitor데몬셋 생성# 데몬셋 생성kubectl create -f ssd-monitor-daemonset.yaml# 데몬셋 조회kubectl get ds노드 레이블 추가 및 삭제# ssd 레이블 노드에 추가kubectl label node minikube disk=ssd# 데몬셋 pod 조회(추가됨)kubectl get po# ssd 레이블 노드를 hdd로 변경kubectl label node minikube disk=hdd --overwrite# 데몬셋 pod 조회(제거됨)kubectl get po4.5 Job 리소스 완료 가능한 단일 태스크를 수행하는 파드를 실행하기 위한 리소스로 Job이 있다. 완료 가능한 단일 태스크에서는 프로세스가 종료된 후에 다시 시작되지 않는다.4.5.1 Job리소스 특징 다른 리소스와 유사하지만 잡은 파드의 컨테이너 내부에서 실행중인 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 파드를 실행시킬 수 있다. 작업이 재대로 완료되는 것이 중요한 임시 작업에 유용하다. 이러한 잡 리소스에 정의하기에 좋은 예로는 데이터를 어딘가에 저장하고 있고, 이 데이터를 변환해서 어딘가로 전송하는 케이스를 들수 있다. 잡에서 과관리하느 파드는 성공적으로 끝날 때까지 다시 스케줄링이 된다.4.5.2 잡 리소스 정의apiVersion: batch/v1 # batchAPI그룹의 버전을 선택해야 한다.kind: Jobmetadata: name: batch-jobspec: template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure # 재시작 정책을 사용할 수 있음.(Always, OnFailure, Naver) containers: - name: main image: luksa/batch-job4.5.3 파드를 실행한 잡 확인# Job 생성kubectl create -f batch-job.yaml# Job 확인kubectl get jods# Job pod 확인 (completed된 job도 표시됨)kubectl get po# 로그 확인kubectl logs batch-job-9dhsc# job 삭제(job을 삭제하면 pod도 삭제된다)kubectl delete job batch-job완료된 파드를 삭제하지 않는 이유 파드가 완료될 때 파드가 삭제되지 않는 이유는 해당 파드의 로그를 검사할 수 있도록 하기 위함이다.4.5.4 잡에서 여러 파드 인스턴스 실행하기 2개 이상의 파드 인스턴스를 생성해 병렬 또는 순차 처리를 구성할 수 있다.(completions, parallelism 속성 이용)순차적으로 잡 파드 실행하기 spec에 completions값 지정 이렇게 처리하면 5개의 파드가 성공적으로 완료될 때까지 과정을 계속한다. 중간에 실패하는 파드가 있다면 잡은 새 파드를 생성하여 실제 5개 이상의 파드가 생성될 수 있다.apiVersion: batch/v1kind: Jobmetadata: name: multi-completion-batch-jobspec: completions: 5 template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job병렬로 잡 파드 실행하기 parallelism을 2로 설정하면 동시에 2개의 파드가 생성되어 병렬처리로 실행된다.apiVersion: batch/v1kind: Jobmetadata: name: multi-completion-batch-jobspec: completions: 5 parallelism: 2 template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job잡 스케일링 잡이 실행되는 동안 parallelism 속성을 변경하면 동시에 처리되는 파드 수를 조절할 수 있다. kubectl scale job multi-completion-batch-job --replicas 3 4.5.5 잡 파드가 완료되는데 걸리는 시간 제한하기 및 재시도 횟수 설정 activeDeadlineSeconds 속성을 설정하면 파드의 실행 시간에 제한을 두고 시간을 넘어서면 잡을 실패한것으로 처리할수도 있다. backoffLimit 필드를 지정해 실패한것으로 표시되기 전에 잡을 재시도할수 있는 횟수도 설정할 수 있다.(기본값 6)4.6 크론잡(CronJon) 잡을 주기적으로 또는 한번만 실행되도록 스케줄링하기 많은 배치 잡이 미래의 특정 시간 또는 지정된 간격으로 반복 실행해야 한다. 쿠버네티스를 이를 지원하기 위한 크론잡 리소스 기능을 제공한다.4.6.1 크론잡 정의apiVersion: batch/v1beta1kind: CronJobmetadata: name: batch-job-every-fifteen-minutesspec: schedule: &quot;0,15,30,45 * * * *&quot; # 매일 매시간 0,15,30,45분에 실행되는 cronJob jobTemplate: spec: template: metadata: labels: app: periodic-batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job크론잡 생성 및 확인# 크론잡 생성kubectl create -f cronjob.yaml# 크론잡 확인kubectl get cronjob# 크론잡 삭제kubectl delete cronjob batch-job-every-fifteen-minutes4.6.2 스케줄된 잡의 실행 방법 이해 예정된 시간을 너무 초과해서 시작되서는 안되는 엄격한 요구사항이 요구될떄도 있는데 이를 위한 옵션을 제공한다. startingDeadlineSeconds 필드를 지정( 초단위) 일반적인 상황에서 크론잡은 스케줄에 설정한 각 실행에 항상 하나의 잡만 생성하지만, 2개의 잡이 동시에 생성되거나 전혀 생성되지 않을수도 있다. 이런 문제를 해결하기 위해 멱등성(한번 실행이 아니라 여러번 실행해도 동일한 결과가 나타나야함)가지도록 개발해야 한다. 이전에 누락된 잡 실행이 있다면 다음번 작업에서 해당 작업을 같이 수행해주도록 개발하는것이 좋다. 4.7 요약 컨테이너가 더 이상 정상적이지 않으면 즉시 쿠버네티스가 컨테이너를 다시 시작하도록 라이브니스 프로브를 지정할 수 있다. 직접 생성한 파드는 실수로 삭제되거나 실행중인 노드에 장애가 발생하거나 노드에서 퇴출되면 다시 생성되지 않기 때문에 직접 생성해서 사용하면 안된다. 레플리케이션컨트롤러는 의도하는 수의 파드 복제본을 항상 실행 상태로 유지해준다. 파드를 수평으로 스케일링(확장)하려면 쉽게 레플리케이션컨트롤러에 의도하는 레플리카 수를 변경하는 것만으로도 가능하다. 파드는 레플리케이션컨트롤러가 소유하지 않으며, 필요한 경우 레플리케이션컨트롤러간에 이동할 수 있다. 템플릿을 변경해도 기존의 파드에는 영향을 미치지 않는다. 레플리카셋과 디폴로이먼트로 교체해야 하며, 레플리카셋과 디폴로이먼트는 동일한 기능을 제공하면서 추가적인 강력한 기능을 제공한다. 레플리카셋은 임의의 클러스턴 ㅗ드에 파드를 스케줄링하는 반면, 데몬셋은 모든 노드에 데몬셋이 정의한 파드의 인스턴스 하나만 실행되도록 한다. 배치 작업을 수행하는 파드는 쿠버네티스의 잡 리소스로 생성해야 한다. 특정 시점에 주기적으로 수행해야 하는 잡은 크론잡 리소스를 통해 생성할 수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 3. 파드 : 쿠버네티스에서 컨테이너 실행", "url": "/posts/devlog-platform-kubernetes-in-action3/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-03 17:34:00 +0900", "snippet": "3. 파드 : 쿠버네티스에서 컨테이너 실행3.1 파드 소개 파드는 함께 배치된 컨테이너 그룹이며, 쿠버네티스의 기본 빌딩 블록. 컨테이너를 개별적으로 배포하기보다는 컨테이너를 가진 파드를 배포하고, 운영한다. 무조건 2개 이상을 컨테이너를 포함시키라는 의미는 아니고 일반적으로는 하나의 컨테이너만 포함된다. 파드의 핵심은 파드가 여러 컨테이너를 가지고 있을 경우 모든 컨테이너는 항상 하나의 워커 노드에서 실행되고, 여러 워커 노드에 걸쳐 실행되지 않는다는것.3.1.1 파드가 필요한 이유 여러 프로세스를 실행하는 단일 컨테이너보다 다중 컨테이너가 좋기 때문 쿠버네티스에서는 프로세스를 항상 컨테이너에서 실행시키고, 각각의 컨테이너는 격리된 머신과 비슷하다. 그래서 여러 프로세스를 단일 컨테이너 안에서 실행하는것이 타당한것처럼 보일수 있는데. 컨테이너는 단일 프로세스를 실행하는 것을 목적으로 설계되었음. 그래서 각 프로세스를 자체의 개별 컨테이너로 실행해야 한다. 하지만 실제 서비스 상황에서는 여러개의 컨테이너를 포함시킬 수 있는 개념이 필요할 수 있음. 예를 들면 WAS를 쿠버네티스로 서빙한다고 했을떄 쿠버네티스 스케줄러에 의해서 pod이 존재하는 노드는 언제든 수시로 변경될 수 있는데 이 경우 로그가 유실될 수 있기 때문에 이런 로그를 유실되지 않기 위해서는 로그를 중앙저장소로 보내주는 별도의 프로세스가 있어야 함. 이 경우 파드 구성을 WAS 컨테이너 + 로그전송 프로세스 컨테이너로 하여 하기 위함이 아닌가 싶음. 3.1.2 파드 이해하기 여러 프로세스를 단일 컨테이너로 묶지 않기 때문에 컨테이너를 함께 묶고 하나의 단위로 관리할 수 있는 또 다른 상위 구조가 필요하다. (파드가 필요한 이유) 컨테이너가 제공하는 모든 기능을 활용하는 동시에 프로세스가 함께 실행되는것처럼 보이게 할수 있음.같은 파드에서 컨테이너 간 부분 격리 쿠버네티스는 파드 안에 있는 모든 컨테이너가 자체 네임스페이스가 아닌 동일한 리눅스 네임스페이스를 공유하도록 도커를 설정 즉 동일한 네트워크 네임스페이스, UTS(Unix Timesharing System, 호스트 이름을 네임스페이스로 격리) 안에서 실행되므로 같은 파드 내에서 이것들을 서로 공유할 수 있다. 여기서 주의할 점은 파일시스템같은 경우는 컨테이너 이미지에서 나오기 때문에 기본적으로 완전히 분리된다고 보면 된다. 쿠버네티스 볼륨 개념을 이용하면 컨테이너간 파일 디렉터리를 공유하는것도 가능컨테이너가 동일한 IP와 포트 공간을 공유하는 방법 파드 안의 컨테이너가 동일한 네트워크 네임스페이스에서 실행되기 때문에 도일한 IP주소와 포트공간을 공유한다. 그래서 동일한 파드 안 컨테이너에서 실행되는 프로세스들은 같은 포트번호를 사용하지 않도록 해야 한다. 파드 안의 컨테이너들끼리는 로컬호스트를 통해 서로 통신이 가능(동일한 루프백 네트워크 인터페이스를 갖음)파드간 플랫 네트워크 소개 모든 파드는 하나의 플랫한 공유 네트워크 주소 공간에 상주 모든 파드는 다른 파드의 IP주소를 사용해 접근이 가능하다. LAN과 유사한 방식으로 상호 통신이 가능 파드는 논리적인 호스트로서 컨테이너가 아닌 환경에서의 물리적 호스트 혹은 VM과 매우 유사하게 동작함.3.1.3 파드에서 컨테이너의 적절한 구성다계층 애플리케이션을 여러 파드로 분할 프론트엔드와 백엔드 예제에서 파드를 2개로 분리하면 쿠버네티스가 프론트엔드를 한 노드로 백엔드는 다른 노드에 스케줄링해서 인프라스트러겨의 활용도를 향상시킬수 있음. 이게 만약 같은 파드에 있다면 둘중 항상 같은 노드에서 실행되겠지만, 서로 필요한 리소스의 양은 다를수 있기에 분리하는것이 좋다.개별 확장이 가능하도록 여러 파드로 분할 두 컨테이너를 하나의 파드에 넣지 말아야하는 이유 중 하나는 “스케일링” 때문 쿠버네티스 스케줄링 기본 단위는 파드이다. 쿠버네티스에서는 컨테이너 단위로 수평 확장할수 없고 파드 단위로만 수평확장(scale out)이 가능하다. 컨테이너를 개별적으로 스케일링하는 것이 필요하다고 판단되는 경우 별도 파드에 배포하자.파드에서 여러 컨테이너를 사용하는 경우 애플리케이션이 하나의 주요 프로세스와 하나 이상의 보안 프로세스로 구성된 경우 특정 디렉터리에서 파일을 제공하는 웹서버로 예를 들자면 주 컨테이너는 웹서버라고 하고, 추가 컨테이너(사이드카 컨테이너)는 외부 소스에서 주기적으로 콘텐츠를 받아 웹서버 디렉터리에 저장하는 방식을 예로 들수 있다.(ex. sitemap을 받아와서 디렉터리에서 저장하는 것과 같은) 다른 에제로는 로그 로테이터와 수집기, 데이터 프로세서, 통신 어댑터 등이 있을수 있다.파드에서 여러 컨테이너를 사용하는 경우 결정 컨테이너를 함께 실행해야 하는가? 혹은 서로 다른 호스트에서 실행할 수 있는가? 여러 컨테이너가 모여 하나의 구성 요소로 나타내는가? 혹은 개별적인 구성 요소인가? 컨테이너가 함꼐, 혹은 개별적으로 스케일링돼야 하는가? 컨테이너는 여러 프로세스를 실행시키지 말아야하고, 파드는 동일한 머신에서 실행할 필요가 없다면 여러 컨테이너를 포함하지 말아야 한다.3.2 YAML 또는 JSON 디스크립터로 파드 생성 kubectl run 명령어를 이용해서 리소스를 만들수도 있지만, yaml 파일에 모든 쿠버네티스 오브젝트를 정의하면 버전 관리 시스템에 넣는것도 가능하고, 모든 이점을 누릴수 있다.3.2.1 기존 파드의 YAML 디스크립터 살펴보기# 메타 데이터apiVersion: v1kind: Podmetadata: creationTimestamp: &quot;2020-08-03T14:21:44Z&quot; generateName: kubia- labels: run: kubia name: kubia-5wsx8 namespace: default ownerReferences: - apiVersion: v1 blockOwnerDeletion: true controller: true kind: ReplicationController name: kubia uid: cb311984-2974-4434-8979-b7a11f07388d resourceVersion: &quot;4551&quot; selfLink: /api/v1/namespaces/default/pods/kubia-5wsx8 uid: ed4f042e-f547-4003-9a89-8005bca02f60# 스펙spec: containers: - image: sungsu9022/kubia imagePullPolicy: Always name: kubia ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-zszt7 readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: docker-desktop priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-zszt7 secret: defaultMode: 420 secretName: default-token-zszt7status: conditions: - lastProbeTime: null lastTransitionTime: &quot;2020-08-03T14:21:44Z&quot; status: &quot;True&quot; type: Initialized - lastProbeTime: null lastTransitionTime: &quot;2020-08-03T14:21:49Z&quot; status: &quot;True&quot; type: Ready - lastProbeTime: null lastTransitionTime: &quot;2020-08-03T14:21:49Z&quot; status: &quot;True&quot; type: ContainersReady - lastProbeTime: null lastTransitionTime: &quot;2020-08-03T14:21:44Z&quot; status: &quot;True&quot; type: PodScheduled# Status containerStatuses: - containerID: docker://796528475e929b9f1ec28fc5ae3ca722f1279b8f852b262dfef549bbdebdcd7a image: sungsu9022/kubia:latest imageID: docker-pullable://sungsu9022/kubia@sha256:3811562a9c1c45f2e03352bf99cacf4e024f9c3a874a3d862f0208367eace763 lastState: {} name: kubia ready: true restartCount: 0 started: true state: running: startedAt: &quot;2020-08-03T14:21:48Z&quot; hostIP: 192.168.65.3 phase: Running podIP: 10.1.0.13 podIPs: - ip: 10.1.0.13 qosClass: BestEffort startTime: &quot;2020-08-03T14:21:44Z&quot;파드를 정의하는 주요 부분 소개 Metadata : 이름, 네임스페이스, 레이블 및 파드에 관한 기타 정보를 포함 Spec : 파드 컨테이너, 볼륨, 기타 데이터 등 파드 자체에 관한 실제 명세를 가진다. Status : 파드 상태, 각 컨테이너 설명과 상태, 파드 내부 IP, 기타 기본 정보 등 현재 실행중인 파드에 관한 정보 포함(새 파드를 만들때는 작성하지 않는 내용)3.2.2 파드를 정의하는 간단한 YAML 정의apiVersion: v1kind: Podmetadata: name: kubia-manual # 파드 이름spec: containers: - image: luksa/kubia # 컨테이너 이미 name: kubia # 컨테이너 이름 ports: - containerPort: 8080 # 애플리케이션 수신 포트 protocol: TCP컨테이너 포트 지정 스펙에 포트를 명시하지않아도 해당 파트에 접속할 수 있으나, 명시적으로 정의해주면 클러스터를 사용하는 모든 사람이 파드에 노출한 포트를 빠르게 볼 수 있어서 정의하는 것이 좋음.오브젝트 필드들에 대한 설명 확인(도움말같은)kubectl explain podskubectl explain pod.spec3.2.3 kubectl create 명령으로 파드 만들기 yaml 또는 json 파일로 리소스를 동일하게 만들 수 있음.kubectl create -f kubia-manual.yaml실행중인 파드의 전체 정의 가져오기kubectl get po kubia-manual -o yamlkubectl get po kubia-manual -o json3.2.4 애플리케이션 로그 보기 컨테이너 런타임(도커)은 스트림을 파일로 전달하고 아래 명령을 이용해 컨테이너 로그를 가져온다.docker log &amp;lt;container id&amp;gt; 쿠버네티스 logs를 이용해 파드 로그 가져오기 kubectl logs kubia-manual 파드에 컨테이너가 하나만 있다면 로그를 가져오는 것은 매우 간단함 컨테이너 로그는 하루 단위로, 로그 파일이 10MD 크기에 도달할때마다 로테이션 되기 떄문에 별도로 로그 관리하는 방식이 필요할듯컨테이너 이름을 지정해 다중 컨테이너 파드에서 로그 가져오기kubectl logs kubia-manual -c kubia 현재 존재하는 파드의 컨테이너 로그만 가져올수 있음. 파드가 삭제되면 로그도 같이 삭제된다. 일반적인 서비스 환경에서는 클러스터 전체의 중앙집중식 로깅을 설정하는 것이 좋음3.2.5 파드에 요청 보내기 기존에 kubectl expose 명령으로 외부에서 파드에 접속할 수 있는 서비스를 만들었었는데 “포트 포워딩” 방식을 이용해 테스트와 디버깅 목적으로 연결할수 있음.로컬 네트워크 포트를 파드의 포트로 포워딩 서비스를 거치지 않고 특정 파드와 통신하고 싶을때 쿠버네티스는 해당 파드로 향하나느 포트 포워딩을 구성해준다. 아래와 같인 하면 localhost:8888로 해당 파드의 8080포트와 연결시킬 수 있다.kubectl port-forward kubia-manual 8888:80803.3 레이블을 이용한 파드 구성 파드 수가 증가함에 따라 파드를 부분집합으로 분류할 필요가 있음. MSA에서 배포된 서비스가 매우 많고 여기서 여러 버전 혹은 릴리스(안정, 베타 카나리 등)이 동시에 실행될수도 있는데 이러면 시스템에 수백개의 파드가 생길수 있고 이를 정리할 매커니즘이 필요한데 이것이 레이블이다. 레이블을 통해 파드와 기타 다른 쿰버네티스 오브젝트의 조직화를 할 수 있음(파드 뿐만 아니라 다른 쿠버네티스 리소스 가능)3.3.1 레이블 소개 파드와 모든 쿠버네티스 리소스를 조직화할 수 있는 단순하면서 강력한 기능 리소스에 첨부하는 키-값 쌍 레이블 셀렉터를 사용해 리소스를 선택할때 호라용 레이블 키가 해당 리소스 내에서 고유하다면 하나 이상 원하는 만큼의 레이블을 가질 수 있다.3.3.2 파드를 생성할 때 레이블 지정 kubia-manual-with-labels.yamlapiVersion: v1kind: Podmetadata: name: kubia-manual-v2 labels: # 레이블 2개 지정 creation_method: manual env: prodspec: containers: - image: luksa/kubia name: kubia ports: - containerPort: 8080 protocol: TCP# Pod 생성kubectl create -f kubia-manual-with-labels.yaml# pod label 정보 조회kubectl get po --show-labels# 특정 레이블에만 관심 있는 경우 -L 스위치를 지정해 각 레이블을 자체 열에 표시할 수 있음.kubectl get po -L creation_method,env3.3.3 기존 파드 레이블 수정# 기존 pods 레이블 추가kubectl label po kubia-manual creation_method=manual# pods 레이블 수정kubectl label po kubia-manual-v2 env=debug --overwrite# 레이블 조회kubectl get po -L creation_method,env3.4 레이블 셀렉터를 이용한 파드 부분 집합 나열 레이블이 중요한 이유는 레이블 셀렉터와 함꼐 사용된다는 것이다. 레이블 셀렉터는 리소스 중에서 다음 기준에 따라 리소르를 선택한다. 특정한 키를 포함하거나 포함하지 않는 레이블 특정한 키와 값을 가진 레이블 특정한 키를 갖고 있지만, 다른 값을 가진 레이블 3.4.1 레이블 셀렉터를 사용해 파드 나열# creation_method 레이블 manual을 가지고 있는 파드 나열kubectl get po -l creation_method=manual# env를 가지고 있는 파드 나열kubectl get po -l env# env를 가지고 있지 않은 파드 나열kubectl get po -l &#39;!env&#39;# creation_method 레이블을 가진 것중에 값이 manual이 아닌것kubectl get po -l &#39;creation_method!=manual&#39;kubectl get po -l &#39;env in (prod,devel)&#39;kubectl get po -l &#39;env notin (prod,devel)&#39;3.4.2 레이블 셀렉터에서 여러 조건 사용# 셀렉터는 쉼표로 구분된 여러 기준을 포함하는 것도 가능하다.kubectl get po -l creation_method=manual,env=debug 레이블 셀렉터를 이용해 여러 파드를 한번에 삭제할수도 있음.3.5 레이블과 셀렉터를 이용해 파드 스케줄링 제한 파드 스케줄링할 노드를 결정할 떄 영향을 미치고 싶은 상황이 있을수 있음. SSD를 가지고 있는 워커 노드에 할당해야 하는 경우 이때 특정노드를 지정하는 방법은 좋은것은 아님(아마도 노드이름이나 별도의 정보를 기준으로 하는것을 말하는듯) 노드를 지정하는 대신 필요한 노드 요구사항을 기술하고 쿠버네티스가 요구사항을 만족하는 노드를 직접 선택하도록 해야 하는데 이때 노드 레이블과 레이블 셀렉터를 통해서 할 수 있다. SSD 노드가 2개가 있을떄 A노드로 지정을 해버리면 혹시 A노드에 문제가 생기면 스케줄링을 재대로 못할수 있다는 의미(만약 저런 지정이 없었다면 B로 스케줄링 되었을것) 3.5.1 워커 노드 분류에 레이블 사용 노드를 포함한 모든 쿠버네티스 오브젝트에 레이블을 부착할수 있기 때문에 노드 분류에 레이블을 사용하고 쿠버네티스가 스케줄링할때 활용할 수 있다.# 이런식으로 노드에 label을 붙인다.(로컬에서 docker-desktop밖에 없어서 직접해보지는 ㅇ낳음)kubectl label node gke-kubia-85f6-node-0rrx gpu=true# 노드 조회kubectl get nodes -l gpu=true3.5.2 특정 노드에 파드 스케줄링apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: nodeSelector: gpu: &quot;true&quot; # gpu=true 레이블을 포함한 노드에 이 파드라 배포하도록 지시 containers: - image: luksa/kubia name: kubia3.5.3 하나의 특정 노드 스케줄링 nodeSelector에 실제 호스트 이름을 지정할 경우 해당 노드가 오프라인 상태인 경우 파드가 스케줄링 되지 않을수 있어서 레이블 셀렉터를 통해 지정한 특정 기준을 만족하는 노드의 논리적인 그룹으로 처리될 수 있도록 해야 한다. 추가로 노드에 파드를 스케줄링할 때 영향을 줄수 있는 방법은 16장에서 또 나옴3.6 파드에 어노테이션 달기 어노테이션은 키-값 쌍으로 레이블과 거의 비슷하지만 식별 정보로 사용되지 않음. 반면 훨씬 더 많은 정보를 보유할 수 있다. 쿠버네티스 새로운 기능 릴리스시 어노테이션을 사용하곤 한다. 어노테이션을 유용하게 사용하는 방법은 파드나 다른 API 오브젝트에 설명을 추가하는 것을 예를 들수 있음. 이렇게 하면 클러스터를 사용하는 모든 사람이 개별 오브젝트에 관한 정보를 쉽게 찾을수 있음. 3.6.1 오브젝트의 어노테이션 조회 레이블에 넣을 데이터는 보통 짧은 데이터이고, 어노테이션에는 상대적으로 큰 데이터를 넣을 수 있음# kubectl describe {리소스} {name}kubectl get po kubia-xxxxx -o yarml # 1.9부터 이 정보는 yaml안에서 확인할수 없도록 변경됨3.6.2 어노테이션 추가 및 수정# 어노테이션 추가kubectl annotate pod kubia-manual naver.com/someannotation=&quot;naver foo bar&quot;# describe을 통해 pod의 어노테이션 정보 조회kubectl describe pod kubia-manual3.7 네임스페이스를 사용한 리소스 그룹화 각 오브젝트는 여러 레이블을 가질 수 있기 때문에 오브젝트 그룹은 서로 겹쳐질 수 있다. 클러스터에서 작업을 수행할 때 레이블 셀렉터를 명시적으로 지정하지 않으면 항상 모든 오브젝트를 보게 된다. 이런 문제를 해결하기 위해 쿠버네티스는 네임스페이스 단위로 오브젝트를 그룹화한다. docker에서 이야기했던 리눅스 네임스페이스와는 별개임3.7.1 네임스페이스의 필요성 네임스페이스를 사용하면 많은 구성 요소를 가진 복잡한 시스템을 좀 더 작은 개별 그룹으로 분리할 수 있음. 멀티테넌트환경처럼 리소르르 분리하는 데 사용된다. 리소스를 프로덕션 개발, QA환경 혹은 원하는 다른 방법으로 나누어 사용할 수 있다. 단 노드의 경우는 전역 리소스이고, 단일 네임스페이스에 얽매이지 않는다.(이러한 이유는 4장에서 다룸)3.7.2 다른 네임스페이스와 파드 살펴보기 명시적으로 지정하지 않았다면 default 네임스페이스를 사용했을 것``` shkubectl get nskubectl get po -n kube-system - 쿠베 시스템에 대한 네임스페이스가 분리되어 있어서 뭔가 유저가 만든 리소스들과 깔끔하게 분리하여 관리할수 있음. - 매우 큰 규모에서 쿠버네티스를 사용한다면 네임스페이스를 사용해서 서로 관계없는 리소스를 겹치지 않는 그룹으로 분리할 수 있다. - 네임스페이스 기준으로 리소스 이름에 관한 접근범위를 제공하므로 리소스 이름 충돌에 대한 걱정도 할 필요 없음. - 이 외에도 특정 사용자에 대한 리솟 ㅡ접근 권한을 관리할수도 있고, 개별 사용자가 사용할 수 있는 컴퓨팅 리소스를 제한하는 데에도 사용된다.### 3.7.3 네임스페이스 생성``` yamlapiVersion: v1kind: Namespacemetadata: name: custom-namespace# yaml 파일을 이용한 네임스페이스 생성kubectl create -f custom-namespace.yaml# 커맨드라인 명령어로 생성kubectl create namespace custom-namespace3.7.4 다른 네임스페이스의 오브젝트 관리 생성한 네임스페이스 안에 리소스를 만들기 위해서는 metadata 섹션에 namespace항목을 넣거나 kubectl create 명령시 네임스페이스를 지정하면 된다.# custom-namespace에 pod 생성kubectl create -f kubia-manual.yaml -n custom-namespace 네임스페이스 context 이동# custom-namespace 네임스페이스로 컨텍스트 변경kubectl config set-context --current --namespace=custom-namespace# default 네임스페이스로 컨텍스트 변경kubectl config set-context --current --namespace=default3.7.5 네임스페이스가 제공하는 격리이해 실행중인 오브젝트에 대한 격리는 제공하지 않음. 네임스페이스에서 네트워크 격리를 제공하는지는 쿠버네티스와 함께 배포하는 네트워킹 솔루션에 따라 다름. 서로 다른 네임스페이스 안에 있는 파드끼리 서로 IP주소를 알고있다면 HTTP 요청과 같은 트래픽을 다른 파드로 보내는데에는 전혀 제약이 없다.3.8 파드 중지와 제거 파드를 삭제하면 쿠버네티스 파드 안에 있는 모든 컨테이너를 종료하도록 지시 이때 SIGTERM 신호를 프로세스에 보내고, 지정된 시간(기본 30초)동안 기다린다. 시간 내에 종료되지 않으면 SIGKILL 신호를 통해 종료시킨다. 프로세스가 항상 정상적으로 종료되게 하기 위해서는 SIGTERM 신호를 올바르게 처리해야 한다.3.8.1 이름으로 파드 제거# 이름으로 삭제kubectl delete po kubia-gpu# 공백을 구분자로 여러개를 한번에 지울수 있음.kubectl delete po kubia-gpu kubia-gpu23.8.2 레이블 셀렉터를 이용한 파드 삭제# 레이블 셀렉터를 이요한 삭제kubectl delete po -l creation_method=manualkubectl delete po -l rel=canary3.8.3 네임스페이스를 삭제한 파드 제거kubectl delete ns custom-namespace3.8.4 네임스페이슬 유지하면서 네임스페이스 안에 모든 파드 삭제 여기서 파드를 삭제하더라도 레플리케이션컨트롤러로 생긴 파드가 있었다면 삭제하더라도 다시 스케줄링 될것(이 경우 파드를 삭제하기 위해서는 리플리케이션컨트롤러도 같이 삭제해야 한다. kubectl delete po --all 3.8.5 네임스페이스에서 (거의) 모든 리소스 삭제 all 키워드를 쓰면 대부분의 리소스는 삭제할 수 있지만, 7장에 나올 시크릿 같은 특정 리소스는 그대로 보존되어 있다. 이 키워드를 통해 kubernetes 서비스도 삭제가 가능하지만 잠시 후에 다시 생성된다. kubectl delete all --all 3.9 요약 특정 컨테이너를 파드로 묶어야 하는지 여부를 결정하는 방법 파드는 여러 프로세스를 실행할 수 있으며 컨테이너가 아닌 세계의 물리적 호스트와 비슷하다. yaml 또는 json 디스크립터를 작성해 파드를 작성하고 파드 정의와 상태를 확인할 수 있다. 레이블과 레이블 셀렉터를 사용해 파드를 조직하고 한번에 여러 파드에서 작업을 쉽게 수행할 수 있다. 노드 레이블과 셀렉터를 사용해 특정 기능을 가진 노드에 파드를 스케줄링 할 수 있다. 어노테이션을 사용하면 사람 또는 도구, 라이브러리에서 더 큰 데이터를 파드에 부착할 수 있다. 네임스페이스는 다른 팀들이 동일한 클러스터를 별도 클러스터를 사용하는 것처럼 이용할 수 있게 해준다. kubectl explain 명령으로 쿠버네티스 리소스 도움말을 볼 수 있음.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 2. 도커와 쿠버네티스 첫걸음", "url": "/posts/devlog-platform-kubernetes-in-action2/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-02 17:34:00 +0900", "snippet": "2. 도커와 쿠버네티스 첫걸음2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기2.1.1 Hello World 컨테이너 실행하기docker run busybox echp &quot;Hello world&quot;백그라운드에 일어난 동작 이해하기 docker run 명령을 수행했을 떄 일어나는 일들2.1.2 간단한 node.js 애플리케이션 생성 예제 코드 있음.2.1.3 이미지를 위한 DockerFile 생성FROM node:7ADD app.js /app.jsENTRYPOINT [&quot;node&quot;, &quot;app.js&quot;]2.1.4 컨테이너 이미지 생성 도커 이미지 조회 docker images 도커 이미지 생성 docker buld -t kubia . 2.1.5 컨테이너 이미지 실행 컨테이너 이미지 실행docker run --name kubia-container -p 8080:8080 -d kubiacurl localhost:8080 실행중인 컨테이너 조회docker ps 컨테이너에 관한 추가 정보 얻기docker inspect kubia-contaniner[ { &quot;Id&quot;: &quot;d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2&quot;, &quot;Created&quot;: &quot;2020-08-03T14:04:38.901107275Z&quot;, &quot;Path&quot;: &quot;node&quot;, &quot;Args&quot;: [ &quot;app.js&quot; ], &quot;State&quot;: { &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 2660, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-08-03T14:04:39.219869415Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; }, &quot;Image&quot;: &quot;sha256:5b3381f90920e1c043628736040dca33fefeff5df30a14376e0d387e51439e0d&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2-json.log&quot;, &quot;Name&quot;: &quot;/kubia-contaniner&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: { &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: { &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: {} }, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: { &quot;8080/tcp&quot;: [ { &quot;HostIp&quot;: &quot;&quot;, &quot;HostPort&quot;: &quot;8080&quot; } ] }, &quot;RestartPolicy&quot;: { &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 }, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] }, &quot;GraphDriver&quot;: { &quot;Data&quot;: { &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74-init/diff:/var/lib/docker/overlay2/64e81c4fc48f121c861dbe5cc5cb5022a2ec2f7574e82e5f8948e04c87746b81/diff:/var/lib/docker/overlay2/5d2ddb4be75ae0be6e92a332006d173a1f20b92ec8688b10d6ac01bee176c542/diff:/var/lib/docker/overlay2/1a703255cdcdd1f16f5a3bd8a4b998be442fdb25c7a45108df017f1bb6c8960f/diff:/var/lib/docker/overlay2/f65dcb07c2b440dd4da92dc5536ee68a0952023878e59b335554a5d4ca9fdc8d/diff:/var/lib/docker/overlay2/57db638f03c99ab27a1426cc52ccc8911b103a5128a81b56fee29852dafe65bd/diff:/var/lib/docker/overlay2/af3ec0c3056b0a7ce999eff9a10084409f532b44e27418e736901749621ce194/diff:/var/lib/docker/overlay2/81250fe4e2e75f440c5aa17df795342744a5e70cd0e90ee4b54f1a20b36504aa/diff:/var/lib/docker/overlay2/5f0ac8d1e4a787d1229a165e4ea65375daa78d49a0ec9ec933ca3e5a2f7f1feb/diff:/var/lib/docker/overlay2/37442cc111e0254803eb97bc76e3e04ed907fa92aeacd37c10cf4b68c9c9ed94/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/work&quot; }, &quot;Name&quot;: &quot;overlay2&quot; }, &quot;Mounts&quot;: [], &quot;Config&quot;: { &quot;Hostname&quot;: &quot;d9e72c4f7867&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;ExposedPorts&quot;: { &quot;8080/tcp&quot;: {} }, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;, &quot;NPM_CONFIG_LOGLEVEL=info&quot;, &quot;NODE_VERSION=7.10.1&quot;, &quot;YARN_VERSION=0.24.4&quot; ], &quot;Cmd&quot;: null, &quot;Image&quot;: &quot;kubia&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: [ &quot;node&quot;, &quot;app.js&quot; ], &quot;OnBuild&quot;: null, &quot;Labels&quot;: {} }, &quot;NetworkSettings&quot;: { &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;b3896d08952f17bf1357487b848f07f3479552c4ccfce4c21a3a4e4f9c0d05b7&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: { &quot;8080/tcp&quot;: [ { &quot;HostIp&quot;: &quot;0.0.0.0&quot;, &quot;HostPort&quot;: &quot;8080&quot; } ] }, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/b3896d08952f&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;f07d0019848fb4f71c8be009ca864596dc3ca7a270264a07b0cbc2af9e7e430a&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;Networks&quot;: { &quot;bridge&quot;: { &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;316f7a8d294ea277f1380cd6eda453adc349411009ecaa7b9e622c1f0910df78&quot;, &quot;EndpointID&quot;: &quot;f07d0019848fb4f71c8be009ca864596dc3ca7a270264a07b0cbc2af9e7e430a&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null } } } }]2.1.6 실행중인 컨테이너 내부 탐색하기# 컨테이너 내부 쉘 실행docker exec -it kubia-contaniner bash2.1.7 컨테이너 중지와 삭제# 컨테이너 중지docker stop kubia-contaniner# 컨테이너 삭제docker rm kubia-contaniner2.1.8 이미지 레지스트리에 이미지 푸시# 추가 태그로 이미지 태그 지정docker tag kubia sungsu9022/kubia# 도커 허브에 이미지 푸시docker push sungsu9022/kubia# 다른머신에서 이미지 실행하기docker run -p 8080:8080 -d sungsu9022/kubia2.2 쿠버네티스 설치하기 공식문서에 minikube로 시작하는 가이드가 있음. 그게 가장 간편해보임. docker desktop이 설치되어있다면 거기 옵션으로 kubernetes를 시작할수도 있음.2.2.3 쿠버네티스 자동완성 설정하기 …2.3 쿠버네티스 앱 실행# 파드 실행kubectl run kubia --image=sungsu9022/kubia --port=8080 --generator=run/v1# 파드 조회kubectl get pods# 서비스 오브젝트 생성kubectl expose rc kubia --type=LoadBalancer --name kubia-http# 서비스 조회kubectl get serviceskubectl get svc minikube에서는 서비스 지원하지 않아 외부 포트를 통해 서비스 접근해야 함.# replicationcontrollers 정보 조회kubectl get replicationcontrollers# replicationcontrollers 수늘리기kubectl scale rc kubia --replicas=3# 실행중인 노드까지 표시kubectl get pods -p wide# 파드 세부 정보 살펴보기kubectl describe pod kubia-5wsx8# dashboardminikube dashboardReference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 1. 쿠버네티스 소개", "url": "/posts/devlog-platform-kubernetes-in-action1/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-01 17:34:00 +0900", "snippet": "1. 쿠버네티스 소개쿠버네티스 등장 배경 거대한 모놀리스 레거시 애플리케이션은 점차 마이크로 서비스라는 독립적으로 실행되는 더 작은 구성 요소로 세분화되고 있다.마이크로 서비스는 서루 분리돼 있기 떄문에 개별적으로 개발, 배포, 업데이트, 확장할 수 있다. 이로써 오늘날 급변하는 비즈니스 요구사항을 충족시킬 만큼 신속하게 자주 구성 요소를 변경할 수 있게 되었다.하지만 배포 가능한 구성 요소 수가 많아지고 데이터 센터의 규모가 커지면서 전체 시스템을 원활하게 구성, 관리 유지하는 일이 점점 더 어려워졌다.이런 구성 요소의 서버 배포를 자동으로 스케줄링하고 구성, 관리, 장애 처리를 포함하는 자동화가 필요한데, 이것이 바로 “쿠버네티스”가 등장한 이유이다.쿠버네티스 어원 쿠버네티스는 조종사, 조타수(선박의 핸들을 잡고 있는 사람)를 뜻하는 그리스어쿠버네티스 쿠버네티스는 하드웨어 인프라를 추상화하고 데이터 센터 전체를 하나의 거대한 컴퓨팅 리소스로 제공한다. 실제 세세한 서버 정보를 알 필요 없이 애플리케이션 구성요소를 배포하고 실행할 수 있다.쿠버네티스를 이용하여 하드웨어에서 실행되는 수만 개의 애플리케이션을 일일이 알 필요가 없게 되었다.1.1 쿠버네티스와 같은 시스템이 필요한 이유1.1.1 모놀리스 애플리케이션에서 마이크로 서비스 전환 모놀리스 애플리케이션은 앱을 실행하는데 충분한 리소스를 제공할 수 있는 소수의 강력한 서버가 필요하다. 여기서 시스템의 증가하는 부하를 처리할수 있는 방법은 Scale up과 Scale out이 있는데..수직 확장(scale up) 시스템의 증가하는 부하를 처리하려고 하면 CPU, 메모리 등을 추가해 서버를 수직 확장(scale up)하거나 서버를 추가하는 방법 비교적 비용이 많이 들고 실제 확장에 한계가 있음.수평 확장(scale out) 애플리케이션의 복사본을 실행해 전체 시스템을 수평 확장(scale out)하는 방법 상대적으로 저렴하지만, 애플리케이션 코드의 큰 변경이 필요할 수도 있고, 항상 가능하지 않음.(예를 들면 RDBMS 같은 경우에 scale out은 불가하고, HA 구성을 위해서는 다른 방식으로 사용)마이크로서비스로 애플리케이션 분할 하나의 포르세스였던것을 독립적인 프로세스로 나누어서 실행하고, 잘 정의된 API로 상호 통신한다.(Resetful API를 제공하는 HTTP 또는 AMQP와 같은 비동기 프로토콜)마이크로서비스 확장 전체 시스템을 함께 확장해야 하는 모놀리스 시스템과 달리 마이크로 서비스 확장은 서비스별로 수행되므로 리소스가 더 필요한 서비스만 별도로 확장할 수 있으며 다른 서비스는 그대로 둬도 된다.마이크로서비스 배포 마이크로서비스에도 단점이 있는데, 구성요소가 많아지면 배포 조합의 수 뿐만 아니라 구성 요소 간의 상호 종속성 수가 훨씬 더 많아지므로 배포 관련 결정이 점점 더 어려워질 수 있음. 마이크로서비스는 여러 개가 서로 함께 작업을 수행하므로 서로를 찾아 통신해야 하는데, 서비스 수가 증가함에 따라 특히 서버 장애 상황에서 해야 할 일을 생각해봤을 때 전체 아키텍쳐 구성의 어려움과 오류 발생 가능성이 높아진다. 또한 실행 호출을 디버깅하고 추적하기 어려운 단점이 있을수 있는데 Zipkin과 같은 분산 추적 시스템으로 해결은 가능함.환경 요구 사항의 다양성 애플리케이션이 서로 다른 버전의 동일한 라이브러리를 필요로 하는 경우 애플리케이션 구성 요소간 종속성의 차이는 불가피하다. 동일한 호스트에 배포해야 하는 구성 요소 수가 많을수록 모든 요구사항을 충족시키려 모든 종속성을 관리하기가 더 어려워진다.1.1.2 애플리케이션에 일관된 환경 제공 애플리케이션을 실행하는 환경이 다른것은 사실 심각한 문제 중에 하나이다. 개발과 프로덕션 환겨 사이의 큰 차이 뿐만 아니라 각 프로덕션 머신간에도 차이가 있다. 이런 차이는 하드웨어에서 운영체제, 각 시스템에서 사용 가능한 라이브러리에 이르기까지 다양하다. 프로덕션 환경에서만 나타나는 문제를 줄이려면 애플리케이션 개발과 프로덕션이 정확히 동일한 환경에서 실행돼 운영체제, 라이브러리, 시스템 구성, 네트워킹 환경, 기타 모든 것이 동일한 환경을 만들 수 있다면 사실 이상적이다.1.1.3 지속적인 배포로 전환 : 데브옵스와 노옵스 과거 개발 팀의 업무는 애플리케이션을 만들고 이를 배포(CD)하고 관리하며 계속 운영하는 운영팀에 넘기는것(?, 회사마다 다름)이었다고 한다. 개발팀이 애플리케이션을 배포하고 관리하는 것이 모두가 낫다고 생각한다. 개발자, 품질보증(QA), 운영팀이 전체 프로세스에서 협업해야 하는데 이를 “데브옵스”라고 한다.개발자와 시스템 관리자 각자가 최고로 잘하는 것을 하게 하는것! 하드웨어 인프라를 전혀 알지 못하더라도 관리자를 거치지 않고 개발자는 애플리케이션을 직접 배포할 수 있다. 이는 가장 이상적인 방법이며, 노옵스(NoOps)라고 부른다, 쿠버네티스를 사용하면 이런것들을 해결할 수 있음. 하드웨어를 추상화하고 이를 애플리케이션 배포, 실행을 위한 플랫폼으로 제공함으로써, 개발자는 시스템 관리자의 도움 없이도 애플리케이션을 구성, 배포할 수 있으며, 시스템 관리자는 실제 실행되는 애플리케이션을 알 필요 없이 인프라를 유지하고 운영하는 데 집중할 수 있다. 1.2 컨테이너 기술 소개 쿠버네티스는 애플리케이션을 격리하는 기능을 제공하기 위해 리눅스 컨테이너 기술을 사용한다.쿠버네티스 자체를 깊이 파고들기 전에 먼저 컨테이너의 기본에 익숙해져야 한다.또한 도커나 rkt(rock-it)과 같은 컨테이너 기술이 어떤 문제를 해결하는지 이해해야 한다.1.2.1 컨테이너 이해 애플리케이션이 더 작은 수의 커다란 구성요소로만 이뤄진 경우 구성요소에 전용 가상머신을 제공하고 고유한 운영체제 인스턴스를를 제공해 환경을 격리할수 있다. 하지만 MSA로 인해 구성요소가 작아지면서 숫자가 많아지기 시작하면 하뒈어 리소스 낭비가 생길 수 있음. 일반적으로 각각의 가상머신을 개별적으로 구성하고 관리해야 해서 시스템 관리자의 작업량도 상당히 증가한다.리눅스 컨테이너 기술로 구성 요소 격리 가상머신을 사용해 각 마이크로서비스의 환경을 격리하는 대신 개발자들은 리눅스 컨테이너 기술로 눈을 돌림 컨테이너 기술은 가상머신과 유사하게 서로 격리하지만 오버헤드가 훨씬 적음. 컨테이너에서 실행되는 프로세스는 다른 모든 프로세스와 마찬가지로 호스트 운영체제 내에서 실행된다.(가상머신은 별도의 운영체제에서 실행됨)컨테이너와 가상머신 비교컨테이너 컨테이너는 훨씬더 가벼워서 동일한 하드웨어 스펙으로 더 많은 수의 소프트웨어 구성 요소를 실행할 수 있음. 호스트 OS에서 실행되는 하나의 격리된 프로세스일뿐, 앱이 소비하는 리소스만 소비하고 추가 프로세스의 오버헤드가 없음. 호스트 OS에서 실행되는 동일한 커널에서 시스템 콜을 사용한다. 어떠한 종류의 가상화도 필요없음. 각각의 컨테이너가 모두 동일한 커널을 호출함으로 보안 위협이 발생할 수 있다. 컨테이너를 실행할때 VM처럼 부팅할 필요가 없고, 즉시 프로세스를 실행시킬 수 있음.가상머신 구성 요소 프로세스 뿐만 아니라 시스템 프로세스를 실행해야 하기 때문에 추가 컴퓨팅 리소스가 필요하다. 호스트에 가상머신 3개를 실행하면 3개의 완전히 분리된 운영체제가 실행되고 동일한 하드웨어를 공유한다. 시스템콜을 하이퍼 바이저를 통해 받아서 수행 OS를 필요로 하는 하이퍼바이저 타입이 있고, 호스트 타입을 필요로 하지 않는 타입이 있음. 독립적인 커널을 호출하므로 보안 위험이 컨테이너에 비해 적다.컨테이너 격리를 가능하게 하는 매커니즘 첫번째는 리눅스 네임스페이스로 각 프로세스가 시스템(파일, 프로세스, 네트워크 인터페이스, ㅎ스트 이름 등)에 대한 독립된 뷰만 볼 수 있도록 하는것 두번째는 리눅스 컨트롤 그룹(cgroups)으로 프로세스가 사용할 수 있는 리소스(CPU, 메모리, 네트워크 대역폭 등)의 양을 제한하는 것네임 스페이스의 종류 마운트(mnt) 프로세스 ID(pid) 네트워크(net) 프로세스간 통신(ipc) 호스트와 도메인 이름(uts - Unix Time Sharing) 사용자 ID(user)리눅스 네임스페이스로 프로세스 격리 여러 종류의 네임스페이스가 있기 때문에 프로세스는 하나의 네임스페이스에만 속하는 것이 아니라 여러 네임스페이스에 속할 수 있다. 각 네임스페이스는 특정 리소스 그룹을 격리하는데 사용됨. 2개의 서로 다른 UTS 네임스페이스를 프로세스에 각각 지정하면 서로 다른 로컬 호스트 이름을 보게할 수 도 있음.(두 프로세스는 마치 두 개의 다른 시스템에서 실행중인것처럼 보이게 할 수 있음.) 각 컨테이너는 고유한 네트워크 네임스페이슬르 사용하므로 각 컨테이너는 고유한 네트워크 인터페이스 세트를 보수 있음.프로세스의 가용 리소스 제한 프로세스의 리소스 사용을 제한하는 리눅스 커널 기능인 cgroups으로 이뤄진다.1.2.2 도커 컨테이너 플랫폼 소개 도커는 컨테이너를 여러 시스템에 쉽게 이식 가능하게 하는 최초의 컨테이너 시스템 앱, 라이브러리, 여러 종속성, 심지어 전체 OS 파일시스템까지도 도커를 실행하는 다른 컴퓨터에 애플리케ㅣ션을 프로비저닝하는 데 사용할 수 있다. 도커로 패키징된 앱을 실행하면 함께 제공된 파일 시스템 내용을 정확하게 볼수 있다. 앱은 실행중인 서버의 내용은 볼 수 없으므로 서버에 개발 컴퓨터와 다른 설치 라이브러리가 설치돼 있는지는 중요하지 않다. 가상머신에 운영체제를 설치하고 그 안에 앱을 설치한 다음 가상 머신 이미지를 배포하고 실행하는 가상머신 이미지를 만드는것과 유사함. 도커 기반 컨테이너 이미지와 가상머신 이미지의 큰 차이점은 컨테이너 이미지가 여러 이미지에서 공유되고 재사용될 수 있는 레이러로 구성되어 있다는것 동일한 레이러르 포함하는 다른 컨테이너 이미지를 실행할 때 다른 레이어에서 이미 다운로드된 경우 이미지의 특정 레이어만 다운로드 하면됨. 도커 개념 이해 도커는 앱을 패키징, 배포, 실행하기 위한 플랫폼전체 환경과 함꼐 패키지화할 수 있음.도커를 사용하여 패키지를 중앙 저장소로 전송할 수 있고, 이를 도커를 실행하는 모든 컴퓨터에 전송할 수 있음.이미지 애플리케션과 해당 환경을 패키지화한 것 앱에서 사용할 수 있는 파일시스템과 이미지가 실행될때 실행돼야 하는 실행파일 경로와 같은 메타데이터를 포함한다.레지스트리 도커 이미지를 저장하고 다른 사람이나 컴퓨터 간에해당 이미지를 쉽게 공유할 수 있는 저장소 ( push / pull)컨테이너 실행중인 컨테이너는 도커를 실행하는 호스트에서 실행되는 프로세스이지만 호스트와 호스트에서 실행중인 다른 프로세스와는 완전히 격리돼 있음. 리소스 사용이 제한돼 있으므로 할당된 리소스의 양(CPU, RAM 등)만 엑세스하고 사용할 수 있다.도커 이미지의 빌드 및 배포, 실행가상 머신과 도커 컨테이너 비교 가상머신에서 실행될 떄와 두 개의 별도 컨테이너로 실행될 때 앱 A,B가 동일한 바이너리, 라이브러리에 접근할 수 있음. 컨테이너는 격리된 자체 파일시스템이 있는데 이떄 같은 파일을 공유할 수 있음.이미지 레이어의 이해 모든 도커 이미지는 다른 이미지 위에 빌드되고, 2개의 다른 이미지는 동일한 부모 이미지를 사용할 수 있으므로, 서로 정확히 동일한 레이어가 포함될 수 있음. 레이어는 배포를 효율적으로 할 뿐만 아니라 이미지의 스토리지 공간을 줄이는 데에도 도움이 된다. 각 레이어는 동일 호스트에서 한번만 저장됨. 동일한 기본 레이러를 기반으로 한 2개의 이미지에서 생성한 2개의 컨테이너는 동일한 파일을 읽을 수 있지만, 그중 하나가 해당 파일을 덮어쓰면 다른 컨테이너에서는 그 변경 사항을 바라보지 않는다. 파일을 공유하더라도 여전히 서로 격리돼 있는데 이것은 컨테이너 이미지 레이어가 읽기 전용이기 때문 컨테이너가 실행될때 이미지 레이어 위에 새로운 쓰기 가능한 레이어가 만들어진다. 컨테이너 이미지의 제한적인 이식성 이해 이론적으로 컨테이너 이미지는 도커를 실행하는 모든 리눅스 시스템에서 실행될 수 있지만, 호스트에서 실행되는 모든 컨테이너가 호스트의 리눅스 커널을 사용한다는 사실과 관련해 주의해야 한다. 컨테이너화된 앱이 특정 커널 버전이 필요하다면 모든 시스템에서 작동하지 않을수 있음. 머신이 다른 버전의 리눅스 커널로 실행되거나 동일한 커널 모듈을 사용할 수 없는 경우에는 앱이 실행될 수 없다. 컨테이너는 가상머신에 비해 훨씬 가볍지만 컨테이너 내부에서 실행되는 앱은 일정한 제약이 있따. 하드웨어 아키텍처용으로 만들어진 컨테이너화된 앱은 해당 아키텍처 시스템에서만 실행될 수 있다는 점을 분명히 해야 한다.1.2.3 도커의 대안으로 rkt 소개 rkt는 현시점 deprecated됨. 도커와 마찬가지로 컨테이너를 실행하기 위한 플랫폼 2018년 이후로 업데이트 없음.1.3 쿠버네티스 소개1.3.1 쿠버네티스의 기원 구글은 보그(Borg, 이후 오메가(Omega)로 바뀐 시스템)라는 내부 시스템을 개발해 애플리케이션 개발자와 시스템 관리자가 수천 개의 애플리케이션과 서비스를 관리하는데 도움을 주었다. 개발과 관리를 단순화할 뿐만 아니라 인프라 활용률을 크게 높일 수 있었다. 2014년 보그, 오메가, 기타 내부 구글 시스템으로 얻은 경험을 기반으로 하는 오픈소스 시스템인 쿠버네티스를 출시1.3.2 넓은 시각으로 쿠버네티스 바라보기 쿠버네티스는 컨테이너화된 애플리케이션을 쉽게 배포하고 관리할 수 있게 해주는 소프트웨어 시스템 애플리케이션은 컨테이너에서 실행되므로 동일한 서버에서 실행되는 다른 앱에 영향을 미치지 않으며, 이는 동일한 하드웨어에서 완전히 다른 조직의 앱을 실행할때 매우 중요하다. 호스팅된 앱을 완전히 격리하면서 하드웨어를 최대한 활용한다. 모든 노드가 마치 하나의 거대한 컴퓨터인 것처럼 수천대의 컴퓨터 노드에서 스포트웨어 애플리케이션을 실행할 수 있다.쿠버네티스 핵심 이해 시스템은 마스터 노드와 여러 워커 노드로 구성 구성요소가 어떤 노드에 배포되든지 개발자나 시스템 관리자에게 중요하지 않다. 개발자는 앱 디스크립터를 쿠버네티스 마스터에게 게시하면 쿠버네티스는 해당 앱을 워커 노드 클러스터에 배포한다.개발자가 앱 핵심 기능에 집중할 수 있도록 지원 쿠버네티스는 마치 클러스터의 운영체제로 생각할 수 있음. 서비스 디스커버리 ,스케일링, 로드밸런싱, 자가 치유, 리더 선출 같은 것들을 포함하여 지원한다.운영 팀이 효과적으로 리소스를 활용할 수 있도록 지원 각 앱들은 어떤 노드에서 실행되든 상관이 없기 때문에 쿠버네티스는 언제든지 앱을 재배치하고, 조합함으로써 리소스를 수동 스케줄링보다 훨씬 잘 활용할 수 있다.1.3.3 쿠버네티스 클러스터 아키텍쳐 이해 하드웨어 수준에서 쿠버네티스 클러스터는 여러 노드로 구성되며, 2가지 유형으로 나눌 수 있다. 마스터 노드 : 전체 쿠버네티스 시스템을 제어하고 관리하는 쿠버네티스 컨트롤 플레인을 실행워커 노드 : 실제 배포되는 컨테이너 애플리케이션을 실행 컨트롤 플레인(Control Plane) 클러스터를 제어하고 작동시키는 역할 하나의 마스터 노드에서 실행하거나, 여러 노드로 분할되고 복제하여 고가용성을 보장할 수 있는 여러 구성 요소로 구성할 수 있다. API서버는 사용자, 컨트롤 플레인 구성 요소와 통신 스케줄러는 앱의 배포를 담당(앱의 배포 가능한 각 구성요소를 워크 노드에 할당) 컨트롤러 매니저는 구성요소 복제본, 워커 노드 추적, 노드 장애 처리 등과 같은 클러스터단의 기능을 수행 Etcd는 클러스터 구성을 지속적으로 저장하는 신뢰할 수 있는 분산 데이터 저장소노드(Worker Node) 컨테이너화된 애플리케이션을 실행하는 시스템 컨테이너 런타임은 도커, rkt 또는 다른 컨테이너 런타임이 될 수 있다. kebelet은 API 서버와 통힌하고 노드의 컨테이너를 관리한다. kebe-proxy(쿠버네티스 서비스 프록시)는 앱 구성요소간 네트워크 트래픽을 로드밸런싱한다.1.3.4 쿠버네티스에서 애플리케이션 실행 애플리케이션을 하나 이상의 컨테이너 이미지로 패키징하고 해당 이미지를 레지스트리로 푸시한 다음에 쿠버네티스 API 서버에 앱 디스크립션을 게시해야 한다.앱 디스크립션에 포함되는 내용들 컨테이너 이미지 앱 구성요소가 포함된 이미지 해당 구성요소가 서로 통신하는 방법 동일 서버에 함께 배치되어야 하는 구성 요소 실행될 각 구성 요소의 본제본 수 내부 또는 외부 클라이언트에 서비스를 제공하는 구성요소 하나의 IP주소로 노출해 다른 구성 요소에서 검색가능하게 해야 하는 구성요소 등디스크립션으로 컨테이너를 실행하는 방법 이해 1) API 서버가 앱 디스크립션을 처리할때 스케줄러는 각 컨테이너에 필요한 리소스를 계산하고 각 노드에 할당되지 않은 리소스를 기반으로 사용 가능한 워커 노드에 지정된 컨테이너를 할당한다. 2) kebulet은 컨테이너 런타임(도커)에 필요한 컨테이너 이미지를 가져와 컨테이너를 실행하도록 지시한다.실행된 컨테이너 유지 앱이 실행되면 쿠버네티스는 앱의 배포 상태가 사용자가 제공한 디스크립션과 일치하는지 지속적으로 확인한다. 예를 들어 5개의 웹 서버 인스턴스를 실행하도록 지정하면 쿠버네티스는 항상 정확히 5개의 인스턴스를 계속 실행 프로세스 중단 등 인스턴스가 제대로 동작하지 않으면 자동으로 다시 시작 워커 노드 전체가 종료되거나 하면 이 노드에서 실행중인 모든 컨테이너 노드를 새로 스케줄링하고, 새로 선택한 노드에서 실행한다. 복제본 수 스케일링 실행되는 동안 복제본 수를 늘릴지 줄일지 결정할 수 있음. 최적의 본제본 수를 결정하는 작업을 쿠버네티스에게 위힘할 수 있다. 쿠버네티스는 CPU부하, 메모리 사용량, 초당 요청수 등 실시간 메트릭을 기반으로 복제본 수를 자동으로 조정할 수 있다.이동한 애플리케이션에 접근하기 쿠버네티스는 클라이언트가 특정 서비르르 제공하는 컨테이너를 쉽게 찾을 수 있도록 동일한 서비스를 제공하는 컨테이너를 알려주면 하나의 고정 IP주소로 모든 컨테이너를 노출하고 해당 주소를 클러스터에서 실행중인 모든 앱에 노출한다. DNS로 서비스 IP를 조회할 수도 있음. kube-proxy는 서비스를 제공하는 모든 컨테이너에서 서비스 연결이 로드밸런싱되도록 한다. 이런 매커니즘에 의해서 컨테이너들이 클러스터 내에서 이동하더라도 컨테이너에 항상 연결할 수 있음.1.3.5 쿠버네티스 사용의 장점 시스템 관리자는 앱을 배포하고 실행하기 위해 아무것도 설치할 필요가 없음. 개발자는 시스템 관리자의 도움 없이 즉시 앱을 실행할 수 있다.애플리케이션 배포의 단순화 개발자는 클러스트를 구성하는 서버에 관해 알 필요가 전혀 없다. 모든 노드는 단순히 전체 컴퓨팅 리소스일뿐, 앱에 적절한 시스템 리소를 제공할 수 있는 한 어느 서버에서 실행중인지는 신경쓰지 않아도 된다. 특정 앱이 특정 종류의 하드웨어에서 실행해야 하는 경우에도 지원이 가능(예를 들면 SSD를 이용해야 하는 경우)하드웨어 활용도 높이기 인프라와 애플리케이션을 분리해서 생ㅇ각할 수 있다. 쿠버네티스는 요구사항에 대한 디스크립션과 노드에서 사용 가능한 리소스에 따라 앱을 실행한 가장 적합한 노드를 선택할 수 있다. 쿠버네티스는 언제든지 클러스터 간에 앱이 이동할 수 있으모로 수동으로 수행하는것보다 훨씬 더 인프라를 잘 활용할 수 있다.상태 확인과 자가 치유 서버 장애 발생시 언제든지 클러스터 간에 앱을 이동시킬수 있는 시스템이 갖출수 있다. 쿠버네티스는 앱 구성요소와 구동중인 워커 노드를 모니터링하다가 노드 장애 발생시 자동으로 앱을 다른 노드로 스케줄링한다.오토스케일링 급격한 부하 급증에 대응하기 위해 개별 앱의 부하를 운영팀이 지속적으로 모니터링할 필요가 없다. 각 앱이 사용하는 리소스를 모니터링하고, 실행중인 인스턴스 수를 자동으로 조정하도록 지시할수 있다.애플리케이션 개발 단순화 개발과 프로덕션 환경이 모두 동일한 환경에서 실행된다는걸 보장할 수 있어서 버그발견시 큰 효과를 얻을수 있음. 또한 서비스 디스커버리와 같은 일반적으로 구현해야 하는 기능들을 구현할 필요가 없어졌다. 쿠버네티스 API 서버를 직접 쿼리하면 개발자가 리더 선정 같은 복잡한 메커니즘을 구현하지 않아도 된다. 새로운 버전의 앱을 출시할때 신버전이 잘못됐는지 자동으로 감지하고 즉시 롤아웃을 중지할수 있어서 신뢰성을 증가시켜 CD(continuous delivery)을 가속화할수 있음.1.4 요약 모놀리스 애플리케이션은 구축하기 쉽지만 시간이 지남에 따라 유지 관리가 어려워지고 때로는 확장이 불가능할수 있다. 마이크로서비스 기반 애플리케이션 아키텍처는 각 구성요소의 개발을 용이하게 하지만, 하나의 시스템으로 작동하도록 배포하고 구성하기가 어렵다. 리눅스 컨테이너는 가상머신과 동일한 이점을 제공하지만 훨씬 더 가볍고 하드웨어 활용도를 높일 수 있다. 도커는 OS환경과 함꼐 컨테이너화된 애플리케이션을 좀 더 쉽고 빠르게 프로비저닝할 수 있도록 지원해 기존 리눅스 컨테이너 기술을 개선했다. 쿠버네티스는 전체 데이터 센터를 앱 실행을 위한 컴퓨팅 리소스로 제공한다. 개발자는 시스템 관리자의 도움 없이도 쿠버네티스로 앱을 배포할 수 있다. 시스템 관리자는 쿠버네티스가 고장 난 노드를 자동으로 처리하도록 할수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[HackerRank] Java Anagrams", "url": "/posts/devlog-java-algolithm3/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-23 17:34:00 +0900", "snippet": "[HackerRank] Java AnagramsPloblemMy solution char별로 카운트해야 된다고 생각해서 Map에 넣고 Count를 한다음에 각 맵끼리 비교하는것으로 문제를 풀었다.private void anagram(String a, String b) { boolean anagram = isAnagram(a,b); if(anagram) { System.out.println(&quot;Anagrams&quot;); } else { System.out.println(&quot;Not Anagrams&quot;); } } private boolean isAnagram(String a, String b) { if(isValidParam(a, b)) { return false; } Map&amp;lt;Character, Integer&amp;gt; aCountMap = makeCountMap(a); Map&amp;lt;Character, Integer&amp;gt; bCountMap = makeCountMap(b); log.debug(&quot;aCountMap : {}&quot;, aCountMap); log.debug(&quot;bCountMap : {}&quot;, bCountMap); return aCountMap.equals(bCountMap); } private Map&amp;lt;Character, Integer&amp;gt; makeCountMap(String a) { Map&amp;lt;Character, Integer&amp;gt; countMap = new HashMap&amp;lt;&amp;gt;(); for(Character c : a.toLowerCase().toCharArray()) { Integer count = Optional.ofNullable(countMap.get(c)) .map(cnt -&amp;gt; cnt + 1) .orElse(1); countMap.put(c, count); } return countMap; } private boolean isValidParam(String a, String b) { return (a.length() &amp;lt;= 1 &amp;amp;&amp;amp; a.length() &amp;lt;= 50 &amp;amp;&amp;amp; b.length() &amp;lt;=1 &amp;amp;&amp;amp; b.length() &amp;lt;= 50); }Simple solution 다른사람들이 손쉽게 풀이한 문제를 공유한다. 공간복잡도도 더 조금쓰면서 코드도 심플..(시무룩)private void othersAnagram(String a, String b) { if (a.length() != b.length()) { System.out.println(&quot;Not Anagrams&quot;); return; } char[] a1 = a.toLowerCase().toCharArray(); char[] a2 = b.toLowerCase().toCharArray(); Arrays.sort(a1); Arrays.sort(a2); if(Arrays.equals(a1, a2)) { System.out.println(&quot;Anagrams&quot;); } else { System.out.println(&quot;Not Anagrams&quot;); } }Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_Anagrams.javaRefference https://www.baeldung.com/java-strings-anagrams" }, { "title": "Spring AOP 이야기 - AOP는 언제 써야할까?", "url": "/posts/devlog-spring-aop/", "categories": "DevLog, Spring", "tags": "Java, Spring", "date": "2020-06-22 17:34:00 +0900", "snippet": "Spring AOP Spring을 공부하다보면 필연적으로 AOP를 만나게 되는데, 개발자들은 이걸 언제 쓰는게 좋을지 고민하곤 하는데요. 관련해서 사용방법과 언제 써야할지를 한번 알아보겠습니다.1. AOP란? 부가기능 모듈을 객체지향 기술에서 주로 사용하는 오브젝트와는 다르게 특별한 이름으로 부르기 시작한 것 그 자체로 애플리케이션의 핵심 기능을 담고 있지는 않지만, 애플리케이션을 구성하는 중요한 한 가지 요소이고, 핵심 기능에 부가되어 의미를 갖는 특별한 모듈 어드바이저는 가장 단순한 형태의 애스팩트AOP(애스펙트 지향 프로그래밍, Aspect Oriented Programming) OOP를 돕는 보조적인 기술일뿐, OOP를 대체하는 새로운 개념이 아니다. 부가기능이 핵심기능 안으로 침투해버리면 핵심기능 설계에 객체지향 기술의 가치를 온전히 부여하기 힘들어짐. AOP는 애스펙트를 분리함으로써 핵심기능을 설계하고 구현할 때 객체지향적인 가치를 지킬 수 있도록 돕는 것.2. 언제 AOP를 사용해야 할까?2.1 매번 반복되는 개발 패턴들 private final SomethingService service; private final SomethingRepository repository; @Transactional public void doSomething() { if(isCondition()) { // 무언가를 한다. } if(isCondition2()) { // 무언가를 한다. } else { } if(isCondition3()) { if(isCondition4()) { for(Object item : ItemList) { // 무언가를 한다. } } else { // 무언가를 한다. } } } Spring에서 제공하고자 했던 그 기술의 철학과 가치에 부합되는 방식으로 개발하자. 객체지향적인 고민(?)을 더하다. 실제로 객체지향의 핵심인 SOLID 원칙 중 단일책임 원칙을 위배하면서 개발하는 경우는 상당수 사실 이렇게 개발하게 된 이유는 현실적으로 이것 하나 떄문에 이렇게까지 해야 하나에 대한 고민도 있을것이고, if문 하나 추가하면 오히려 가독성 측면이나 유지보수 측면에서 더 좋기 떄문이다. 2.2 그러면 언제 AOP를 써야하는가? 반복적으로 동일한 코드를 여기저기에 개발 적용해야 하는 경우(최우선) 자신의 설계에서 A클래스에 부여한 책임을 넘어서는 수준의 일을 처리해야 하는 경우 이 부분은 반복적으로 동일한 코드가 계속 들어가야 하는 경우의 전제조건이 있어야 한다. 만약 하나의 영역에서만 사용되는 코드를 굳이 AOP로 만드는것은 다른사람의 코드 추적을 어렵게 만들지도 모른다. 해당 처리 로직이 클래스에서 처리해야 하는 핵심로직이냐 아니면 부가 기능이냐를 기준으로 잘 판단해서 부가기능인 경우 이 판단이 사실 명확한 근거는 없다. 개인과 팀이 상황에 따라 적절히 판단하는게 좋을것 같다. 2.3 Sevlet Filter vs Interceptor vs AOP AOP에 대해서 자세히 알아보기 전에 먼저 Servlet Filter, Intercpetor, AOP에 대해서 알아볼 필요가 있다. Spring 웹 애플리케이션을 개발하면서 Filter, Interceptor는 심심찮게 접했을 것이다. 동작 방식은 사실 이 3가지 모두 비슷한 점이 있다. 차이가 있다면 처리되는 시점의 차이가 있다. Servlet Filter는 request 최앞단, 최말단에서 동작하고, Interceptor는 Dispatcher Servlet 다음 단계에서 동작하고 AOP는 개발자가 직접 처리시점을 정할수 있다. 3. AOP 사용하기3.1 Spring boot Project 설정 aop 라이브러리 dependency를 추가한다.&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-aop&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;3.2 AOP Configration Configuration클래스 혹은 Application Class에 @EnableAspectJAutoProxy 을 추가해준다. 나는 별도의 Configration도 각 목적에 따라 나누어 처리하는것을 선호해서 별도의 RootConfig.class를 두었고, 그쪽에 선언하였다.@Configuration@EnableAspectJAutoProxypublic class RootConfig {}@SpringBootApplication(scanBasePackageClasses = {RootConfig.class})public class Application extends SpringBootServletInitializer { public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.addListeners(new ApplicationPidFileWriter()); app.run(args); }3.3 @EnableAspectJAutoProxy @EnableAspectJAutoProxy 는 다른 Enable류와 마찬가지로 auto configration annotation이다. AOP와 관련된 @Aspect와 같은 annotation을 사용할수 있도록 해준다. xml에서는 &amp;lt;aop:aspectj-autoproxy&amp;gt;와 동일한 효과라고 생각하면 된다. @EnableAspectJAutoProxy의 proxyTargetClass attribute가 있는데 이 부분에 대해서도 간략히 알고 있으면 좋다.3.4 EnableAspectJAutoProxy.proxyTargetClass 이 부분은 Spring의 역사 이야기를 조금 해야 하는데 proxy 방식에는 JDK Dynamic Proxy방식과 CGLIB 방식이 있다는 것을 알아야 한다. JDK Dynamic Proxy은 인터페이스를 구현해서 구현된 메소드에만 AOP를 적용할수 있는 방법이었어서 이 방식으로 proxy 처리를 하면 인터페이스에 정의된 메소드에만 proxy를 적용할 수 있다. CDLIB는 대상 클래스를 상속받아서 프록시 객체를 구현하는 방식으로 클래스가 final만 아니라면 proxy를 적용할수 있다. proxyTargetClass 이 옵션은 저 방식을 무엇으로 할건지에 대한 것이다. proxyTargetClass의 기본값은 false인데, 그러면 기본이 JDK Dynamic Proxy인것을 알수 있다. 하지만 가장 최근 문서를 읽어보니면 프록시 대상 클래스가 인터페이스를 구현하지 않으면 기본적으로 CGLIB 프록시가 적용된다고 합니다.(spring 5.2 공식 문서 기준)3.5 AOP Aspect 클래스 정의 Aspect를 정의하고 어느시점에 처리할지에 대한 부분은 @Around, @Before, @After가 있다. @Around는 메소드의 실행 전 / 후의 로직을 컨트롤할수 있다. @Before, @After은 말 그대로 메소드 실행 전 / 후이다. 서비스 비즈니스 로직 개발하는 입장에서는 @Around을 이용해야 하는 케이스가 제일 많았다. 로컬환경에서만 동작해야 하는 스펙이 있다고 가정하고 AOP를 만들어보자.동작하는 스펙은 편의상 String을 return하는 메소드 시그니쳐에 DummyString을 return 한다고 가정한다.@Aspect@Component@Slf4j@RequiredArgsConstructorpublic class MainAspect { private final EnvConfig envConfig; @Around(&quot;execution(String com.sungsu.boilerplate.app.main.MainService.*(..)) &amp;amp;&amp;amp; @annotation(com.sungsu.boilerplate.app.main.aspect.ReturnDummy)&quot;) public Object returnDummy(ProceedingJoinPoint joinPoint) throws Throwable { if (!isLocalEnvironment()) { return joinPoint.proceed(); } // local 환경인 경우 본 메소드를 수행하지 않고 dummy value를 return log.info(&quot;returnDummy&quot;); return &quot;returnDummy&quot;; } /** * 로컬 환경인지 확인 * @return */ private boolean isLocalEnvironment() { return envConfig.isLocal(); }}3.6 Aspect 동작 시점 정의 어느 시점에 Aspect를 동작시킬지에 대한 방법은 다양한데, 대표적으로 포인트컷 표현식이 있다. 하지만 위 3.5 예제에서는 보다시피 포인트컷 표현식(@execution) 과 @annotation을 and 조건으로 사용하고 있다. 왜 Annotation을 이용해서 AOP를 적용했을까? 포인트컷 표현식을 이용한 AOP를 적용했을때의 문제@Aspectpublic class TestAspect { @Pointcut(&quot;execution(* transfer(..))&quot;)// 포인트컷 표현식 private void anyOldTransfer() {}// 포인트컷 시그니처} AOP 적용을 위해서 포인트컷 표현식만을 이용한다고 했을 때 AOP가 적용된 메소드에서는 이 메소드에 AOP를 걸었는지 아닌지를 한눈에 파악하기가 어렵다. 물론 IDE에서는 네비게이션을 제공해주기는 하지만 github을 이용해서 코드리뷰를 한다던지 했을때 이를 쉽게 파악할 수 없다. 대표적인 AOP 예인 @Transactional와 같은 경험을 얻기 위해서 위와 같이 처리한 것이다. 자세한 내용은 여기서는 다루지 않고 참조 링크에서 확인 바란다. ( https://blog.outsider.ne.kr/843 )3.7 annotation 정의하기 Meta Annotations     @Retention 어노테이션이 적용되는 범위(?), 어떤 시점까지 어노테이션이 영향을 미치는지 결정(코드, 클래스로 컴파일, 런타임에 반영 등) @Documented 문서에도 어노테이션의 정보가 표현됩니다. @Target 어노테이션이 적용할 위치를 결정합니다. @Inherited 이 어노테이션을 선언하면 자식클래스가 어노테이션을 상속 받을 수 있습니다. @Repeatable 반복적으로 어노테이션을 선언할 수 있게 합니다. import java.lang.annotation.*;@Inherited@Documented@Retention(RetentionPolicy.RUNTIME) // 컴파일 이후에도 JVM에 의해서 참조가 가능합니다.//@Retention(RetentionPolicy.CLASS) // 컴파일러가 클래스를 참조할 때까지 유효합니다.//@Retention(RetentionPolicy.SOURCE) // 어노테이션 정보는 컴파일 이후 없어집니다.@Target({ ElementType.PACKAGE, // 패키지 선언시 ElementType.TYPE, // 타입 선언시 ElementType.CONSTRUCTOR, // 생성자 선언시 ElementType.FIELD, // 멤버 변수 선언시 ElementType.METHOD, // 메소드 선언시 ElementType.ANNOTATION_TYPE, // 어노테이션 타입 선언시 ElementType.LOCAL_VARIABLE, // 지역 변수 선언시 ElementType.PARAMETER, // 매개 변수 선언시 ElementType.TYPE_PARAMETER, // 매개 변수 타입 선언시 ElementType.TYPE_USE // 타입 사용시}) 추가적인 이해를 위해서 @Transactional 을 인용하자면@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional { @AliasFor(&quot;transactionManager&quot;) String value() default &quot;&quot;; @AliasFor(&quot;value&quot;) String transactionManager() default &quot;&quot;; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default -1; boolean readOnly() default false; Class&amp;lt;? extends Throwable&amp;gt;[] rollbackFor() default {}; String[] rollbackForClassName() default {}; Class&amp;lt;? extends Throwable&amp;gt;[] noRollbackFor() default {}; String[] noRollbackForClassName() default {};} 예제코드에 사용한 annotation은 간단하다. 이정도만으로도 원하는 형태로 충분히 사용 가능하다.@Target({ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface ReturnDummy {} annotation을 통해 특정 값을 전달할수도 있다.@Target({ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface ReturnDummy { String data() default &quot;&quot;;} @ReturnDummy(data = &quot;test&quot;) public String method1() { log.info(&quot;method1&quot;); return &quot;method1&quot;; } 혹시 필요하다면 el expression으로 전달할수도 있으니 참고만 하도록 하자.3.8 AOP 메소드에 적용하기 애노테이션과 Aspect를 정의했다면 이제 Service Layer 같은곳에 적용할 차례이다.// MainController.java@Controller@RequiredArgsConstructorpublic class MainController { private final MainService mainService; @GetMapping(value = {&quot;/&quot;, &quot;/main&quot;}) public String main() { mainService.method1(); mainService.method2(); mainService.method3(); return &quot;index&quot;; }}// MainService.java@Service@RequiredArgsConstructor@Slf4jpublic class MainService { @ReturnDummy(data = &quot;test&quot;) public String method1() { log.info(&quot;method1&quot;); return &quot;method1&quot;; } @ReturnDummy(data = &quot;test&quot;) public String method2() { log.info(&quot;method2&quot;); return &quot;method2&quot;; } @ReturnDummy(data = &quot;test&quot;) public String method3() { log.info(&quot;method3&quot;); return &quot;method3&quot;; }} 이렇게 적용하면 로컬환경에서는 각 method1,2,3을 호출해도 해당 메소드는 호출되지 않고 AOP에 정의한 “dummyReturn” 값만을 return 하게 될 것이다. annotation을 정의할떄 @Target을 METHOD, TYPE 2가지를 보통 정의했는데 위 예제는 메소드 각각에 정의했지만 Class에 바로 적용도 가능한다.@Service@RequiredArgsConstructor@Slf4j@ReturnDummy(data = &quot;test&quot;)public class MainService { public String method1() { log.info(&quot;method1&quot;); return &quot;method1&quot;; } public String method2() { log.info(&quot;method2&quot;); return &quot;method2&quot;; } public String method3() { log.info(&quot;method3&quot;); return &quot;method3&quot;; }} 다만 클래스에 적용하는것보단 메소드단위로 적용하는것이 개발할때 더 편리하다.(IntelliJ IDEA 기준으로 method에 정의한 annotation에서는 trace가 가능한 마킹이 생기는것을 알수 있다.)3.9 AOP 테스트 코드 작성 AspectJProxyFactory를 이용해서 aspect를 주입해주고 Proxy 객체로 덮어 씌워주면 된다. 예제는 아래를 참고하면 된다.@RunWith(MockitoJUnitRunner.class)public class MainServiceTest { @InjectMocks private MainService mainService; @InjectMocks private MainAspect aspect; @Mock private EnvConfig envConfig; @Before public void setUp() { AspectJProxyFactory factory = new AspectJProxyFactory(mainService); factory.addAspect(aspect); factory.setProxyTargetClass(true); mainService = factory.getProxy(); } @Test public void method1() { Mockito.when(envConfig.isLocal()).thenReturn(true); assertTrue(MainAspect.DUMMY_VALUE.equals(mainService.method1())); }}4. AOP에 대한 개인 생각 나는 실제 서비스 개발을 하면서 AOP를 꽤 자주 사용하는 편이다. AOP를 써야할지 말지를 판단할떄는 위에 언급한것처럼 각각의 조건을 충족할떄 하는것이 좋다. AOP가 만능은 아니며, 오히려 유지보수성을 떨어뜨릴지도 모른다. 적절한 상황을 잘 판단하여 사용하도록 하자. 내가 AOP를 실무에 적용한 케이스 A/B테스트 적용을 위해 특정 유저군들에만 한시적으로 기능을 오픈하기 위한 방식으로 사용 성능 테스트시에 다른 서비스(서버 단위)에 영향을 주지 않기 위해 더미 처리를 하기 위해 사용 특정 환경에서만 노출되어야 할 부가정보를 추가해주기 위해 모델의 Deprecated된 필드가 있는데(DB에서 제거) 구버전 앱 지원을 위해 상위 레벨에서는 해당 데이터가 필요한 상황이 있어서 이 값을 바인딩해주는 역할로써 AOP 사용 Redis 기반 Lock 적용시 AOP 사용 이 외 임시로 특정기간동안만 추가되는 로직이 있을때 한시적으로 AOP를 적용해서 처리하고, 그 특정기간이 종료되면 Aspect와 Annotation을 제거하여 쉽게 제거하기 위한 목적 등 5. 예제 코드 https://github.com/sungsu9022/spring-boot-boilerplate/commit/bcc91cc94b7f8fe5c233bbf9efe154fb7a00b6e7 https://github.com/sungsu9022/spring-boot-boilerplateReference https://docs.spring.io/spring/docs/5.2.0.M1/spring-framework-reference/core.html#aop https://blog.outsider.ne.kr/843" }, { "title": "[HackerRank] Java String Reverse", "url": "/posts/devlog-java-algolithm2/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-21 17:34:00 +0900", "snippet": "[HackerRank] Java String ReversePloblemMy solution 예전에 동일한 문제 해결 방법으로 Stack에 넣었다가 pop()으로 reverse 한다는 이야기를 들은적이 있어서 그렇게 구현private void solution(String s) { if(isPalindrome(s)) { System.out.println(&quot;Yes&quot;); } else { System.out.println(&quot;No&quot;); } } private boolean isPalindrome(String s) { Stack&amp;lt;Character&amp;gt; stack = new Stack&amp;lt;&amp;gt;(); for (char c : s.toCharArray()) { stack.push(c); } StringBuilder sb = new StringBuilder(); while(!stack.isEmpty()) { sb.append(stack.pop()); } return sb.toString().equals(s); }Simple solution StringBuilder 객체로 만든 뒤 reverse() 호출private void solution2(String s) { System.out.println( s.equals( new StringBuilder(s).reverse().toString()) ? &quot;Yes&quot; : &quot;No&quot; ); }추가 내용 내가 풀이한 방법대로 했을때 공간복잡도를 생각했을때 더 많이 사용될 수 있음. Stack 인스턴스를 만들어야 하기 떄문.. 만약 StringBulder 없이 하더라면 비슷할것으로 생각된다. StringBuilder.reverse() 코드를 보면 for-loop를 돌면서 처리한다.Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_String_Reverse.javaRefference https://www.hackerrank.com/challenges/java-string-reverse/forum" }, { "title": "[HackerRank] Java Loops II", "url": "/posts/devlog-java-algolithm1/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-13 17:34:00 +0900", "snippet": "[HackerRank] Java Loops IIPloblemMy Solutionimport java.util.*;import java.io.*;import java.lang.Math;class Solution{ public static void main(String []argh){ Scanner in = new Scanner(System.in); int t=in.nextInt(); for(int i=0;i&amp;lt;t;i++){ int a = in.nextInt(); int b = in.nextInt(); int n = in.nextInt(); for(int j=1;j&amp;lt;=n;j++) { int value = a; for(int k=0;k&amp;lt;j;k++) { value += Math.pow(2, k) * b; } System.out.printf(&quot;%d &quot;, value); } System.out.println(&quot;&quot;); } in.close(); }}Best Solution 내가 문제를 보면서 푸는것 말고 더 좋은 방법이 없나 검색해보니 더 좋은 방법이 있었다. 2^0 + 2^1 + ... 2^j = 2^(j+1) - 1 수학 공식을 이용해 푸는 방법인데 저는 재귀함수를 써야하나 loop 한번 더 돌아야할것 같다고만 생각했네요class Solution{ public static void main(String []argh){ Scanner in = new Scanner(System.in); int t=in.nextInt(); StringBuilder sb = new StringBuilder(); for(int i=0;i&amp;lt;t;i++){ int a = in.nextInt(); int b = in.nextInt(); int n = in.nextInt(); sb.setLength(0); for(int j=0; j&amp;lt;n; ++j) { // 2^0 + 2^1 + ... 2^j = 2^(j+1) - 1 sb.append((int) (a + b*(Math.pow(2, j+1) - 1))).append(&quot; &quot;); } System.out.println(sb.toString()); } in.close(); }}Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_Loops_II.javaRefference https://www.hackerrank.com/challenges/java-loops/forum/comments/214175" }, { "title": "OSX에서 MongoDB 설치하기", "url": "/posts/devlog-mongodb/", "categories": "DevLog, MongoDB", "tags": "Infra, MongoDB", "date": "2020-06-08 17:34:00 +0900", "snippet": "OSX에서 MongoDB 설치하기1) brew를 통한 mongoDB 설치 OSX에 Mongo DB를 설치하는 방법은 다양합니다. 공식 사이트( https://www.mongodb.com/download-center/community ) 에서 tgz 파일을 다운받아서 설치할수도 있는데요. OSX를 사용한다면 brew와 같은 패키지 매니저를 통해 여러 애플리케이션을 설치하고 관리하는것이 조금더 일반적이라 brew를 이용한 방법으로 작성하였습니다.brew install mongodb-community mongodb로 설치하는 경우 설치가 되지 않는 문제가 있는데요. homebrew PR-43770( https://github.com/Homebrew/homebrew-core/pull/43770 ) 에 의해 제거되었기 떄문입니다.brew install mongodb 추가로 혹시 인스톨상에 문제가 있다면 아래와 같이 시도해보세요.brew services stop mongodbbrew uninstall mongodbbrew tap mongodb/brewbrew install mongodb-community2) mongoDB 간략 설정 brew를 통해 mongodb가 설치되었다면 /usr/local/etc/mongod.conf 파일이 생성된것을 알수 있습니다. 해당 파일의 log와 dbPath를 적절한 위치로 수정합니다.# 적절히 설정 변경systemLog: destination: file path: /Users/user/logs/mongodb/mongo.log logAppend: truestorage: dbPath: /Users/user/data/mongodbnet: bindIp: 127.0.0.13) mongoDB 실행버전 확인mongo -version(3.1) mongoDB server 실행 brew를 이용해 실행시키는 방법(이 경우는 /usr/local/etc/mongod.conf 설정을 들고 뜨도록 되어있습니다.)brew services start mongodb-community mongod를 직접 사용하여 server를 띄워도 됩니다.mongod --config /usr/local/etc/mongod.conf &amp;amp;ps -ef | grep mongo(3.2) mongoDB server 종료brew services stop mongodb-community mongod를 직접 사용하여 프로세스를 올린 경우에는 아래와 같이 프로세스를 종료시킬수 있습니다.# commend parsing error 발생mongod --shutdownkill &amp;lt;mongod process ID&amp;gt; mongod --shutdown은 document 업데이트가 안된것인지 작동하지 않아 그냥 kill을 이용해서 프로세스를 종료하시면 됩니다.(3.3) mongoDB clientmongoRefference https://docs.mongodb.com/manual/tutorial/manage-mongodb-processes/#stop-mongod-processes https://stackoverflow.com/questions/57856809/installing-mongodb-with-homebrew" }, { "title": "jenv를 활용한 JDK 버전 관리", "url": "/posts/devlog-jenv/", "categories": "DevLog, Java", "tags": "Java", "date": "2020-05-30 17:34:00 +0900", "snippet": "jenv를 활용한 JDK 버전 관리jenv란 무엇인가? jenv는 rbenv에서 사용하는 방식을 본떠서 만든 java version 관리 도구입니다. 다양한 버전의 java application 관리 하는 경우에 적절히 손쉽게 version 변경할 수 있습니다.jenv 설치하기brew install jenvsh 설정 추가 jenv를 설치하면 아래와 같은 문구가 나옵니다.# for bash(terminal, iTerm)echo &#39;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profileecho &#39;eval &quot;$(jenv init -)&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profilesource ~/.bash_profile# for zshecho &#39;export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcecho &#39;eval &quot;$(jenv init -)&quot;&#39; &amp;gt;&amp;gt; ~/.zshrcsource ~/.zshrc 여기서 eval &quot;$(jenv init -)&quot; 이 부분은 사실 .bash_profile에까지 추가할 필요는 없다. 한번만 initialize 해주면 되기 때문에 profile에 넣지 말고 커맨드라인에서 한번만 실행시켜주셔도 됩니다.jenv에 java version 추가하기 Java를 개인이 원하는 별도의 위치에 설치하고 쓰는 경우도 있겠지만 자동 설치파일을 통해 설치한 경우“/Library/Java/JavaVirtualMachines/” 하위에 들어가게 되는데요. 이를 추가해주시면 됩니다. 만약 별도의 디렉토리에서 관리하고 있다면 해당 디렉토리 Home을 기준으로 추가해주면 됩니다.jenv add /Library/Java/JavaVirtualMachines/jdk-11.0.2.jdk/Contents/Homejenv add /Library/Java/JavaVirtualMachines/jdk-14.jdk/Contents/Homejenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Homejenv 등록된 버전 확인jenv versionsjenv 사용하기 jenv를 사용하는 방법은 global, local 단위로 설정하는 방법이 있다. global은 말그대로 전체에 사용할 버전을 명시한것이고, local을 디렉토리 단위에서 사용할 java version을 지정하여 관리하는 방법이다. global을 지정하게 되면 ~/.jenv/version 파일에 사용자가 설정한 버전이 입력되어 관리된다. jenv global 14 local을 지정한 경우에는 지정한 디렉토리의 .java-version 파일이 생성되고 local java version이 관리된다. local 지정한 버전를 제거하고 싶다면 해당 파일을 삭제하면 된다. jenv local 14 Reference https://github.com/jenv/jenv" }, { "title": "이펙티브 자바 3판 - 3. 모든 객체의 공통 메서드", "url": "/posts/devlog-effective-java-3e-3/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-05-17 17:34:00 +0900", "snippet": "이펙티브 자바 3판 - 3. 모든 객체의 공통 메서드 Item10. equals 는 일반 규약을 지켜 재정의하라 Item11. equals를 재정의하려거든 hashCode도 재정의하라 Item12. toString을 항상 재정의하라 Item13. clone 재 정의는 주의해서 진행해라. Item14. Comparable을 구현할지 고려하라.Item10. equals 는 일반 규약을 지켜 재정의하라 equals 재정의하는데 있어서 곳곳에 합정이 있으며, 자칫 잘못하면 끔찍한 결과를 초래할 수 있음. 이를 회피하는 가장 쉬운 길은 아예 재정의하지 않는 것이지만 실제로 개발하다보면 재정의가 필요한 경우도 있을수 있다.(물론 lombok을 사용하면 조금 더 쉬워진다.)재정의를 하지 않아야 하는 상황 각 인스턴스가 본질적으로 고유한 경우(값을 표현하는게 아니라 동작하는 개체를 표현하는 클래스 Thread 클래스 같은것, 추가로 우리가 개발하는 Spring bean용 클래스의 경우를 생각해도 좋을 것 같다. 인스턴스의 논리적 동치성(logical equality)을 검사할 일이 없는 경우 Pattern클래스의 인스턴스 2개가 같은 정규표현식을 나타내는지 검사하는것과 같은 행위(논리적 동치성 검사) 설계자는 이 방식을 원하지 않거나 애초에 필요하지 않다고 판단할 수도 있음. 상위 클래스에서 재정의한 equals가 하위 클래스에도 딱 들어맞는 경우 Collection 클래스류와 같은.. 클래스가 private이거나 package-private이고 equals 메서드를 호출할 일이 없는 경우equals를 재정의를 해야 하는 상황 객체 식별성(object identity, 두 객체가 물리적으로 같은지?)이 아니라 논리적 동치성을 확인해야 하는데, 상위 클래스의 equals가 논리적 동치성을 비교하도록 재정의되지 않은 경우 위에 정의된 말이 조금 이해가 어려운데 쉽게 풀이하면 값 클래스 (Interger, String ..) 값이 같은 인스턴스가 둘 이상 만들어지지 않음을 보장하는 인스턴스 통제 클래스 Boolean.TRUE, Boolean,FALSE, Enum류 등 비즈니스 로직에서 사용할 Entity Class 적절히 PK로 활용될 수 있는 데이터셋을 기준으로 equals를 구현할수 있음 equals 재정의할때 따라야할 일반 규약equals 메서드는 동치 관계(equivalence relation)을 구현하며, 다음을 만족 반사성(reflexivity) : null이 아닌 모든 참조 값 x에 대해 x.equals(x)는 true 대칭성(symmetry) : null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)가 true면 y.equals(x)도 true 추이성(transitivity) : null이 아닌 모든 참조 값 x,y,z에 대해 x.equals(y)가 true이고, y.equals(z)가 true면 x.equals(z)도 true다. 일관성(consistency) : null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)를 반복해서 호출해도 항상 true 또는 false만을 반환 null-아님 : null이 아닌 모든 참조값 x에 대해 x.equals(null)은 false이다.대칭성(symmetry)public final class CaseInsenstiveString{ private final String s; public CaseInsensitiveString(String s){ this.s = Objects.requireNorNull(s); } }@Overridepublic boolean equals(Object o) { if (o instanceof CaseInsensitiveString) return s.equalsIgnoreCase( ((CaseInsensitiveString) o).s); if (o instanceof String) // 한 방향으로만 작동한다! return s.equalsIgnoreCase((String) o); return false;} CaseInsensitiveString.equals(String) 에서는 동작하지만 String.equals(CaseInsensitiveString)에서는 동작하지 않고 한방향으로만 작동하기 때문에 대칭성을 명백히 위반하게 됨.추이성(transitivity) null이 아닌 모든 참조 값 x,y,z에 대해 x.equals(y)가 true이고, y.equals(z)가 true면 x.equals(z)도 true다.public class Point { private final int x; private final int y; public Point(int x, int y) { this.x = x; this.y = y; } @Override public boolean equals(Object o) { if (!(o instanceof Point)) return false; Point p = (Point)o; return p.x == x &amp;amp;&amp;amp; p.y == y; }}public class ColorPoint extends Point { private final Color color; public ColorPoint(int x, int y, Color color) { super(x, y); this.color = color;}// 대칭성 위배@Overridepublic boolean equals(Object o) { if (!(o instanceof ColorPoint)) return false; return super.equals(o) &amp;amp;&amp;amp; ((ColorPoint) o).color == color;}// 추이성 위배@Overridepublic boolean equals(Object o) { if (!(o instanceof Point)) { return false; } // o가 일반 Point면 색상을 무시하고 비교한다. if (!(o instanceof ColorPoint)) { return o.equals(this); } ColorPoint cp = (ColorPoint) o; return cp.point.equals(point) &amp;amp;&amp;amp; cp.color.equals(color);} 대칭성 위배 코드를 살펴보면 Point의 equals는 색상을 무시하고, ColorPoint의 equals는 입력 매개변수의 클래스 종류가 다르다며 매번 false를 반환하게 된다. 추이성 위배 코드를 살펴보면 대칭성은 지켜주지만 추이성을 깨버린다. 구체 클래스를 확장해 새로운 값을 추가하면서 equals 규약을 만족시킬 방법은 존재하지 않는다…(객체 지향적 추상화의 이점을 포기하지 않는 한) 리스코프 치환 원칙(Liskov substitution principle) 어떤 타입에 잇어 중요한 속성이라면 그 하위 타입에서도 마찬가지로 중요. 타입의 모든 메서드가 하위 타입에서도 똑같이 잘 동작해야 함. Point를 기준으로 풀이하면 Point의 모든 하위 클래스는 정의상 모두 Point이므로 어디서든 Point 타입으로 활용될 수 있어야 한다는 말 // 리스코프 치환법칙 위배public boolean equals(Object o) { if (o == null || o.getClass() != getClass()) return false; Point p = (Point) o; return p.x == x &amp;amp;&amp;amp; p.y == y;} 위와 같이 equals를 구현해둔 경우에 Point의 하위 클래스를 정의하고 이를 Set과 같은 컬렉션에 넣어서 처리를 하고자하면 재대로 동작하지 않는걸 확인할 수 있다. 이 원인은 Colletion.contains 동작방식에 있음 Colletion.contains(Object o) 동작방식 대부분의 Collection interface 구현체 대부분은 contains을 판단하는데 있어서 객체의 equals 메서드를 사용하여 검증하므로 이 구현이 의도된대로 정의되어있어야 함. 이 문제를 해결할 수 있는 대표적인 방법은 “상속 대신 컴포지션을 사용하는 것”이다. 상위 클래스 타입의 변수를 멤버로 두고 그것을 반환하는 뷰 메소드를 public으로 작성한다. 아무 값도 갖지 않는 클래스를 베이스로 두고 확장한다.(베이스 클래스를 인스턴스하지 않기 때문에 위배되지 않는다) public class ColorPoint { private final Point point; private final Color color; public ColorPoint(int x, int y, Color color) { point = new Point(x, y); this.color = Objects.requireNonNull(color); } /** * 이 ColorPoint의 Point 뷰를 반환한다. */ public Point asPoint() { return point; } @Override public boolean equals(Object o) { if (!(o instanceof ColorPoint)) return false; ColorPoint cp = (ColorPoint) o; return cp.point.equals(point) &amp;amp;&amp;amp; cp.color.equals(color); }}일관성(consistency) null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)를 반복해서 호출해도 항상 true 또는 false만을 반환 두 객체가 같다면 앞으로도 영원히 같아야 한다는 의미 클래스를 작성할때 불변 클래스로 만드는게 나을지를 심사숙고하자. equals의 판단에 신뢰할 수 없는 자원이 끼어들게 해서는안된다. java.net.URL의 equals가 예다. URL과 ip의 호스트를 비교하는데 이때 네트웍을 통하게 된다. null-아님 동치성 검사를 위해 적절한 형변환 후 값을 비교한다. instanceof는 비교하는 객체가 null 인지 검사한다.(그래서 ==null 을 할 필요 없다)// 명시적 null 검사 필요 없음.@Overridepublic boolean equals(Object o) { if(!(o instanceof MyType)) { return false; } MyType type = (MyType) o; ...}equals 메서드 단계별 구현 방법 1) == 연산자를 사용해 입력이 자기 자신의 참조인지 확인. 2) instanceof 연산자로 입력이 올바른 타입인지 확인. 3) 입력을 올바른 타입으로 형변환 4) 입력 객체와 자기 자신의 대응되는 핵심 필드들이 모두 일치하는지 하나씩 검사equals 관련 팁 primitive type (float, double 제외) 는 “==” 연산자로 비교하고, 레퍼런스 타입 필드는 equals 메서드로 비교한다. float,double은 wrapper class 의 static method인 compare 메서드로 비교한다. 이렇게 특별취급하는 이유는 NaN, -0.0f, 특수한 부정소수 값 등을 다뤄야 하기 때문 wrapper class의 equals를 사용할수도 있는데 오토박싱을 수반할 수 있어서 성능상 좋지 않을 수 있음 성능이 걱정된다면 cost가 적을 것 같은 필드부터 비교한다. 동기화용 락(lock) 필드 같은 논리적 상태와 관련 없는 필드는 연산하지 말자. (논리적 상태만을 비교하자) 캐쉬된 값을 저장하는 파생클래스 변수가 있을 경우 활용하자. 완벽한 equals 를 보여주는 코드 public final class PhoneNumber { private final short areaCode, prefix, lineNum; public PhoneNumber(int areaCode, int prefix, int lineNum) { this.areaCode = rangeCheck(areaCode, 999, &quot;지역코드&quot;); this.prefix = rangeCheck(prefix, 999, &quot;프리픽스&quot;); this.lineNum = rangeCheck(lineNum, 9999, &quot;가입자 번호&quot;); } private static short rangeCheck(int val, int max, String arg) { if (val &amp;lt; 0 || val &amp;gt; max) throw new IllegalArgumentException(arg + &quot;: &quot; + val); return (short) val; } @Override public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof PhoneNumber)) return false; PhoneNumber pn = (PhoneNumber)o; return pn.lineNum == lineNum &amp;amp;&amp;amp; pn.prefix == prefix &amp;amp;&amp;amp; pn.areaCode == areaCode; }} 추가 코멘트 Java Platform 내에서 equlas의 역할, 어떻게 동작하는가 이런 개념적인 내용은 Java 개발자라면 필수로 알고 있어야 하는 개념들이라고 생각합니다. 실제로 실무에서 이렇게 equlas 메서드를 직접 구현하는케이스는 상당히 드믈것이라 생각되는데요. 이미 Lombok 등 다른대안들이 많이 있으니깐요. 하지만 그렇다고 이 개념들을 알 필요가 없는것은 아닙니다.Item11. equals를 재정의하려거든 hashCode도 재정의하라 equals를 재정의한 클래스 모두에서 hashCode도 재정의해야 한다. (Lombok의 annotation이 @EqulasAndHashCode 로 묶어둔 이유기도 함.)hashCode 관련 규약(Object 명세) 1) equals 비교에 사용되는 정보가 변경되지 않았다면 해당 객체의 hashCode는 몇 번을 호출해도 일관된 값을 반환해야 한다. 2) equals(Object)가 두 객체를 같다고 판단했다면 hashCode도 같은 값을 반환해야 한다. 3) equals(Object)가 두 객체를 다르다고 판단했더라도, 두 객체의 hashCode가 서로 다른 값을 반환할 필요는 없다. 단, 다른 객체에 대해서는 다른 값을 반환해야 해시 테이블의 성능이 좋아진다는 것을 알아야 한다.hashCode 일반 규약을 어기는 경우 HashMap, hashSet등에서 문제가 발생한다. 위 2번 조항의 내용과 연관성이 있는데 같은 객체라면 같은 해시코드를 반환해야 hashMap, hashSet에 유일하게 저장될 수 있다.HashMap&amp;lt;PhonNumber,String&amp;gt; m = new Hashmap&amp;lt;&amp;gt;();m.put(new PhoneNumber(707, 867, 5309),&quot;jenny&quot;);m.get(new PhoneNumber(707, 867, 5309)); jenny가 나올것 같지만, 실제 결과는 null 넣을때 한번, 꺼낼때 한번 객체를 생성했다. 이들은 논리적 동치이나. 둘의 hashCode가 다르기 때문에 HashMap에서 찾을 수 없음.이 케이스로 꼭 해야 겠다면 put한 객체를 로컬변수로 저장한 뒤 해당 객체를 get()의 파라메터로 넘겨야 한다.절대로 하지 말아야 할 hashCode 구현 행위@overridepublic int hashcode(){ return 42;} 학창시절 HashTable의 시간복잡도를 배우면서 O(1)의 엄청난 성능을 보이는것으로 배웠을텐데, 위처럼 모든 객체가 같은 hashCode를 return하도록 한다면 O(n)의 성능을 만들어낼 수 있다. 모든 해쉬가 같기 때문에 결국 Linked List처럼 동작하게 된다. hash 관련된 내용이잘 설명되어 있는 블로그일반적으로 hashCoed method를 만드는 방법@Override// PhoneNumber.java의 예public int hashCode() { int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result;} 31을 곱하는 이유는 31이 홀수이면서 소수(prime)이기 떄문 이렇게 복잡한 연산으로 hashCode를 만드는 이유는 hash conflict를 최대한 막아 hash 분포를 잘되게 하여 좋은 성능을 유지시키기 위함이다. hash conflict을 더욱 적은 방법을 쓰려고 할떄 고려사항 구아바의 com.google.common.hash.Hashing 참조 조금 살펴봤는데 충돌을 피하기 위한 여러 알고리즘을 적용된 HashFunction을 생성해주는 Util Class네요. 실제로 매우 많은 데이터를 JVM 위에 올려놓고 하나의 Hash Collection을 이용하는 경우 특정 모델의 성능 향상을 위해서 저런 Util을 써볼수 있을 것 같은데 일반적인 엔터프라이즈 웹 애플리케이션 개발에서는 사용하지 않을 가능성이 높아 보이긴 합니다. 성능을 고려하는 목적으로 핵심필드를 빼고 hashcode를 정의하지 말자.(해시테이블 성능을 놓치게된다)Item12. toString을 항상 재정의하라. default 동작은 “클래스_이름@16진수로_표시한_해시코드”가 반환됨. 위 정보는 아주 쓸모가 없음..항상 적합한 문자열을 반환하지 않는다. PhoneNumber@adbbb (클래스이름@16진수해쉬코드) 를 반환한다. 하위 클래스에서는 간결하고 읽기 쉬운(핵심필드 들을) 형태로 toString을 정의해주는 것이 좋다. 문제(디버깅)에 용이하게 만든다. 모든 핵심 필드들을 출력하는 것이 좋다.``` /** 이 전화번호의 문자열 표현을 반환한다. 이 문자열은 “XXX-YYY-ZZZZ” 형태의 12글자로 구성된다. XXX는 지역 코드, YYY는 프리픽스, ZZZZ는 가입자 번호다. 각각의 대문자는 10진수 숫자 하나를 나타낸다. * 전화번호의 각 부분의 값이 너무 작아서 자릿수를 채울 수 없다면, 앞에서부터 0으로 채워나간다. 예컨대 가입자 번호가 123이라면 전화번호의 마지막 네 문자는 “0123”이 된다. */@Overridepublic String toString() { return String.format(“%03d-%03d-%04d”, areaCode, prefix, lineNum);}``` 하위 클래스에서 상위클래스의 적절한 toString이 있다면 말고 없다면 꼭 구현해주는것이 좋다.코멘트 사실 여기에 대한 생각은 log를 남기거나 하는 경우에 해당 클래스의 정보를 표시해주어야 하므로 그런 경우에만 toString을 구현해주는것이 맞는듯 하기도 하다. Lombok을 사용하는 경우에 @ToString만 붙여주면 아주 예쁘게 노출되기 떄문에 실제로 어려운일이 아니기도 하고 말이다. 사실 ToString 관련해서는 책에서 다루는 개념적으로 아주 중요한 내용은 없는듯 하다.Item13. clone 재 정의는 주의해서 진행해라.Cloneable interface 복제해도 되는 클래스임을 명시하는 용도의 믹스인 인터페이스 아쉽게도 의도한 목적을 제대로 이루지 못했음.. 가장 큰 문제는 clone 메서드가 선언된 곳이 Cloneable이 아닌 Object라는 것(심지어 protected ) 이런 내용을 보면 Cloneable 인터페이스가 하는일이 없어보이지만 동작방식을 살펴보면 용도가 있긴 있다.Object.clone 동작 방식 Object.clone은 Cloneable을 구현한 클래스의 인스턴스에서 호출하면 해당 객체의 필드들을 하나하나 복사한 객체를 반환 그렇지 않은 클래스의 인스턴스에서 호출하면 CloneNotSupportedExpcetion을 반환한다.clone으로 파생되는 문제 Cloneable을 구현한 클래스는 clone 메서드를 public으로 제공하며, 사용자는 당연히 복제가 재대로 이뤄지리라 기대한다. 하지만 모순적인 매커니즘이 탄생할수 있고, 생성자를 호출하지 않고도 객체를 생성하면서 모순적인 매커니즘이 탄생한다. 상속 구조에 있는 클래스에서 clone 메서드가 super.clone이 아닌 생성자를 호출해 얻은 인스턴스를 반환하더라도 컴파일 에러는 없을텐데, 이 경우 하위 클래스에서 super.clone()을 호출한 경우 잘못된 클래스의 객체가 만들어져서 결국 하위 클래스의 clone 메서드가 재대로 동작하지 않을수 있음. 쓸데없는 복사를 지양한다는 관점에서는 굳이 clone 메서드를 제공하지 않는게 좋다.공변 반환 타이핑(convariant return typing) 재정의한 메서드의 반환 타입은 상위 클래스의 메서드의 반환하는 타입의 하위 타입일 수 있다는 의미 이 주제를 벗어나기도 해서 일단은 간략하게 이야기하면 클래스 상속 구조가 아래와 같다고 할때 return type이 Animal이더라도 Dog 객체는 Animal을 구현한 하위 클래스의 객체를 반환할 수 있다고 쉽게 생각해도 좋을 것 같다.private static class Dog extends Animal { ...}private static class Animal { ...}클래스가 가변 객체를 참조하느 경우의 clone// Stack의 복제 가능 버전 (80-81쪽)public class Stack implements Cloneable { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { this.elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // 다 쓴 참조 해제 return result; } public boolean isEmpty() { return size ==0; } // 원소를 위한 공간을 적어도 하나 이상 확보한다. private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} clone 메서드가 단순히 super.clone의 결과를 그대로 반환한다면 size 필드는 올바른 값을 갖겠지만, elements 필드는 원본 Stack 인스턴스와 똑같은 배열을 참조하고 있다. 원본이나 복제본 중 하나를 수정하면 다른 하나도 수정되어 불변식을 해칠수 있고, 프로그램이 이상하게 동작하거나 NPE가 발생할 수 있다.(생성자를 호출해서 만든 객체였다면 이런일은 벌어지지 않았을것이지만, clone은 사실상 생성자와 같은 효과를 냄) clone은 원본 객체에 아무런 해를 끼치지 않는 동시에 복제된 객체의 불변식을 보장해야 함. // 코드 13-2 가변 상태를 참조하는 클래스용 clone 메서드 @Override public Stack clone() { try { Stack result = (Stack) super.clone(); result.elements = elements.clone(); return result; } catch (CloneNotSupportedException e) { throw new AssertionError(); } } 이렇게 처리한다면 위 문제를 해결할수 있겠지만, Cloneable 아키텍처는 ‘가변 객체를 참조하는 필드는 final로 선언하라’는 일반 용법과 충돌이 발생함.(여기서 elemtns와 같은 가변 객체 참조 필드는 실제로 final로 선언되어있어야 하는데 그렇다면 위와 같은 clone 방식으로 처리할 수가 없는것임)clone 정의시 주의사항 생성자에서는 재정의될 수 있는 메서드를 호출하지 않아야 함. 상속용 클래스는 Cloneable을 구현해서는 안된다. Cloneable을 구현한 스레드 안전 클래스를 작성할 때는 clone 메서드 역시 적절히 동기화해줘야 한다.clone 정의 방법 Clonealbe을 구현하는 모든 클래스는 clone을 재정의 해야 하고, 접근제한자는 public으로 반환 타입은 클래스 자신으로 변경한다. super.clone() 호출 후 필요한 필드를 전부 적절히 수정(모든 가변 객체를 복사해야함.)언제 clone을 구현해야 할까? 위에 나열되있는것처럼 모든 가변 객체를 copy해주는 등 clone 구현시에는 복잡한 작업들이 뒤따른다. 그러면 꼭 이렇게까지 구현을 해야하는것일까? Cloneable을 이미 구현한 클래스를확장하는 경우 어쩔 수 없이 clone을 잘 작동하도록 구현해야 한다. 그렇지 않은 상황에서는 더 나은 객체복사 방식을 사용하자(복사 생성자, 복사 팩토리와 같은)// 복사 생성자(conversion constructor)public Yum(Yum yum) { ... };// 복사 팩터리(conversion factory)public static Yum newInstance(Yum yum) { ... };복사 생성자와 복사 팩터리가 Cloneable /clone 방식보다 나은 이유 언어 모순적이고 위험천만한 객체 생성 매니즘(생성자를 쓰지 않는 방식)을 사용하지 않음. 정상적인 final 필드 용법과 충돌하지 않음 불필요한 검사 예외를 던지지 않고 형변환도 필요 없음 해당 클래스가 구현한 인터페이스 타입의 인스턴스도 인수로 받을수 있음.핵심 정리 새로운 인터페이스를 만들 때는 절대 Cloneable을 확장해서는 안되며, 새로운 클래스도 이를 구현해서는 안된다. (final class라면 위험이 크지는 않음) 성능 최적화 관점에서 검토한 후 별다른 문제가 없을 때만 드믈게 허용해야 한다. 복제 기능은 생성자와 팩토리를 이용하는게 최고이나 배열만은 clone 메서드 방식이 가장 깔끔한, 이 규칙의 합당한 예외Item14. Comparable을 구현할지 고려하라. Comparable interface의 유일무이한 메서드인 compareTocompareTo는 Object의 메서드가 아님.위 2가지를 제외하고는 Object의 equals와 같지만, compareTo는 단순 동치성 비교에 더해 순서까지 비교할 수 있으며, 제네릭한 특징을 가진다. Comparable을 구현한다는 것은 그 클래스 인스턴스간에 자연적인 순서(natural order)가 있음을 뜻한다.pulbic interface Comparable&amp;lt;T&amp;gt; { int compareTo(T t);}Comparable.compareTo 메서드 일반 규약 이 객체가 주어진 객체보다 작으면 음의 정수, 같으면 0, 크면 양의 정수 반환(비교불가한 경우 ClassCastException 발생) Comparable을 구현한 클래스는 모든 x,y에 대해 sgn(x.compareTo(y)) == -sgn(y.compareTo(x))여야 한다. 추이성을 보장해야 한다. ( x.compareTo(y) &amp;gt; 0 &amp;amp;&amp;amp; y.compareTo(z) &amp;gt; 0 이면 x.compareTo(z) &amp;gt; 0 모든 z에 대해 x.compareTo(y) == 0이면 sgn(x.compareTo(z)) == sgn(y.compareTo(z))이다. (x.compareTo(y) ==0) == (x.equals(y)) =&amp;gt; 필수는 아니지만 권고사항정렬된 컬렉션에서의 동치성 비교 정렬된 컬렉션들은 일반적으로 equals 대신 compareTo를 사용하여 동치성 검증을 한다. compareTo, equals가 일관되지 않은 BigDecimal 클래스 예제 HashSet&amp;lt;BigDecimal&amp;gt; hashSet = new HashSet(); TreeSet&amp;lt;BigDecimal&amp;gt; treeSet = new TreeSet&amp;lt;&amp;gt;(); BigDecimal a = new BigDecimal(&quot;1.0&quot;); BigDecimal b = new BigDecimal(&quot;1.00&quot;); hashSet.add(a); hashSet.add(b); // [1.0, 1.00] System.out.println(hashSet); treeSet.add(a); treeSet.add(b); // [1.0] System.out.println(treeSet); hashSet에서는 equals 메서드로 비교하여 2개의 객체 인스턴스가 다르다고 판단한다. TreeSet에서는 compareTo 메서드로 비교하기 떄문에 BigDecimal 인스턴스를 같다고 판단한다.compareTo 작성시 주의사항 Comparable은 타입을 인수로 받는 제네릭 인터페이스이므로 compareTo 메서드 인수 타입은 컴파일 타임에 정해진다.(인수 타입 확인 및 형변환 필요 없음) 각 필드가 동치인지 비교하는게 아니라 순서를 비교, 객체 참조 필드를 비교하려면 compareTo 메서드를 재귀적으로 호출한다. Comparable을 구현하지 않은 필드나 표준이 아닌 순서로 비교해야 한다면 Comparator을 대신 사용Java8 이후의 Compare Comparator 인터페이스가 일련의 비교자 생성 메서드(com-parator construction method)와 팀을 꾸려 메서드 연쇄 방식으로 비교자를 생성할 수 있게 됨. d이 방식이 간결하기는 하나 약간의 성능저하가 뒤따를수 있음.private static final Comparator&amp;lt;PhoneNumber&amp;gt; COMPARATOR = comparingInt((PhoneNumber pn) -&amp;gt; pn.areaCode) .thenComparingInt(pn -&amp;gt; pn.prefix) .thenComparingInt(pn -&amp;gt; pn.lineNum);public int compareTo(PhoneNumber pn) { return COMPARATOR.compare(this, pn); } comparingInt((PhoneNumber pn) -&amp;gt; pn.areaCode)에서 명시적 Casting한 부분을 주목해보면, 자바의 타입 추론 능력이 이 람다에서 타입을 알아낼만큼 강력하지 않기 때문에 프로그램이 컴파일되도록 명시적 Casting을 처리해준 것.hashCode 값을 기준으로 하는 비교자 // 이 방식은 사용해서는 안됨. 정수 오버플로우를 일으키거나 부동수점 계산 방식에 따른 오류를 낼수 있음. ( 추이성 위배) static Comparator&amp;lt;Object&amp;gt; hashCodeOrder = new Comparator&amp;lt;&amp;gt;() { public int compare(Object o1, Object o2) { return o1.hashCode() - o2.hashCode() }}// 정적 compare 메서드를 활용한 비교자 static Comparator&amp;lt;Object&amp;gt; hashCodeOrder = new Comparator&amp;lt;&amp;gt;() { public int compare(Object o1, Object o2) { return Interger.compare(o1.hashCode(), o2.hashCode()); }}// 정적 compare 메서드를 활용한 비교자 static Comparator&amp;lt;Object&amp;gt; hashCodeOrder = Comparator.comparingInt(o -&amp;gt; o.hashCode());}핵심정리 순서를 고려해야 하는 값 클래스를 작성한다면 꼭 Comparable 인터페이스를 구현하여 그 인스턴스들을 쉽게 정렬하고, 검색하고, 비교 기능을 제공하는 컬렉션과 어우러지도록 해야 한다. compareTo 메서드에서 필드의 값을 비교할때는 “&amp;lt;”, “&amp;gt;” 사용하지 말아야 한다. 박싱된 기본 타입 클래스에서 제공하는 정적 compare 메서드나 Comparator인터페이스가 제공하는 비교자 생성 메서드를 사용하자추가 코멘트 equals, hashCode, Comparable 같은 경우 자바를 이용하는 개발자라면 필수적으로 개념을 확실히 알고 있어야 하지만, 간혹 놓치고 개발하다가 치명적인 버그로 이어지는 경우가 있다. 이런 케이스로 발생하는 버그는 코드를 읽으면서 쉽게 발견되지 않기 때문에 잘 알고 사용하는것이 중요하다. equals, hashCode는 사실상 Lombok같은 라이브러리를 통해서 이용한다면 실수는 거의 없을 것이라고 생각된다. clone은 실제로 직접 개발한 코드에서 정의해서 사용해본 적은 없다. 책에서도 정리된 것처럼 가급적이면 해당 메서드를 구현해야 할 필요성을 못느끼는데 레거시 코드에서 혹시 clone이 정의되어있고, 내가 추가로 개발한 부분이 연관성이 있다면 그떄 관련 개념을 좀 알고 있는것이 도움이 될 것 같다." }, { "title": "Spring Boot JSP 자동 reload 되지 않는 문제 해결 방법", "url": "/posts/devlog-spring-boot-tools/", "categories": "DevLog, Spring", "tags": "Java, Spring, Back-end", "date": "2020-05-03 17:34:00 +0900", "snippet": "Spring Boot JSP 자동 reload 되지 않는 문제 해결 방법내용 legacy Spring Project에서 Spring boot로 전환되었을때 즉시 발견할수 있는 문제중 하나는 IDE에서 jsp파일을 수정해도 auto reload되지 않는 문제입니다.이 경우 JSP에 수정사항을 반영하기 위해서는 Application을 재시작해야 하는데요. 매번 이런 번거로움을 감수하면서 개발하는 것은 말도 안되죠.이를 해결할수 있는 방안이 이미 마련 되어 있습니다.적용 방법1. Dependency 추가 maven 을 사용하는 경우&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-devtools&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt;&amp;lt;/dependencies&amp;gt; gradle을 사용하는 경우dependencies { compile(&quot;org.springframework.boot:spring-boot-devtools&quot;)}2. spring application.yml 또는 application.properties에 아래 설정을 추가spring.devtools.livereload.enabled=truespring: devtools: livereload: enabled: trueReference https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-devtools-livereload" }, { "title": "Spring Boot + JPA + H2 Application 간단하게 띄우기", "url": "/posts/devlog-spring-boot-application/", "categories": "DevLog, Spring", "tags": "Java, Spring, Back-end", "date": "2020-05-01 01:34:00 +0900", "snippet": "Spring Boot + JPA + H2 Application 간단하게 띄우기 1. 프로젝트 만들기 2. 애플리케이션 기본 설정 3. Web MVC Configuration 4. JSP Configration 5. JPA Configration1. 프로젝트 만들기1.1 spring initializr을 통해 프로젝트 뼈대 만들기-https://start.spring.io/ 에서 손쉽게 dependency 가 추가된 프로젝트 틀을 만들수 있다. 하지만 이렇게 만든다고 끝은 아니고.. 추가로 이것저것 설정 등을 해주어야 한다.2. 애플리케이션 기본 설정 아래와 같이 @SpringBootApplication만 붙이면 바로 Application을 구동시킬수 있다. @SpringBootApplication(scanBasePackageClasses = {RootConfig.class})public class Application extends SpringBootServletInitializer { public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.addListeners(new ApplicationPidFileWriter()); app.run(args); } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(Application.class); }} 하지만 일반적으로 만들고자 하는 웹 애플리케이션을 만들려면 추가적인 설정들이 필요한데 필자는 scanBasePackageClasses 속성을 이용해서 RootConfig Configuration class를 만들고 이를 기준으로 처리하도록 하였다.@Configuration@ComponentScan(basePackageClasses = ApplicationPackageRoot.class)public class RootConfig {} 여기서 특정 패키지 루트 하위에 있는 모든 bean 컴포넌트들을 스캔시키기 위해 최상단에 ApplicationPackageRoot 라는 inteface를 만들고 이 패키지 위치를 기준으로 @ComponentScan 처리하도록 했음. 참고로 @Configuration annotation을 사용하면 우리가 일반적으로 AutoConfigration을 위해 사용하는 @Enable{XXX} 류의 설정용 Bean을 생성하여 사용할 수 있다. 이런식으로 프로젝트 구조를 잡고 설정을 했을떄 패키지 구조를 살펴보면 다음과 같다.3. Web MVC Configuration 먼저 dependency를 추가하자.&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt; EnableWebMvc를 통해 기본적인 mvc 설정은 모두 커버가 된다. 자세한 내용이 알고 싶다면 DelegatingWebMvcConfiguration.class 파일을 참조하도록 하자@EnableWebMvcpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(&quot;/static/bundle/**&quot;) .addResourceLocations(&quot;/static/bundle/&quot;).setCachePeriod(3600) .resourceChain(true).addResolver(new PathResourceResolver()); }}4. JSP Configration jsp를 사용하려면 일단 pom.xml에 dependancy를 추가해주어야 한다.&amp;lt;!-- jsp --&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-tomcat&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.tomcat.embed&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;tomcat-embed-jasper&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;javax.servlet&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;jstl&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt; 위 디펜던시를 추가하지 않으면 jsp 파일을 찾을수 없다고 404 Page Not Found를 맞이할 수 있을 것이다. 그리고 템플릿 엔진으로 jsp를 사용하기 위해 필자가 추가로 한 부분은 internalResourceViewResolver Bean을 추가한 것이다. @Configuration@EnableWebMvcpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(&quot;/static/bundle/**&quot;) .addResourceLocations(&quot;/static/bundle/&quot;).setCachePeriod(3600) .resourceChain(true).addResolver(new PathResourceResolver()); } @Bean public ViewResolver internalResourceViewResolver() { final InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setPrefix(&quot;/WEB-INF/templates/&quot;); viewResolver.setSuffix(&quot;.jsp&quot;); return viewResolver; }} 이를 통해 jsp web 페이지를 랜더링할 수 있다. 여기서 이런 방식 말고 다른 방법으로 처리할수도 있는데 그 방법은 application.yml 파일에 설정을 추가해주는 것이다. spring: mvc: view: prefix: /WEB-INF/templates/ suffix: .jsp 5. JPA Configration5.1 DataSource 실제 웹 애플리케이션을 운영하는 것이 아니라 샘플 프로젝트를 만들고 연습하기에 가장 쉬운 방법은 H2 in memory DB를 사용하는 것일 것이다. 이를 사용하기란 매우 간단하다. 먼저 h2 dependency를 pom.xml에 추가한다.&amp;lt;!-- db --&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.h2database&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;h2&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt; PersistenceConfigration에서 DataSource Bean을 정의한다. @Beanpublic DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.H2).build();} 이제 H2를 사용할 준비는 끝났다. 이제 다음으로 넘어가서 JPA 설정을 해보자5.2 JPA 여기서 JPA라고 이야기했지만, 실제로는 JPA 구현체 중에서 유명한 것 중 하나인 hibernate를 사용할 것이다. 이를 사용하기 위해서는 먼저 dependency를 추가해주어야 한다.&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-data-jpa&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.hibernate&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hibernate-ehcache&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;5.4.14.Final&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt; 그리고 이와 관련된 configuration 값들을 정의해준다. spring: datasource: driver-class-name: org.h2.Driver url: jdbc:h2:mem:test;DB_CLOSE_DELAY=-1 username: password: h2: console: enabled: true path: /h2 jpa: database-platform: org.hibernate.dialect.H2Dialect database: H2 generate-ddl: false open-in-view: false show-sql: true hibernate: ddl-auto: create naming: physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl implicit-strategy: org.hibernate.boot.model.naming.ImplicitNamingStrategyLegacyJpaImpl properties: hibernate: format_sql: true use_sql_comments: true cache.use_second_level_cache: true cache.use_query_cache: false generate_statistics: true cache.region.factory_class: org.hibernate.cache.ehcache.EhCacheRegionFactory 이 외에도 hibernate 설정은 아주 다양하게 많이 있을텐데, 자세한 것은 공식 문서를 참조하도록 하자. ( https://hibernate.org/orm/documentation/5.4/ ) 사실 위에서 가장 중요한 부분은 “hibernate.ddl-auto” 설정 값인데 이 값은 만약 실제 서비스에서 사용하고자 한다면 “validate” 정도의 설정값을 사용하는게 좋다. 하지만 예제에서는 인메모리 DB를 사용하는 특성상 create를 사용한것이니 참고하자. application.yml에 설정값을 넣어두었다면 이제 Java Configration 을 정의해보자 @Configuration@EnableJpaRepositories(basePackages = &quot;com.sungsu.boilerplate.app&quot;)@EnableTransactionManagementpublic class PersistenceConfig { @Bean public DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.H2).build(); } @Bean public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) { JpaTransactionManager txManager = new JpaTransactionManager(); txManager.setEntityManagerFactory(entityManagerFactory); return txManager; }} @EnableJpaRepositories의 basePackages 하위에 있는 @Entity 와 @Repository를 자동 등록이 된다. EnableJpaRepositories를 보면 default로 “transactionManager” 를 사용하기 때문에 PlatformTransactionManager Type의 클래스로 bean 등록을 해주면 된다. 이렇게 하면 H2 + JPA application이 완성되었다.ResultconsolewebComment 작성된 내용 중 혹시 제가 잘못 알고 있는 내용이 있다면 언제든지 코멘트 부탁드립니다. 이 포스팅을 쓰면서 만든 샘플코드는 github에 올려두었다. 혹시 신규 프로젝트를 시작하고자할떄 필요하다면 언제든지 사용하셔도 됩니다. https://github.com/sungsu9022/sprping-boot-boilerplate " }, { "title": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 2", "url": "/posts/devlog-effective-java-3e-2-2/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-04-28 01:34:00 +0900", "snippet": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 2 Item5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라. Item6. 불필요한 객체 생성을 피하라 Item7. 다 쓴 객체 참조를 해제하라 Item8. finalizer 와 cleaner 의 사용을 피하라 Item9. 생성자에 매개변수가 많다면 빌더를 고려하라Item5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라. 사용하는 자원에 따라 동작이 달라지는 클래스에는 정적 유틸리티 클래스나 싱글턴 방식이 적합하지 않음. 인스턴스를 생성할 때 생성자에 필요한 자원을 넘겨주는 방식을 사용 의존 객체 주입은 유연성과 테스트에 용이하다. public class SpellChecker { private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) { this.dictionary = Objects.requireNonNull(dictionary); } public boolean isVlaid(String word) { ... } public List&amp;lt;String&amp;gt; suggestions(String type) { ... }} 의존 객체 주입을 생성자, 정적 팩터리, 빌더 등 아무런 방법에 적용하면 됨.팩터리 메서드 패턴 의존 객체 주입의 쓸만한 변형 방식 생성자에 자원 팩터리 객체를 넘겨주는 방식 Supplier 인터페이스를 사용하면 됨. 한정적 와일드카드 타입(bounded wildcard type)을 사용해 팩터리의 타입 매개변수를 제한 Mosaic create(Supplier&amp;lt;? extends Tile&amp;gt; titleFactory) { ... } 의존 객체 주입이 유연성과 테스트 용이성을 개선해주긴 하지만, 의존성이 많아지면 코드를 어렵게 만들기도 함.핵심 정리자원이 클래스 동작에 영향을 준다면 싱글턴과 정적 유틸리티 클래스는 사용하지 않는 것이 좋다.(의존 객체 주입을 통해 하자)Item6. 불필요한 객체 생성을 피하라똑같은 기능의 객체를 매번 생성하기 보다는 객체 하나를 재사용하는 편이 나을 때가 많다.(당연한 이야기)String 인스턴스 관련String s = new String(&quot;bikini&quot;); // 따라하지 말것.String s = &quot;bikini&quot;; 생성자로 생성하는 케이스는 매번 새로운 String 인스턴스를 생성한다. 2번쨰 방식을 사용하면 하나의 String 인스턴스를 사용하고, 가상 머신 안에서 이와 똑같은 문자열 리터럴을 사용하는 모든 코드가 같은 객체를 재사용함이 보장된다. 참조 : https://docs.oracle.com/javase/specs/jls/se7/html/jls-3.html#jls-3.10.5 Boolean(String) 생성자 «« Boolean.valueOf(String)생성비용이 비싼 객체 처리 비싼 객체가 반복해서 필요하다면 캐싱하여 재사용한다. String.matches 메소드를 쓰면 간편하지만, 성능이 중요한 상황에서 반복해서 사용하기엔 적합하지 않음. 해당 메소드에서 생성하는 Pattren 객체는 한번 쓰고 버려짐. Pattren 유한 상태 머신(finite sate machine)을 만들기 때문에 인스턴스 생성 비용이 높음. Regular Expression -&amp;gt; Pattren 객체를 이용 String.matches vs Pattern.matchers 성능 비교 1.1마이크로s / 0.17 마이크로s 오토박싱 오토박싱이란 기본타입과 박싱된 기본 타입을 섞어 쓸때 자동으로 상호 변환해주는 기술. 불필요한 객체를 만들어내는 예 중 하나이다. 오토 박싱이 기본 타입과 그에 대응하는 박싱된 타입의 구분을 흐려주지만, 완전히 없애주는 것은 아님. private static long sum() { Long sum = 0L; for (long i = 0; i &amp;lt;= Integer.MAX_VALUE; i++) sum += i; return sum; } sum 변수의 long이 아닌 Long으로 선언해서 불필요한 Long 인스턴스가 생성됨. 박싱된 타입보다는 기본 타입을 사용하고, 의도치 않은 오토박싱이 숨어들지 않도록 주의하자.객체 생성이 비싸니 피해야 한다(?) -&amp;gt; 객체 풀 아주 무거운 객체가 아닌 다음에야 단순히 객체 생성을 피하고자 객체 풀을 만들다던지는 안하는 것이 좋음. DB 연결같은 경우 생성비용이 비싸니 재사용하는 편이 낫지만 그렇지 않은 경우가 많음. 객체 풀은 코드를 헷갈리게 만들고 메모리 사용량을 늘리고, 성능을 떨어뜨린다.방어적 복사 vs 불필요한 객체 생성 방어적 복사가 필요한 상황에서 객체를 재사용했을 때의 피해는 필요 없는 객체를 반복 생성했을 때의 피해보다 훨씬 크다. 잘 모르면 차라리 불필요한 객체 생성은 여러번 하는게 나을수도 있음. Item7. 다 쓴 객체 참조를 해제하라메모리 누수public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); return elements[--size]; } /** * 원소를 위한 공간을 적어도 하나 이상 확보한다. * 배열 크기를 늘려야 할 때마다 대략 두 배씩 늘린다. */ private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 위 스택을 오래 수행하다보면 점차 가비지 컬렉션 활동과 메모리 사용량이 늘어나 성능 저하 발생 메모리 누수의 원인? 객체들의 다 쓴 참조(obsolete reference)을 여전히 가지고 있기 때문. ( elements 배열의 활성 영역 밖 ) 해법 해당 참조를 다 썼을 때 null 처리(참조 해제) - pop 시점에 null 처리 public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // 다쓴 참조 해제 return result;} 객체 참조를 null 처리 해야 하는 경우 객체 참조를 null 처리하는 일은 예외적인 경유여야 한다. 가장 좋은 참조 해제 방법 참조를 담은 변수를 유효 범위(scope) 밖으로 밀어내는 것 자기 메모리를 직접 관리하는 클래스 인 경우 프로그래머가 항상 메모리 누수에 주의해야 함. 캐시 역시 메모리 누수를 일으키는 주범 WeakHashMap http://blog.breakingthat.com/2018/08/26/java-collection-map-weakhashmap/ key의 참조가 사라지면 자동으로 GC 대상이 됨 정확히 이런 케이스에서만 유용```public class WeakHashMapTest { public static void main(String[] args) { WeakHashMap&amp;lt;Integer, String&amp;gt; map = new WeakHashMap&amp;lt;&amp;gt;(); Integer key1 = 1000; Integer key2 = 2000; map.put(key1, &quot;test a&quot;); map.put(key2, &quot;test b&quot;); key1 = null; System.gc(); //강제 Garbage Collection map.entrySet().stream().forEach(el -&amp;gt; System.out.println(el)); }}``` LinkedHashMap http://javafactory.tistory.com/735 리스너(Listener) 혹은 콜백(Callback) 클라이언트 코드에서 콜백을 등록만 하고 명확히 해제하지 않는 경우에 발생할 수 있음. 콜백을 약한 참조(weak reference)로 저장하면 즉시 수거 ( WekHashMap에 키로 저장) 핵심 정리 메모리 누수는 겉으로 잘 드러나지 않음. 철저한 코드리뷰 힙 프로파일러 같은 디버깅 도구를 동원해야만 발견되기도 함. 즉 발견하기 어렵기 때문에 예방법을 잘 익히자!Item8. finalizer 와 cleaner 의 사용을 피하라GC는 컨트롤 가능한가? 내가 원할때 소멸시키는가 / 아니다. finalizer의 대안 cleaner 역시 문제가 많다. try with resource(auto closable) vs finalize gc 성능이 50배(12ns vs 550ns) 차이 난다. 그럼 언제 저것들을 쓰고 있나? / 효과있나? 닫지 않은 파일/커넥션등을 아주~늦게 나마 회수해준다.(FileInputStream, ThreadPoolExecutor) 네이티브피어(jni 같이 c 등 다른 언어 메소드를 연결하는 것) 객체(자바 객체가 아니니 알지 못해서) //이때는 성능저하가 불가피 할 듯 보이고 close()를 꼭 해야할 것 같다. 이 대안은 그럼 무엇? AutoCloseable을 구현한다.``` javapublic class Room implements AutoCloseable { private static final Cleaner cleaner = Cleaner.create(); // 청소가 필요한 자원. 절대 Room을 참조해서는 안 된다! private static class State implements Runnable { int numJunkPiles; // Number of junk piles in this room State(int numJunkPiles) { this.numJunkPiles = numJunkPiles; } // close 메서드나 cleaner가 호출한다. @Override public void run() { System.out.println(&quot;Cleaning room&quot;); numJunkPiles = 0; } } // 방의 상태. cleanable과 공유한다. private final State state; // cleanable 객체. 수거 대상이 되면 방을 청소한다. private final Cleaner.Cleanable cleanable; public Room(int numJunkPiles) { state = new State(numJunkPiles); cleanable = cleaner.register(this, state); } @Override public void close() { cleanable.clean(); }}``` 사실 위의 코드는 정확한 흐름을 모르겠다.(수정필요) 그럼 힙 메모리 세팅, gc 종류 선택 등에 대해 공유(난 경험이 없다..)Item9. try - finally 보다 try-with-resource를 사용하라 자원(파일, 커넥션) 을 닫는 것을 클라이언트가 놓칠 수 있다.(커넥션이 계속 열고 안닫아지면..) 일반적으로 finally에 close를 많이 하는게 대다수이지만 JAVA7에서 추가된 try-with-resource를 사용하는것이 좋음. static String firstLineOfFile(String path) throws IOException { BufferedReader br = new BufferedReader(new FileReader(path)); try { return br.readLine(); } finally { br.close(); } } 만약 한번더 오픈을 한다면? static void copy(String src, String dst) throws IOException { InputStream in = new FileInputStream(src); try { OutputStream out = new FileOutputStream(dst); try { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &amp;gt;= 0) out.write(buf, 0, n); } finally { out.close(); } } finally { in.close(); } } 이 경우 어떤 문제에 의해서 close에서도 문제가 생기다면? 두번째(close)예외의 메시지만 준다. 그래서 문제 파악을 힘들게 만든다. 아래는 try with resource 로 고친 코드``` java static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); }}static void copy(String src, String dst) throws IOException { try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &amp;gt;= 0) out.write(buf, 0, n); } }```" }, { "title": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 1", "url": "/posts/devlog-effective-java-3e-2-1/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-04-25 01:34:00 +0900", "snippet": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 1 Item1. 생성자 대신 정적 팩터리 메서드를 고려하자. Item2. 생성자에 매개변수가 많다면 빌더를 고려하라 Item3. private 생성자나 열거 타입으로 싱글턴임을 보증하라. Item4. 인스턴스화를 막으려거든 private 생성자를 사용하라. 객체를 만들어야 할 때와 만들지 말아야 할 때를 구분하는 법올바른 객체 생성 방법과 불필요한 생성을 피하는 방법제때 파괴됨을 보장하고 파괴 전에 수행해야 할 정리 작업Item1. 생성자 대신 정적 팩터리 메서드를 고려하자. 클래스는 생성자와 별도로 정적 팩터리 메서드(static factory method)를 제공할 수 있다. public static Boolean valueOf(boolean b) { return b ? Boolean.TRUE : BOolean.FALSE;} 정적 팩터리 메서드가 생성자보다 좋은 장점 5가지 1 이름을 가질 수 있다. BigInteger(int, int, Random)과 정적 팩터리 메서드인 BigInteger.probablePrime 중 ‘어느 쪽이 값이 소수인 BigInteger를 반환한다’는 의미를 더 잘 설명할 것 같은지? 각 목적에 맞는 생성자가 여러개 있다고 했을때, 각각의 생성자가 어떤 역할을 하는지 정확히 기억하기 어려워 엉뚱한 것을 호출하는 실수를 할 수 있다. 2 호출될 때마다 인스턴스를 새로 생성하지는 않아도 된다. 불변 클래스(immutable class)는 인스턴스를 미리 만들어 놓거나 새로 생성한 인스턴스를 캐싱하여 재활용하는 식으로 불필요한 객체 생성을 피할 수 있다. Boolean.valueOf(boolean) 메서드는 객체를 아예 생성하지 않음.(성능 향상에 기여) 반복되는 요청에 같은 객체를 반환하는 식으로 인스턴스를 철저히 통제할 수 있음 - 통제(instance-controlled) 클래스 이렇게 통제하는 이유 싱글턴(singleton)으로 만들수도, 인스턴스화 불가(noninstantiable)로 만들 수 있음 동치인 인스턴스가 단 하나뿐임을 보장 가능(열거타입과 같이) 플라이웨이트 패턴의 근간( 나중에 보는걸로..) 3 반환 타입의 하위 타입 객체를 반환할 수 있는 능력이 있다. 추상화된 구조인 경우 인터페이스를 정적 팩터리 메서드의 반환 타입으로 해서 구현 클래스를 공개하지 않고 객체를 반환할 수 있음. API를 작게 유지할 수 있다. Collection 프레임워크는 45개 클래스를 공개하지 않기 때문에 API 외견을 훨씬 작게 만들수 있었음. 자바8부터는 인터페이스가 정적 메서드를 가질 수 없다는 제한이 풀려서 인스턴스화 불가 동반 클래스를 둘 이유가 별로 없다. 자바9에서는 private 정적 메서드까지 허락하지만, 정적 필드와 정적 멤버 클래스는 여전히 public이어야 한다. 4 입력 매개변수에 따라 매번 다른 클래스의 객체를 반환할 수 있음. EnumSet 클래스는 public 생성자 없이 오직 정적 팩터리만 제공. 원소가 64개 이하면 ㅣong변수 하나로 관리 -&amp;gt; RegularEnumSet 원소가 65개 이상이면 long 배열로 관리하는 JumboEnumSt 클라이언트는 위와 같은 사실을 몰라도 됨 5 정적 팩터리 메서드를 작성하는 시점에는 반환할 객체의 클래스가 존재하지 않아도 된다. 이러한 유연함은 Service Provider 프레임워크의 근간이 된다. 대표적인 예로 JDBC 있다. DriverManager.registerDriver() 메서드로 각 DBMS별 Driver를 설정한다. (제공자 등록 API) DriverManager.getConnection() 메서드로 DB 커넥션 객체를 받는다. (service access API) Connection Interface는 DBMS 별로 동작을 구현하여 사용할 수 있다. (service Interface) 위와 같이 동작하게 된다면 차후에 다른 DBMS가 나오더라도 같은 Interface를 사용하여 기존과 동일하게 사용이 가능하다. 정적 팩터리 메서드 단점 1 상속을 하려면 public이나 protected 생성자가 필요하니 정적 팩터리 메서드만 제공하면 하위 클래스를 만들 수 없다. 상속보단 컴포지션을 사용하도록 유도하고 불변 타입으로 만들려면 이 제약을을 지켜야 하는건 오히려 장점 2 정적 팩터리 메서드는 프로그래머가 찾기 어렵다. 생성자처럼 잘 드러나지 않음. API 문서를 잘 써놓고 메서드 이름도 널리 알려진 규약을 따라 지어야 함. 명명 규칙 설명 예시 from 매개변수를 하나를 받아서 생성 Date d = Date.from(instant) of 여러 매개변수를 받아서 생성 Set&amp;lt;Rank&amp;gt; faceCards = EnumSet.of(JACK, QUEEN, KING); valueOf from과 of의 더 자세한 버전 BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE); instace 혹은 getInstance 매개변수로 명시한 인스턴스를 반환, 같은 인스턴스 보장(x) StackWalker luke = StackWalker.getInstnace(options); create 혹은 newInstance 매번 새로운 인스턴스를 생성해 반환 Object newArray = Array.newInstace(classObject, arrayLen); get”Tpye” 생성할 클래스가 아닌 다른 클래스에 팩터리 메서드를 정의할 때 FileStore fs = Files.getFileStore(path); new”Type” 매번 새로운 인스턴스를 생성해 반환하지만 다른 클래스에 팩터리 메서드를 정의할 때 BufferedReader br = Files.newBufferedReader(path); “type” getType과 newType의 간결한 버전 List&amp;lt;Complaint&amp;gt; litany = Collections.list(legacyLitany); 핵심 정리 정적 팩터리 메서드와 public 생성자는 각자의 쓰임새가 있으니 상대적인 장단점을 이해하고 사용하는 것이 좋다.Item2. 생성자에 매개변수가 많다면 빌더를 고려하라점층적 생성자 패턴 과거에 주로 사용했던 방식 public NutritionFacts(int servingSize, int servings) { this(servingSize, servings, 0); } public NutritionFacts(int servingSize, int servings, int calories) { this(servingSize, servings, calories, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat) { this(servingSize, servings, calories, fat, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium) { this(servingSize, servings, calories, fat, sodium, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium, int carbohydrate) { this.servingSize = servingSize; this.servings = servings; this.calories = calories; this.fat = fat; this.sodium = sodium; this.carbohydrate = carbohydrate; } 점층적 생성자 패턴을 쓸수도 있지만, 매개변수 개수가 많아지면 클라이언트 코드를 작성하거나 읽기 어렵다. 타입이 같은 매개변수가 연달아 늘어서 있으면 찾기 어려운 버그로 이어질 수 있다.NutritionFacts cocaCola = new NutritionFacts(240, 8, 100, 0, 35, 27); // 각각의 값이 어떤걸 뜻하는지 한눈에 파악할 수 없음.자바빈즈 패턴(JavaBeans pattern) 매개변수가 없는 생성자로 객체를 만든 후, 세터(setter) 메서드들을 호출해주는 방식 NutritionFacts cocaCola = new NutritionFacts();cocaCola.setServingSize(240);cocaCola.setServings(8);cocaCola.setCalories(100);cocaCola.setSodium(35);cocaCola.setCarbohydrate(27); 자바빈즈 패턴에서는 객체 하나를 만들려면 메서드를 여러개 호출해야 하고, 객체가 완전히 생성되기 전까지는 일관성(consistency)이 무너진 상태에 놓이게 된다. 클래스를 불변으로 만들 수 없는 것도 문제 이러한 단점을 완화하고자 생성이 끝난 객체를 수동으로 freezing해서 변경할 수 없도록 하기도 한다.(그래도 별로)빌더 패턴(Builder pattern) 실제로 실무에서 많이 사용됨 빌더 패턴을 고려한다면 lombok을 사용하자 lombok@Entity@Getter@Builderpublic class Category { @Id @GeneratedValue private int id; @Column private String name; @Column private String icon; private Level level; // @OneToMany(mappedBy = &quot;id&quot;, cascade = CascadeType.ALL) private List&amp;lt;Category&amp;gt; subCagetoryList;} @Test public void builderTest() { Category.builder() .id(1) .level(Level.FIRST) .name(&quot;팬션의류/잡화&quot;) .icon(&quot;패션의류 아이콘.png&quot;) .subCagetoryList(new ArrayList&amp;lt;&amp;gt;()) .build(); } 빌더패턴은 파이썬과 스칼라에 있는 명명된 선택적 매개변수(named optional parameters)를 흉내낸 것. 잘못된 매개변수를 최대한 일찍 발견하려면 빌더의 생성자와 메서드에서 입력 매개변수를 검사하고 build 메소드가 호출하는 생성자에서 여러 매개변에 걸친 불변식(invariant)을 검사하자. lombok을 사용하더라도 bulderMethod를 지정한다던지 해서 검증 가능. 불변(immutable 혹은 immutability)과 불변식(invariant) 불변(immutable) - 어떠한 변경도 허용하지 않겠다는 뜻으로 가변(mutable) 객체와 구분하는 용도로 쓰인다. 대표적으로 String 객체는 한번 생성되면 절대 바꿀 수 없는 불변 객체 불변식(invariant) - 프로그램이 실행되는 동안, 혹은 정해진 기간동안 반드시 만족해야 하는 조건(리스트의 크기는 반드시 0 이상) 가변(mutable) 객체에서도 불변식(invariant)가 존재할 수 있음. 계층적으로 설계된 클래스와 함께 쓰기에도 좋음. Builder도 상속해서 처리하는데 lombok을 사용하는 것이 더 좋은 방식이라고 생각됨. @NoArgsConstructor@Getter@Builderpublic class BaseFashionItem implements FashionItem { private String brand; private Category category; private String name; private long price;}public class Shirt extends BaseFashionItem { private ColorType colorType; private SizeType sizeType; @Builder public Shirt(String brand, Category category, String name, long price, ColorType colorType, SizeType sizeType) { super(brand, category, name, price); this.colorType = colorType; this.sizeType = sizeType; }} @Test public void builderTest() { Shirt shirt = Shirt.builder() .category(Category.builder().build()) .brand(&quot;brand&quot;) .name(&quot;name&quot;) .price(15000) .colorType(ColorType.BLACK) .sizeType(SizeType.M) .build(); } 빌더 패턴의 단점 객체를 만들려면 그에 앞서 빌더부터 만들어야 하는 점.(성능에 민감한 상황에서는 문제가 될 수도 있다.) API는 시간이 지날수록 매개변ㄴ수가 많아지기 때문에 애초에 빌더를 고려하는 편이 나을 때가 많다.Item3. private 생성자나 열거 타입으로 싱글턴임을 보증하라.싱글턴이란? 인스턴스를 오직 하나만 생성할 수 있는 클래스 싱글턴의 전형적인 예로는 함수와 같은 무상태(stateless) 개체나 설계상 유일해야 하는 시스템 컴포넌트를 들수 있다. spring bean 객체를 보통 singleton scope를 이용해서 사용함. 클래스를 싱글턴으로 만들면 이를 사용하는 클라이언트를 테스트하기가 어려워 짐. 싱글턴 인스턴스를 mock 구현으로 대체할 수 없음. 인터페이스를 구현해서 만든 싱글턴이면 가능 private 생성자를 만들면 클라이언트에서는 객체 생성 할수 있는 방법이 존재하지 않아 인스턴스가 전체에서 하나뿐임을 보장할 수 있음. 리플렉션 API를 이용해서 호출 가능한데, 이를 방어하기 위해서 생성자에서 두번째 객체가 생성되려 할떄 예외를 던지게 하면 됨.싱글턴을 만드는 방법 1 public static final 필드 방식 싱글턴임이 API에 명백히 드러나고, 간결하다는 장점이 있음. public class Elvis {public static final Elvis INSTANCE = new Elvis();private Elvis() { }public void leaveTheBuilding() { System.out.println(&quot;Whoa baby, I&#39;m outta here!&quot;);}// 이 메서드는 보통 클래스 바깥(다른 클래스)에 작성해야 한다!public static void main(String[] args) { Elvis elvis = Elvis.INSTANCE; elvis.leaveTheBuilding();}} 2 정적 팩터리 메서드 방식 API를 바꾸지 않고도 싱글턴이 아니게 변경할 수 있음.(getInstnace 메소드를 변경하면 됨) 정적 팩터리를 제네릭 싱글턴 팩터리로 변경할 수 있음. 메서드 참조를 공급자로 사용할 수 있음. public class Elvis {private static final Elvis INSTANCE = new Elvis();private Elvis() { }public static Elvis getInstance() { return INSTANCE; }public void leaveTheBuilding() { System.out.println(&quot;Whoa baby, I&#39;m outta here!&quot;);}// 이 메서드는 보통 클래스 바깥(다른 클래스)에 작성해야 한다!public static void main(String[] args) { Elvis elvis = Elvis.getInstance(); elvis.leaveTheBuilding();}} 3 Enum 더 간결하고, 추가 노력 없이 직렬화 할 수 있음. 복잡한 직렬화 상황, 리플렉션 공격에서도 완벽히 방어 대부분 상황에서는 원소가 하나뿐인 Enum이 싱글턴을 만드는 가장 좋은 방법 Enum에서 Enum을 참조하는 경우 순환 참조에 주의 실글턴 클래스를 직렬화하는 방법 모든 인스턴스 필드를 일시적(transient)이라고 선언하고 readResolve 메소드를 제공(Item 89) private Object readResolve() {return INSTNACE;} Item4. 인스턴스화를 막으려거든 private 생성자를 사용하라. 정적 메서드와 정적 필드만을 담은 클래스를 만들고 싶을때가 있다. 객체지향적이지는 않지만 나름의 쓰임새가 있음.(ex java.lang.Math, Utils 시리즈) 추상클래스를 만들어서 인스턴스화를 금지하는 것은 안됨(이렇게 해본적도 없음.) private 생성자를 추가해서 클래스의 인스턴스화를 막자. lombok을 이용하면 편함. @NoArgsConstructor(access = AccessLevel.PRIVATE)public class ContentLanguageConfig {public static final List&amp;lt;Language&amp;gt; SERVICE_LIST;public static final List&amp;lt;Language&amp;gt; EMAIL_PUSH_SUPPORT_LIST;public static final List&amp;lt;Language&amp;gt; EMAIL_JOIN_SUPPORT_LIST;static { SERVICE_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.THAI, Language.INDONESIAN, Language.JAPANESE); EMAIL_PUSH_SUPPORT_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.JAPANESE); EMAIL_JOIN_SUPPORT_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.JAPANESE);}/** * 서비스중인 언어인지 확인 * @param language * @return */public static boolean isService(Language language) { return SERVICE_LIST.contains(language);}} 생성자 내에서 Exception을 던지도록 한다면 주석을 달아두자 이 방식을 적용했을때 상속을 불가능하게 하는 효과도 가져옴.Comment 위에 있는 내용들은 어쩌보면 자바의 기본적인 내용임에도 불구하고 실제로는 놓치는 경우가 많을수 있어서 해당 작업을 직접 해보는 것이 좋다. sonarqube 정적 분석을 하는 경우에 Util Class에서 Instance화를 하지 않도록 가이드 해주기도 하므로 Sonarqube 같은 정적 분석툴을 도입하고 분석결과를 잘 적용하는것도 코드 퀄리티를 향상시키는데 도움이 된다." }, { "title": "Hello World", "url": "/posts/essay-1/", "categories": "Essay", "tags": "Hello World, Test", "date": "2020-04-24 08:32:00 +0900", "snippet": "Hello Worldprintln(&quot;hello world&quot;)" } ]
